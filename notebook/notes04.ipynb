{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLLib Development Notes\n",
    "April 2022\n",
    "\n",
    "Reverse time order - Newest on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17 May 2022\n",
    "- Adding artifcats to [../workflow/seg.yaml](../workflow/seg.yaml) to pass log intex between steps to produce one continuous tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 May 2022\n",
    "1. How to experiment on convergence?\n",
    "    - Which convergence tests apply to CNN convergence\n",
    "        <svg id=\"svg\" viewbox=\"21.659997940063477,68.5999984741211,268.79998779296875,193.60000610351562\" style=\"height:193.60000610351562\"><path d=\"M 137.26,78.6 L 136.46,79.4 L 136.46,81 L 136.46,84.2 L 136.46,87.4 L 136.46,92.2 L 136.46,101 L 135.66,109 L 135.66,117 L 135.66,125 L 135.66,136.2 L 135.66,144.2 L 135.66,152.2 L 135.66,161 L 135.66,167.4 L 135.66,173 L 135.66,177.8 L 135.66,182.6 L 134.86,185.8 L 134.86,189.8 L 134.06,192.2 L 133.26,195.4 L 132.46,197.8 L 131.66,199.4 L 130.86,201.8 L 130.86,202.6 L 130.86,204.2 L 130.06,205.8 L 130.06,206.6 L 130.06,208.2 L 130.06,209 L 130.86,209 L 131.66,209 L 132.46,209 L 134.06,208.2 L 135.66,208.2 L 138.86,207.4 L 142.86,207.4 L 150.06,206.6 L 157.26,206.6 L 166.86,205.8 L 177.26,205 L 189.26,204.2 L 204.46,204.2 L 216.46,203.4 L 230.06,203.4 L 242.06,203.4 L 255.66,202.6 L 265.26,201.8 L 272.46,201 L 278.06,200.2 L 280.46,200.2\" fill=\"none\" stroke=\"#6190e8\" stroke-width=\"2\"></path><path d=\"M 254.86,81.8 L 254.86,83.4 L 254.86,86.6 L 254.06,89 L 253.26,93.8 L 252.46,97.8 L 250.86,103.4 L 248.46,109 L 246.06,114.6 L 243.66,120.2 L 239.66,127.4 L 234.86,134.6 L 231.66,139.4 L 226.86,144.2 L 221.26,149 L 214.06,155.4 L 208.46,160.2 L 202.06,164.2 L 195.66,168.2 L 189.26,172.2 L 184.46,174.6 L 178.86,177 L 170.86,180.2 L 165.26,183.4 L 161.26,185.8 L 156.46,187.4 L 151.66,189 L 150.06,189 L 149.26,189\" fill=\"none\" stroke=\"#6190e8\" stroke-width=\"2\"></path>  <text font-family=\"inherit\" font-size=\"14\" fill=\"#6190e8\" x=\"140.46\" y=\"249\">Cross entropy loss</text><text font-family=\"inherit\" font-size=\"14\" fill=\"#6190e8\" x=\"32.46\" y=\"149\">Architecture Loss</text></svg> \n",
    "\n",
    "    - Does convergence depend on initial conditions?\n",
    "    - Does convergence depend on search method?\n",
    "1. How to experiment on relaxation?\n",
    "    - Output difference between relaxed model and pruned model\n",
    "    - Objective performance of relaxed model and retrained pruned model\n",
    "    - Sparsity of pruned model\n",
    "1. How effectively is the space searched\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 May 2021\n",
    "- Objective: Initialize model with pretrained weights to improve training and convergence\n",
    "- Observe if there is a relationship between pretrained weights and pruning\n",
    "- Transfer learning: \n",
    "    1. Create full unpruned model\n",
    "    2. model.load_state_dict(<model weights>, strict=False)\n",
    "\n",
    "_Convergence and stability_\n",
    "- There is a lot of discussion of convergence and stability in NAS and structured pruning [STACNAS: TOWARDS STABLE AND CONSISTENT OPTIMIZATION FOR DIFFERENTIABLE NEURAL ARCHITECTURE SEARCH](https://openreview.net/pdf?id=rygpAnEKDH)\n",
    "- Formal methods to evaluate convergence and stability are missing from deep learning optimization literature\n",
    "- Stability [Numerical stability](https://en.wikipedia.org/wiki/Numerical_stability#:~:text=The%20usual%20definition%20of%20numerical,)%20%E2%88%92%20y*%20is%20small.), [Stability theory](https://en.wikipedia.org/wiki/Stability_theory)\n",
    "- Convergence: [Convergent series](https://en.wikipedia.org/wiki/Convergent_series), [Convergence tests](https://en.wikipedia.org/wiki/Convergence_tests), [Conditional convergence](https://en.wikipedia.org/wiki/Conditional_convergence)\n",
    "- Relaxation similarity to Pruned model with various approaches - Nothing, DAIS, weighting basis function\n",
    "- Display pruning map across training\n",
    "- Append Tensorboard across Argo steps\n",
    "- Quick method to view Tensorboard for specific run from Jupyter\n",
    "- Once we can view training, evaluate various approaches:\n",
    "    1. Train -> Prune -> Train -> Prune\n",
    "    1. Simultaneous train & prune\n",
    "    1. Train to specific size & prune\n",
    "    1. Weighting methods from training\n",
    "    1. [Figure 3 Loss Contours](https://openreview.net/pdf?id=SYuJXrXq8tw)\n",
    "    1. [Figure 1 Loss surface](https://arxiv.org/pdf/1712.09913.pdf)\n",
    "- What can be answered about generalization of pruned network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 May 2022\n",
    "- CRISP training of UNET on the LIT dataset takes a few hours\n",
    "- The same parameters on COCO take a few days\n",
    "- The COCO dataset is much more diverse than the LIT dataset\n",
    "- The COCO dataset is 100x larger than the LIT dataset\n",
    "- A6000 GPU efficiency is 70% on COCO\n",
    "- I am only using half the available A6000 memory.  Increasing this should reduce the training time\n",
    "- Look at [Deeplab V3](https://arxiv.org/pdf/1706.05587.pdf) with CRISP\n",
    "- Deeplab V3 : [Pytorch implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/deeplabv3.py)\n",
    "- Deeplab V3 \n",
    "    1. encoder\n",
    "        - resnet of different sizes\n",
    "        - mobilenet V3\n",
    "        - ...\n",
    "    1. classifier - FCN head: Conv2d->Batch Norm->relu->Dropout->Conv2d\n",
    "    1. Atrous spatial pyramid pooling: list of Atrous convolutions->BatchNorm->Relu followed\n",
    "    1. Conv->Batch norm->Relu->Dropout across pooling levels\n",
    "- Add per channel relaxation layer following each conv->batch norm\n",
    "- Add network weight metric\n",
    "- Add network pruning function\n",
    "- How to structure this?\n",
    "    1. If search and augment network it can be applied to existing networks without rewriting\n",
    "    1. Instance norm or other norms would be sufficient.  Not just batch norm.\n",
    "    1. Test pruning with and without norms on problems that where convolution scaling can offset continuous relaxation\n",
    "    1. Append norm if needed and relaxation layers to network\n",
    "    1. Efficiently compute architecture loss from augmented network, not class\n",
    "    1. Efficient prune from network, not class\n",
    "    1. Begin with cell2d\n",
    "    1. Explore network-level pruning\n",
    "    1. What is the effect of network-level pruning on stability and convergence\n",
    "    1. Metrics for stability and converence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 May 2022\n",
    "- The past couple of weeks, I have been splitting off code \n",
    "- The python library [pymlutil](https://pypi.org/project/pymlutil/) contains general utility functions\n",
    "- The python library [torchdatasetuitl](https://pypi.org/project/torchdatasetutil/) contains creating and using datasets\n",
    "- I will be removing this code from pymlutil soon.\n",
    "- One goal for this is to enable the dataset to be selected as a parameter\n",
    "- To do this the input channels and format must to be specified in the network creation.\n",
    "- I am adding class_dictionary['input_channels'] and class_dictionary['input_type'] to the class_dictionary.  This way, selecting the class dictionary defines the input type\n",
    "- If the input dataset can be selected by parameter rather than requiring code changes, I would like to enable transfer learning across training sets.\n",
    "- I would like to pick up the Torch pretrained [UNET](https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/) and [Deeplab V3](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)\n",
    "- To enable transfer learning, I need to change from torch.load to model.load_state_dict: https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html\n",
    "- torch.load loads the pickled python object which I needed to do previously because pruned state in class variables - e.g. - convolution size after pruning is complete\n",
    "- Transfer learning only may only work for unpruned models\n",
    "- After a model is pruned, how would transfer learning from an unpruned model work?  \n",
    "    - require model pruning weights\n",
    "    - copy parameters based on pruning weights\n",
    "    - easier and same result if you initialize the model just before pruning,\n",
    "    - initialize parameters from transfer learning\n",
    "    - prune model with weights from transfer learning\n",
    "\n",
    "    \"input_channels\":3,\n",
    "    \"input_type\": \"float32\",\n",
    "- For transfer learning, need to handle changes in input channel depth from 1 to 3 and visa versa - for datasets with grayscale and RGB inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 April 2022\n",
    "- Training LIT\n",
    "- Increasing the learning rate 1 2e-4 appears to have pushed the model convergence to train the network to find nothing - always background\n",
    "- There is a heavy class imbalance\n",
    "- Adding class weighting to full training\n",
    "- Next work on switching datasets with parameter\n",
    "- Add cityscapes dataset\n",
    "- Add denoise dataset\n",
    "- Pytorch [Dataset and Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "- Uses: load, split, train, test, view\n",
    "- [datasets/cocostore.py](../datasets/cocostore.py) - loads coco into torch dataset from S3\n",
    "- [datasets/imstore.py](../datasets/imstore.py) - loads directories of images into torch dataset\n",
    "- [dataset/citytorch.py](../datasets/citytorch.py) - loads cityscapes to a torch dataset \n",
    "- [OpenImages dataset](https://storage.googleapis.com/openimages/web/factsfigures.html)\n",
    "- Disable augmentation on validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 April 2022\n",
    "- Scale annealing and prune basis based on start & end points.  \n",
    "- Switch to exponential function where start and end point are specified\n",
    "- Initially grow based on epochs\n",
    "- Experiment with growing based on step to make training more independent of dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 April 2022\n",
    "- CRISP training with pruning basis function crisplit_20220401i_pb00_\n",
    "- Pruned 94% of network while increasing segmentation similarity\n",
    "- May achieve~ 25X speedup\n",
    "- Pruning search not strictly 1 and 0.  \n",
    "\n",
    "\n",
    "[T00cw]: ../img/crisplit_20220401i_pb00_00_cw.png\n",
    "[T01cw]: ../img/crisplit_20220401i_pb00_01_cw.png\n",
    "[T02cw]: ../img/crisplit_20220401i_pb00_02_cw.png\n",
    "[T03cw]: ../img/crisplit_20220401i_pb00_03_cw.png\n",
    "[T00gn]: ../img/crisplit_20220401i_pb00_00_gn.png\n",
    "[T01gn]: ../img/crisplit_20220401i_pb00_01_gn.png\n",
    "[T02gn]: ../img/crisplit_20220401i_pb00_02_gn.png\n",
    "[T03gn]: ../img/crisplit_20220401i_pb00_03_gn.png\n",
    "|  | normalized-train | train | train_fine | prune |\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|Cross Entropy Loss|0.163|0.0068|0.0112|0.005|\n",
    "|Reamining Ratio   |1.0  |0.132 |0.131 |0.057|\n",
    "|Test similarity   |     |      |0.113 |0.817|\n",
    "| Prune Weights |![][T00cw]|![][T01cw] |![][T02cw] |![][T03cw]|\n",
    "| Gradient Norm |![][T00gn]|![][T01gn] |![][T02gn] |![][T03gn]|\n",
    "\n",
    "- Final losses before pruning:\n",
    "\n",
    "| Loss               | Value    |\n",
    "|:------------------:|:--------:|\n",
    "| cross_entropy_loss | 0.011264 |\n",
    "| architecture_loss  | 0.001306 |\n",
    "| prune_loss         | 0.000196 |\n",
    "\n",
    "- Increasing prune loss should incrase this sepration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Exponential():\n",
    "    def __init__(self,vx=0.0, vy=0.0, px=1.0, py=1.0, power=2.0):\n",
    "        self.vx = vx\n",
    "        self.vy = vy\n",
    "        self.px = px\n",
    "        self.py = py\n",
    "        if power < 0:\n",
    "            raise ValueError('Exponential error power {} must be >= 0'.format(power))\n",
    "        self.power = power\n",
    "        if px <= vx:\n",
    "            raise ValueError('Exponential error px={} must be > vx'.format(px, vx))\n",
    "        else:\n",
    "            self.a = (py-vy)/np.power(px-vx,power)\n",
    "    def f(self, x):\n",
    "        dx = x-self.vx\n",
    "        y = self.a*np.power(x-self.vx,self.power) + self.vy\n",
    "        return y\n",
    "\n",
    "vx = -1\n",
    "px = 1\n",
    "expf =  Exponential(vx=vx, vy=0.0, px=px, py=1.0, power=2.0)\n",
    "\n",
    "x = np.arange(vx, px, 0.01)\n",
    "plt.plot(x, expf.f(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 April 2022\n",
    "- Prune loss basis function training to 0 size\n",
    "- This resulted in much more agressive pruning\n",
    "- Slightly worse cross entropy loss for train and train_fine\n",
    "- Much worse cross entropy loss for pruned network\n",
    "- The tensorboard plots show a similar timing of the architecture reduction curves but with prune_loss, the architecture settles at a lower level.\n",
    "- Architecture reduction occurs early and converges rapidly compared to cross entropy loss.  \n",
    "- Reduce k_structure from 0.03 to 0.1 to slow architecture search\n",
    "- Increase k_prune_basis from 0.01 to 0.3 to see if the resuls in eliminating fence sittes more effectively\n",
    "- Increase k_prune_exp from 5.0 to 50.0 apply prunce basis later in training\n",
    "\n",
    "[T00cw]: ../img/crisplit_20220331h_pb0_00_cw.png\n",
    "[T01cw]: ../img/crisplit_20220331h_pb0_01_cw.png\n",
    "[T02cw]: ../img/crisplit_20220331h_pb0_02_cw.png\n",
    "[T03cw]: ../img/crisplit_20220331h_pb0_03_cw.png\n",
    "[T00gn]: ../img/crisplit_20220331h_pb0_00_gn.png\n",
    "[T01gn]: ../img/crisplit_20220331h_pb0_01_gn.png\n",
    "[T02gn]: ../img/crisplit_20220331h_pb0_02_gn.png\n",
    "[T03gn]: ../img/crisplit_20220331h_pb0_03_gn.png\n",
    "|  | normalized-train | train | train_fine | prune |\n",
    "|:--:|:--:|:--:|:--:|:--:|\n",
    "|Cross Entropy Loss|0.164|0.032|0.020|0.040|\n",
    "|Reamining Ratio|1.0|0.050|0.041|0.00024|\n",
    "|Test similarity|  |  |0.106|0.028|\n",
    "| Prune Weights |![][T00cw]|![][T01cw] |![][T02cw] |![][T03cw]|\n",
    "| Gradient Norm |![][T00gn]|![][T01gn] |![][T02gn] |![][T03gn]|\n",
    "\n",
    "- Signmod scale as linear function stiffens very quickly.  Want it to be gradual at the beginning and stiffen towards the end of the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def SigmoidScale(step, start_x = 0, end_x = 25, start_y = 0, end_y=100):\n",
    "    kSigmoid = start_y + (end_y-start_y)*step/(end_x-start_x)\n",
    "    return kSigmoid\n",
    "x = np.arange(0.0, 30.0, 0.01)\n",
    "k_prune_exp = 1\n",
    "sigmoid_scale = 5\n",
    "sigmoid_scale_exp = 0.25\n",
    "plt.plot(x, SigmoidScale(x, end_x=25, end_y=150))\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(-1.0, 1.0, 0.01)\n",
    "step =3\n",
    "kSigmoid = SigmoidScale(step, end_x=25, end_y=150)\n",
    "print('kSigmoid({})={}'.format(step, kSigmoid))\n",
    "plt.plot(x, sigmoid(x, k=kSigmoid))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 April 2022\n",
    "- updated [pymlutil](https://github.com/bhlarson/pymlutil) to deliver new tag and pipy version when excuting ./setup\n",
    "- Added pattern for documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 April 2022\n",
    "- Added imstore.py CreateDataLoaders to load unique datasets for any number of sets with unique parameters \n",
    "- Moving imstore common processing to ImUtil\n",
    "- Move into  library for dataset loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
