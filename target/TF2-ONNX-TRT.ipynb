{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow through ONNX:\n",
    "\n",
    "From [Using Tensorflow 2 through ONNX.ipynb](https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/3.%20Using%20Tensorflow%202%20through%20ONNX.ipynb)\n",
    "\n",
    "The ONNX path to getting a TensorRT engine is a high-performance approach to TensorRT conversion that works with a variety of frameworks - including Tensorflow and Tensorflow 2.\n",
    "\n",
    "TensorRT's ONNX parser is an all-or-nothing parser for ONNX models that ensures an optimal, single TensorRT engine and is great for exporting to the TensorRT API runtimes. ONNX models can be easily generated from Tensorflow models using the ONNX project's keras2onnx and tf2onnx tools.\n",
    "\n",
    "In this notebook we will take a look at how ONNX models can be generated from a Keras/TF2 ResNet50 model, how we can convert those ONNX models to TensorRT engines using trtexec, and finally how we can use the native Python TensorRT runtime to feed a batch of data into the TRT engine at inference time.\n",
    "\n",
    "Essentially, we will follow this path to convert and deploy our model:\n",
    "\n",
    "![Tensorflow+ONNX](./images/tf_onnx.png)\n",
    "\n",
    "__Use this when:__\n",
    "- You want the most efficient runtime performance possible out of an automatic parser\n",
    "- You have a network consisting of mostly supported operations -  including operations and layers that the ONNX parser uniquely supports (Such as RNNs/LSTMs/GRUs)\n",
    "- You are willing to write custom C++ plugins for any unsupported operations (if your network has any)\n",
    "- You do not want to use the manual layer builder API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Checking your GPU status:__\n",
    "\n",
    "Lets see what GPU hardware we are working with. Our hardware can matter a lot because different cards have different performance profiles and precisions they tend to operate best in. For example, a V100 is relatively strong as FP16 processing vs a T4, which tends to operate best in the INT8 mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "IJBfZsGo8yaV",
    "outputId": "f4c4e20d-fcfd-43a2-b10d-c6978c25c91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 19 16:01:31 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:15:00.0  On |                  Off |\n",
      "| 33%   26C    P8    18W / 260W |   1219MiB / 24217MiB |      4%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to sucessfully deploy a TensorRT model, you have to make __five key decisions__:\n",
    "\n",
    "1. __What format should I save my model in?__\n",
    "2. __What batch size(s) am I running inference at?__\n",
    "3. __What precision am I running inference at?__\n",
    "4. __What TensorRT path am I using to convert my model?__\n",
    "5. __What runtime am I targeting?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What format should I save my model in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to load up a pretrained ResNet50 model. This can be done easily using keras.applications - a collection of pretrained image model classifiers that can additionally be used as backbones for detection and other deep learning problems.\n",
    "\n",
    "We can load up a pretrained classifier with batch size 32 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iVRVItvR8quS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cKT07xPV8qua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102973440/102967424 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TestSave/assets\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Does keras2onnx survive saving and loading model from disk\n",
    "import tensorflow as tf\n",
    "savepath = 'TestSave'\n",
    "model.save(savepath, save_format='tf')\n",
    "model = tf.keras.models.load_model(savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of checking our non-optimized model, we can use a dummy batch of data to verify our performance and the consistency of our results across precisions. 224x224 RGB images are a common  format, so lets generate a batch of them.\n",
    "\n",
    "Once we generate a batch of them, we will feed it through the model using .predict() to \"warm up\" the model. The first batch you feed through a deep learning model often takes a lot longer as just-in-time compilation and other runtime optimizations are performed. Once you get that first batch through, further performance tends to be more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "8ETjzebW8qu2",
    "outputId": "03a8e7c1-215c-404a-8cac-8030132657dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       ...,\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dummy_input_batch = np.zeros((BATCH_SIZE, 224, 224, 3))\n",
    "\n",
    "model.predict(dummy_input_batch) # warm up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Baseline Timing:__\n",
    "\n",
    "Once we have warmed up our non-optimized model, we can get a rough timing estimate of our model using %%timeit, which runs the cell several times and reports timing information.\n",
    "\n",
    "Lets take a look at how long our model takes to run at baseline before doing any TensorRT optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "eMu3dZlM96bh",
    "outputId": "537a88e2-ad7d-413a-f815-abd91f010e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.3 ms ± 4.27 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "result = model.predict_on_batch(dummy_input_batch) # Check default performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at the resulting batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       ...,\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04],\n",
       "       [1.6964244e-04, 3.3007504e-04, 6.1350627e-05, ..., 1.4622412e-05,\n",
       "        1.4449914e-04, 6.6087063e-04]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict_on_batch(dummy_input_batch)\n",
    "result[:10] # The probabilities for the first ten Imagenet classes in the first sample of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay - now that we have a baseline model, lets convert it to the format TensorRT understands best: ONNX. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert Keras model to ONNX intermediate model and save:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ONNX format is a framework-agnostic way of describing and saving the structure and state of deep learning models. We can convert Tensorflow 2 Keras models to ONNX using the keras2onnx tool provided by the ONNX project. (You can find the ONNX project here: https://onnx.ai or on GitHub here: https://github.com/onnx/onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aG3tXUEx8quf"
   },
   "outputs": [],
   "source": [
    "import onnx, keras2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting a model with default parameters to an ONNX model is fairly straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "QxLAvWp68quk",
    "outputId": "d750962a-d098-4a63-c195-c3442211cdc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f084ca2714f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras2onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras2onnx/main.py\u001b[0m in \u001b[0;36mconvert_keras\u001b[0;34m(model, name, doc_string, target_opset, channel_first_inputs, debug_mode, custom_op_conversions)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mparse_graph_modeless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopology\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_opset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mparse_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopology\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_opset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras2onnx/parser.py\u001b[0m in \u001b[0;36mparse_graph\u001b[0;34m(topo, graph, target_opset, output_names, keras_node_dict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0mtopo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_output_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m     return _parse_graph_core_v2(\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras_node_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_keras\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_parse_graph_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras2onnx/parser.py\u001b[0m in \u001b[0;36m_parse_graph_core_v2\u001b[0;34m(graph, keras_node_dict, topology, top_scope, output_names)\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         layer_info, model_ = _parse_nodes_v2(graph, inference_nodeset, input_nodes, keras_node_dict, node,\n\u001b[0m\u001b[1;32m    718\u001b[0m                                              varset, visited, q_overall)\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlayer_info\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already processed by the _parse_nodes_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras2onnx/parser.py\u001b[0m in \u001b[0;36m_parse_nodes_v2\u001b[0;34m(graph, inference_nodeset, graph_inputs, keras_node_dict, node, varset, visited, q_overall)\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             layer_info = LayerInfo.create(node, layer_key,\n\u001b[0m\u001b[1;32m    668\u001b[0m                                           {**keras_node_dict, **current_layer_outputs}, inference_nodeset)\n\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras2onnx/_parser_tf.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(node, layer, outputs_map, inference_nodeset)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0;31m# in layer_spec model, the layer name will be checked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mfstr_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisited\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minference_nodeset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "onnx_model = keras2onnx.convert_keras(model, model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That said, we do need to make one change for our model to work with TensorRT. Keras by default uses a dynamic input shape in its networks - where it can handle arbitrary batch sizes at every update. While TensorRT can do this, it requires extra configuration. \n",
    "\n",
    "Instead, we will just set the input size to be fixed to our batch size. This will work with TensorRT out of the box!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Configure ONNX File Batch Size:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ We need to do two things to set our batch size with ONNX. The first is to modify our ONNX file to change its default batch size to our target batch size. The second is setting our converter to use the __explicit batch__ mode, which will use this default batch size as our final batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onnx_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-03cb3a4a8186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onnx_model' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = onnx_model.graph.input\n",
    "for input in inputs:\n",
    "    dim1 = input.type.tensor_type.shape.dim[0]\n",
    "    dim1.dim_value = BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save Model:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jFT6-13f8qup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"resnet50_onnx_model.onnx\"\n",
    "onnx.save_model(onnx_model, model_name)\n",
    "print(\"Done saving!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get our model into ONNX format, we can convert it efficiently using TensorRT. For this, TensorRT needs exclusive access to your GPU. If you so much as import Tensorflow, it will generally consume all of your GPU memory. To get around this, before moving on go ahead and shut down this notebook and restart it. (You can do this in the menu: Kernel -> Restart Kernel)\n",
    "\n",
    "Make sure not to import Tensorflow at any point after restarting the runtime! \n",
    "\n",
    "(The following cell is a quick shortcut to make your notebook restart:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZUnHVHE8quu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting kernel  in three seconds...\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "print(\"Restarting kernel  in three seconds...\")\n",
    "time.sleep(3)\n",
    "print(\"Restarting kernel now\")\n",
    "os._exit(0) # Shut down all kernels so TRT doesn't fight with Tensorflow for GPU memory - TF monopolizes all GPU memory by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What batch size(s) am I running inference at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have actually already set our inference batch size - see the note above in section 1!\n",
    "\n",
    "We are going to set our target batch size to a fixed size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do two things to set our batch size to a fixed batch size with ONNX: \n",
    "\n",
    "1. Modify our ONNX file to change its default batch size to our target batch size, which we did above.\n",
    "2. Use the trtexec --explicitBatch flag, which we also did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What precision am I running inference at?\n",
    "\n",
    "Now, we have a converted TensorRT engine. Great! That means we are ready to load it into the native Python TensorRT runtime. This runtime strikes a balance between the ease of use of the high level Python runtimes and the low level C++ runtimes.\n",
    "\n",
    "First, as before, lets create a dummy batch. Importantly, by default TensorRT will use the input precision you give it as the default precision for the rest of the network. \n",
    "\n",
    "Remember that lower precisions than FP32 tend to run faster. There are two common reduced precision modes - FP16 and INT8. Graphics cards that are designed to do inference well often have an affinity for one of these two types. This guide was developed on an NVIDIA V100, which favors FP16, so we will use that here by default. INT8 is a more complicated process that requires a calibration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "USE_FP16 = True\n",
    "\n",
    "target_dtype = np.float16 if USE_FP16 else np.float32\n",
    "dummy_input_batch = np.zeros((BATCH_SIZE, 224, 224, 3), dtype = np.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What TensorRT path am I using to convert my model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT is able to take ONNX models and convert them entirely into a single, efficient TensorRT engine. Restart your Jupyter kernel, and then start here!\n",
    "\n",
    "We can use trtexec, a command line tool for working with TensorRT, in order to convert an ONNX model to an engine file.\n",
    "\n",
    "To convert the model we saved in the previous steps, we need to point to the ONNX file, give trtexec a name to save the engine as, and last specify that we want to use a fixed batch size instead of a dynamic one.\n",
    "\n",
    "__Remember to shut down all Jupyter notebooks and restart your Jupyter kernel after \"1. What format should I save my model in?\" - otherwise this cell will crash as TensorRT competes with Tensorflow for GPU memory:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "h60Gmotx8quz",
    "outputId": "065384aa-c848-4194-c72c-cad0d80449ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec # trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine.trt --explicitBatch --fp16\n",
      "[02/18/2021-18:59:14] [I] === Model Options ===\n",
      "[02/18/2021-18:59:14] [I] Format: ONNX\n",
      "[02/18/2021-18:59:14] [I] Model: resnet50_onnx_model.onnx\n",
      "[02/18/2021-18:59:14] [I] Output:\n",
      "[02/18/2021-18:59:14] [I] === Build Options ===\n",
      "[02/18/2021-18:59:14] [I] Max batch: explicit\n",
      "[02/18/2021-18:59:14] [I] Workspace: 16 MiB\n",
      "[02/18/2021-18:59:14] [I] minTiming: 1\n",
      "[02/18/2021-18:59:14] [I] avgTiming: 8\n",
      "[02/18/2021-18:59:14] [I] Precision: FP32+FP16\n",
      "[02/18/2021-18:59:14] [I] Calibration: \n",
      "[02/18/2021-18:59:14] [I] Refit: Disabled\n",
      "[02/18/2021-18:59:14] [I] Safe mode: Disabled\n",
      "[02/18/2021-18:59:14] [I] Save engine: resnet_engine.trt\n",
      "[02/18/2021-18:59:14] [I] Load engine: \n",
      "[02/18/2021-18:59:14] [I] Builder Cache: Enabled\n",
      "[02/18/2021-18:59:14] [I] NVTX verbosity: 0\n",
      "[02/18/2021-18:59:14] [I] Tactic sources: Using default tactic sources\n",
      "[02/18/2021-18:59:14] [I] Input(s)s format: fp32:CHW\n",
      "[02/18/2021-18:59:14] [I] Output(s)s format: fp32:CHW\n",
      "[02/18/2021-18:59:14] [I] Input build shapes: model\n",
      "[02/18/2021-18:59:14] [I] Input calibration shapes: model\n",
      "[02/18/2021-18:59:14] [I] === System Options ===\n",
      "[02/18/2021-18:59:14] [I] Device: 0\n",
      "[02/18/2021-18:59:14] [I] DLACore: \n",
      "[02/18/2021-18:59:14] [I] Plugins:\n",
      "[02/18/2021-18:59:14] [I] === Inference Options ===\n",
      "[02/18/2021-18:59:14] [I] Batch: Explicit\n",
      "[02/18/2021-18:59:14] [I] Input inference shapes: model\n",
      "[02/18/2021-18:59:14] [I] Iterations: 10\n",
      "[02/18/2021-18:59:14] [I] Duration: 3s (+ 200ms warm up)\n",
      "[02/18/2021-18:59:14] [I] Sleep time: 0ms\n",
      "[02/18/2021-18:59:14] [I] Streams: 1\n",
      "[02/18/2021-18:59:14] [I] ExposeDMA: Disabled\n",
      "[02/18/2021-18:59:14] [I] Data transfers: Enabled\n",
      "[02/18/2021-18:59:14] [I] Spin-wait: Disabled\n",
      "[02/18/2021-18:59:14] [I] Multithreading: Disabled\n",
      "[02/18/2021-18:59:14] [I] CUDA Graph: Disabled\n",
      "[02/18/2021-18:59:14] [I] Separate profiling: Disabled\n",
      "[02/18/2021-18:59:14] [I] Skip inference: Disabled\n",
      "[02/18/2021-18:59:14] [I] Inputs:\n",
      "[02/18/2021-18:59:14] [I] === Reporting Options ===\n",
      "[02/18/2021-18:59:14] [I] Verbose: Disabled\n",
      "[02/18/2021-18:59:14] [I] Averages: 10 inferences\n",
      "[02/18/2021-18:59:14] [I] Percentile: 99\n",
      "[02/18/2021-18:59:14] [I] Dump refittable layers:Disabled\n",
      "[02/18/2021-18:59:14] [I] Dump output: Disabled\n",
      "[02/18/2021-18:59:14] [I] Profile: Disabled\n",
      "[02/18/2021-18:59:14] [I] Export timing to JSON file: \n",
      "[02/18/2021-18:59:14] [I] Export output to JSON file: \n",
      "[02/18/2021-18:59:14] [I] Export profile to JSON file: \n",
      "[02/18/2021-18:59:14] [I] \n",
      "[02/18/2021-18:59:14] [I] === Device Information ===\n",
      "[02/18/2021-18:59:14] [I] Selected Device: Quadro RTX 6000\n",
      "[02/18/2021-18:59:14] [I] Compute Capability: 7.5\n",
      "[02/18/2021-18:59:14] [I] SMs: 72\n",
      "[02/18/2021-18:59:14] [I] Compute Clock Rate: 1.77 GHz\n",
      "[02/18/2021-18:59:14] [I] Device Global Memory: 24217 MiB\n",
      "[02/18/2021-18:59:14] [I] Shared Memory per SM: 64 KiB\n",
      "[02/18/2021-18:59:14] [I] Memory Bus Width: 384 bits (ECC disabled)\n",
      "[02/18/2021-18:59:14] [I] Memory Clock Rate: 7.001 GHz\n",
      "[02/18/2021-18:59:14] [I] \n",
      "----------------------------------------------------------------\n",
      "Input filename:   resnet50_onnx_model.onnx\n",
      "ONNX IR version:  0.0.7\n",
      "Opset version:    12\n",
      "Producer name:    keras2onnx\n",
      "Producer version: 1.7.0\n",
      "Domain:           onnxmltools\n",
      "Model version:    0\n",
      "Doc string:       \n",
      "----------------------------------------------------------------\n",
      "[02/18/2021-18:59:27] [W] [TRT] /workspace/TensorRT/parsers/onnx/onnx2trt_utils.cpp:218: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[02/18/2021-18:59:30] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[02/18/2021-19:00:24] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[02/18/2021-19:00:24] [I] Engine built in 70.5508 sec.\n",
      "[02/18/2021-19:00:24] [I] Starting inference\n",
      "[02/18/2021-19:00:28] [I] Warmup completed 0 queries over 200 ms\n",
      "[02/18/2021-19:00:28] [I] Timing trace has 0 queries over 3.0174 s\n",
      "[02/18/2021-19:00:28] [I] Trace averages of 10 runs:\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.57674 ms - Host latency: 8.25873 ms (end to end 12.7706 ms, enqueue 0.69769 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.53723 ms - Host latency: 8.21382 ms (end to end 12.9233 ms, enqueue 0.654083 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.11967 ms - Host latency: 7.737 ms (end to end 12.0634 ms, enqueue 0.564871 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.70404 ms - Host latency: 7.37631 ms (end to end 11.258 ms, enqueue 0.518823 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.71819 ms - Host latency: 7.34548 ms (end to end 10.8966 ms, enqueue 0.531787 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.81364 ms - Host latency: 7.42823 ms (end to end 11.4438 ms, enqueue 0.555359 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.818 ms - Host latency: 7.43254 ms (end to end 11.4992 ms, enqueue 0.594739 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.81769 ms - Host latency: 7.43046 ms (end to end 11.4876 ms, enqueue 0.563385 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.29329 ms - Host latency: 7.91026 ms (end to end 12.4589 ms, enqueue 0.634082 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.67969 ms - Host latency: 8.33804 ms (end to end 13.227 ms, enqueue 0.625824 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.52704 ms - Host latency: 8.14534 ms (end to end 12.9104 ms, enqueue 0.565289 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.64918 ms - Host latency: 8.27152 ms (end to end 13.1298 ms, enqueue 0.563525 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.73561 ms - Host latency: 8.37678 ms (end to end 13.3005 ms, enqueue 0.613489 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.99851 ms - Host latency: 8.61926 ms (end to end 13.7191 ms, enqueue 0.658502 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.57777 ms - Host latency: 8.19817 ms (end to end 13.0371 ms, enqueue 0.625256 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.70059 ms - Host latency: 8.31952 ms (end to end 13.2617 ms, enqueue 0.596729 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 7.23275 ms - Host latency: 8.85574 ms (end to end 14.242 ms, enqueue 0.681006 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.64624 ms - Host latency: 8.27343 ms (end to end 13.0955 ms, enqueue 0.607605 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.53099 ms - Host latency: 8.1858 ms (end to end 12.9144 ms, enqueue 0.610767 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.64891 ms - Host latency: 8.29613 ms (end to end 13.1906 ms, enqueue 0.662988 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.27289 ms - Host latency: 7.93711 ms (end to end 12.5342 ms, enqueue 0.618506 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.8066 ms - Host latency: 7.41813 ms (end to end 11.4771 ms, enqueue 0.52998 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.823 ms - Host latency: 7.43759 ms (end to end 11.5088 ms, enqueue 0.564673 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.86285 ms - Host latency: 7.47562 ms (end to end 11.558 ms, enqueue 0.528198 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.86779 ms - Host latency: 7.48116 ms (end to end 11.6114 ms, enqueue 0.568274 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.85741 ms - Host latency: 7.46847 ms (end to end 11.5896 ms, enqueue 0.42854 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.84595 ms - Host latency: 7.45909 ms (end to end 11.4994 ms, enqueue 0.528918 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.98035 ms - Host latency: 7.59518 ms (end to end 11.8029 ms, enqueue 0.530688 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.87512 ms - Host latency: 7.48781 ms (end to end 11.5779 ms, enqueue 0.527576 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.84382 ms - Host latency: 7.45811 ms (end to end 11.5468 ms, enqueue 0.538367 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.848 ms - Host latency: 7.46375 ms (end to end 11.5131 ms, enqueue 0.527783 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.83406 ms - Host latency: 7.44739 ms (end to end 11.4751 ms, enqueue 0.542749 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.86914 ms - Host latency: 7.5363 ms (end to end 11.5626 ms, enqueue 0.548877 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.27285 ms - Host latency: 7.95532 ms (end to end 12.3443 ms, enqueue 0.654736 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.63857 ms - Host latency: 8.30164 ms (end to end 13.1321 ms, enqueue 0.669434 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.4741 ms - Host latency: 8.09465 ms (end to end 12.8532 ms, enqueue 0.616186 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.39072 ms - Host latency: 8.00859 ms (end to end 12.5663 ms, enqueue 0.589209 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.12961 ms - Host latency: 7.74883 ms (end to end 12.1593 ms, enqueue 0.574219 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.81509 ms - Host latency: 7.45916 ms (end to end 11.4817 ms, enqueue 0.571094 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.81743 ms - Host latency: 7.45359 ms (end to end 11.496 ms, enqueue 0.542822 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.58547 ms - Host latency: 8.2011 ms (end to end 12.8887 ms, enqueue 0.540112 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.63198 ms - Host latency: 8.25303 ms (end to end 13.0713 ms, enqueue 0.608325 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.43938 ms - Host latency: 8.05864 ms (end to end 12.8024 ms, enqueue 0.608398 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.46799 ms - Host latency: 8.08796 ms (end to end 12.8032 ms, enqueue 0.650049 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 6.43411 ms - Host latency: 8.05098 ms (end to end 12.772 ms, enqueue 0.587231 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.83396 ms - Host latency: 7.44954 ms (end to end 11.4994 ms, enqueue 0.532324 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.84263 ms - Host latency: 7.45811 ms (end to end 11.4925 ms, enqueue 0.532349 ms)\n",
      "[02/18/2021-19:00:28] [I] Average on 10 runs - GPU latency: 5.83862 ms - Host latency: 7.45305 ms (end to end 11.5038 ms, enqueue 0.542261 ms)\n",
      "[02/18/2021-19:00:28] [I] Host Latency\n",
      "[02/18/2021-19:00:28] [I] min: 7.29041 ms (end to end 7.36298 ms)\n",
      "[02/18/2021-19:00:28] [I] max: 13.3104 ms (end to end 17.7831 ms)\n",
      "[02/18/2021-19:00:28] [I] mean: 7.85052 ms (end to end 12.2732 ms)\n",
      "[02/18/2021-19:00:28] [I] median: 7.47888 ms (end to end 11.7791 ms)\n",
      "[02/18/2021-19:00:28] [I] percentile: 9.63806 ms at 99% (end to end 14.1799 ms at 99%)\n",
      "[02/18/2021-19:00:28] [I] throughput: 0 qps\n",
      "[02/18/2021-19:00:28] [I] walltime: 3.0174 s\n",
      "[02/18/2021-19:00:28] [I] Enqueue Time\n",
      "[02/18/2021-19:00:28] [I] min: 0.254883 ms\n",
      "[02/18/2021-19:00:28] [I] max: 1.26636 ms\n",
      "[02/18/2021-19:00:28] [I] median: 0.558533 ms\n",
      "[02/18/2021-19:00:28] [I] GPU Compute\n",
      "[02/18/2021-19:00:28] [I] min: 5.66827 ms\n",
      "[02/18/2021-19:00:28] [I] max: 11.6919 ms\n",
      "[02/18/2021-19:00:28] [I] mean: 6.2215 ms\n",
      "[02/18/2021-19:00:28] [I] median: 5.86035 ms\n",
      "[02/18/2021-19:00:28] [I] percentile: 8.01794 ms at 99%\n",
      "[02/18/2021-19:00:28] [I] total compute time: 3.00498 s\n",
      "&&&& PASSED TensorRT.trtexec # trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine.trt --explicitBatch --fp16\n"
     ]
    }
   ],
   "source": [
    "# May need to shut down all kernels and restart before this - otherwise you might get cuDNN initialization errors:\n",
    "if USE_FP16:\n",
    "    !trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine.trt  --explicitBatch --fp16\n",
    "else:\n",
    "    !trtexec --onnx=resnet50_onnx_model.onnx --saveEngine=resnet_engine.trt  --explicitBatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-\n",
    "\n",
    "__The trtexec Logs:__\n",
    "\n",
    "Above, trtexec does a lot of things! Some important things to note:\n",
    "\n",
    "__First__, _\"PASSED\"_ is what you want to see in the last line of the log above. We can see our conversion was successful!\n",
    "\n",
    "__Second__, can see the resnet_engine.trt engine file has indeed been successfully created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 150628\n",
      "drwxrwxr-x  6 1000 1000      4096 Feb 18 19:00  .\n",
      "drwxr-xr-x  1 root root      4096 Feb 18 18:56  ..\n",
      "drwxr-xr-x  2 root root      4096 Feb 18 18:32  .ipynb_checkpoints\n",
      "-rw-rw-r--  1 1000 1000      3274 Oct 19 13:05 'Official TF-TRT Fixed.ipynb.zip'\n",
      "-rw-rw-r--  1 1000 1000      8860 Nov 15 23:42  README.md\n",
      "-rw-rw-r--  1 1000 1000     16971 Oct 19 13:05 'TF TRT Examples.zip'\n",
      "-rw-rw-r--  1 1000 1000     29195 Feb 18 18:59  TF2-ONNX-TRT.ipynb\n",
      "drwxr-xr-x  2 root root      4096 Oct  2 12:54  __pycache__\n",
      "-rw-rw-r--  1 1000 1000     15899 Jan 28 14:26  classify_trt.py\n",
      "-rw-rw-r--  1 1000 1000      9838 Jan 27 14:45  conv.py\n",
      "-rw-rw-r--  1 1000 1000      5468 Jan 28 16:56  convert.py\n",
      "-rw-rw-r--  1 1000 1000    136234 Mar 31  2020  devboard-power-data-co.jpg\n",
      "-rw-rw-r--  1 1000 1000    139978 Mar 31  2020  devboard-serial-power-co.jpg\n",
      "-rwxrwxrwx  1 1000 1000        98 Feb 18 18:53  dj\n",
      "-rwxrwxrwx  1 1000 1000        70 Feb 18 18:31  djb\n",
      "-rw-rw-r--  1 1000 1000      1206 Feb 18 18:50  dockerfile\n",
      "-rw-rw-r--  1 1000 1000      1118 Sep 30 16:07  engine.py\n",
      "drwxrwxr-x 12 1000 1000      4096 Feb  3 14:24  examples\n",
      "-rw-rw-r--  1 1000 1000     62046 Apr  1  2020  filezilla.png\n",
      "drwxr-xr-x  2 1000 1000      4096 Feb 18 18:13  images\n",
      "-rw-rw-r--  1 1000 1000      2712 Sep 28 15:14  inference.py\n",
      "-rwxrwxr-x  1 1000 1000    109848 Sep 19 13:49  inftrt\n",
      "-rw-rw-r--  1 1000 1000       288 Sep 18 15:29  inftrt.cpp\n",
      "-rw-rw-r--  1 1000 1000      5480 Nov 13 18:03  inftrt.py\n",
      "-rw-r--r--  1 root root 102145169 Feb 18 18:58  resnet50_onnx_model.onnx\n",
      "-rw-r--r--  1 root root  51372514 Feb 18 19:00  resnet_engine.trt\n",
      "-rw-rw-r--  1 1000 1000     69949 Apr  1  2020  sftp_nautilus.png\n",
      "-rw-rw-r--  1 1000 1000      5571 Nov 13 15:31  tfonxtrt.py\n",
      "-rw-rw-r--  1 1000 1000      5571 Feb  3 13:44  tfonxtrt01.py\n",
      "-rw-rw-r--  1 1000 1000      1990 Sep 22 16:06  trt.py\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Third__, you can see timing details above using trtexec - these are in the ideal case with no overhead. Depending on how you run your model, a considerable amount of overhead can be added to this. We can do timing in our Python runtime below - but keep in mind performing C++ inference would likely be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What TensorRT runtime am I targeting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to run our TensorRT inference in Python - so the TensorRT Python API is a great way of testing our model out in Jupyter, and is still quite performant.\n",
    "\n",
    "To use it, we need to do a few steps:\n",
    "\n",
    "__Load our engine into a tensorrt.Runtime:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dX2jFwrA8qu6"
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "f = open(\"resnet_engine.trt\", \"rb\")\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(f.read())\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if this cell is having issues, restarting all Jupyter kernels and rerunning only the batch size and precision cells above before trying again often helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Allocate input and output memory, give TRT pointers (bindings) to it:__\n",
    "\n",
    "d_input and d_output refer to the memory regions on our 'device' (aka GPU) - as opposed to memory on our normal RAM, where Python holds its variables (such as 'output' below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q3UJcdWy8qu8"
   },
   "outputs": [],
   "source": [
    "output = np.empty([BATCH_SIZE, 1000], dtype = target_dtype) # Need to set output dtype to FP16 to enable FP16\n",
    "\n",
    "# Allocate device memory\n",
    "d_input = cuda.mem_alloc(1 * dummy_input_batch.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set up prediction function:__\n",
    "\n",
    "This involves a copy from CPU RAM to GPU VRAM, executing the model, then copying the results back from GPU VRAM to CPU RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6R-F8JtV8qu-"
   },
   "outputs": [],
   "source": [
    "def predict(batch): # result gets copied into output\n",
    "    # Transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "    # Execute model\n",
    "    context.execute_async_v2(bindings, stream.handle, None)\n",
    "    # Transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    # Syncronize threads\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we need to run predictions using our TensorRT engine in a Python runtime!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we can see how quickly we can feed a singular batch to TensorRT, which we can compare to our original Tensorflow experiment from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AdKZzW7O8qvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Done warming up!\n"
     ]
    }
   ],
   "source": [
    "print(\"Warming up...\")\n",
    "\n",
    "predict(dummy_input_batch)\n",
    "\n",
    "print(\"Done warming up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the %%timeit Jupyter magic again. Note that %%timeit is fairly rough, and for any actual benchmarking better controlled testing is required - preferably outside of Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XAtWnCK38qvD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2 ms ± 7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pred = predict(dummy_input_batch) # Check TRT performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 14\n"
     ]
    }
   ],
   "source": [
    "print (\"Prediction: \" + str(np.argmax(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(dummy_input_batch)\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Profiling </h4>\n",
    "\n",
    "This is a great next step for further optimizing and debugging models you are working on productionizing\n",
    "\n",
    "You can find it here: https://docs.nvidia.com/deeplearning/tensorrt/best-practices/index.html\n",
    "\n",
    "<h4>  TRT Dev Docs </h4>\n",
    "\n",
    "Main documentation page for the ONNX, layer builder, C++, and legacy APIs\n",
    "\n",
    "You can find it here: https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\n",
    "\n",
    "<h4>  TRT OSS GitHub </h4>\n",
    "\n",
    "Contains OSS TRT components, sample applications, and plugin examples\n",
    "\n",
    "You can find it here: https://github.com/NVIDIA/TensorRT\n",
    "\n",
    "\n",
    "#### TRT Supported Layers:\n",
    "\n",
    "https://github.com/NVIDIA/TensorRT/tree/master/samples/opensource/samplePlugin\n",
    "\n",
    "#### TRT ONNX Plugin Example:\n",
    "\n",
    "https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#layers-precision-matrix\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ONNXExample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
