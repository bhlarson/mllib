{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLLib Development Notes\n",
    "\n",
    "Reverse time order - Newest on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 August 2022\n",
    "- A complete parameter weight or flops counter that can be back-propegated contains the relaxed input vector and relaxed output vector\n",
    "- Torch dynamically computes its network graph\n",
    "- Torch has nn.Module.children() but not .parnets() so you can walk forward in the network graph \n",
    "- Torch backpropegation must walk backwards through the graph\n",
    "- Can relxation data be passed through network during forward?\n",
    "- Can relaxation data be passed through the network using \"children\"\n",
    "- How to account for diverging input signals like the decoder portion of a U-NET - inputs are received from the pass-through connection and the previous connection?\n",
    "- Option 1 - include the relaxed vector in in Module.forward(x)\n",
    "    - This mean a single method of navigating the network\n",
    "    - Two types of data will be passed as x in the forward call\n",
    "    - Must explicitly run a forward call before computing loss - this is the standard pattern\n",
    "    - It looks like this isn't fesible with torch tensors and forward\n",
    "- Option 2 - Pass the relaxed vector through the graph using nn.Module.children()\n",
    "    - Need to know initial layer\n",
    "    - Can walk through layers and pass relaxation functions\n",
    "    - Requires explicit call and second handling of graph\n",
    "- Option 3- share weights between modules\n",
    "- Option 4 - Place the relxation into a seprate module and call it in muliple places\n",
    "- Torch.Tensor.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 August 2022\n",
    "- It looks like parameter sparcity is a poor proxy for model run time on UNET\n",
    "- FLOPS as objective rather than weights would take into account the image size at each layer\n",
    "- Parameter sparcity is not taking into account actual compute time\n",
    "- Results from crisplit_20220811_223202_hiocnn0 and crisplit_20220811_224009_hiocnn0 illustrate 97% parameter sparsity but only 2x speeed increase\n",
    "- This is explained by flops because the weight for the center & smallest image layers were pruned and weights for the larger layers were retained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 August 2022\n",
    "- Archicture reduction learning_rate 2e-4, k_structure 0.1, k_prune_sigma 1 \n",
    "- Maintained cross entropy loss and mean intersection/union until final pruning\n",
    "- Pruning slowed but did not stop as cross entropy loss and MIOU increased.\n",
    "- Cross entropy loss is the smallest loss: cross_entropy_loss: 0.007638514041900635 architecture_loss: 0.09289148449897766 prune_loss: 0.17779725790023804\n",
    "- Prune loss impacted and slowed archetecture loss curves\n",
    "- Failed train-fine due to out of memory error\n",
    "- Computed pruned size from size in previous step not from origonal size\n",
    "\n",
    "![crisplit_20220801h00_tb](../img/crisplit_20220801h00_tb_k_structure_01.png)\n",
    "\n",
    "- Execution order: Train->Prune->Test\n",
    "- Tests performed after pruning\n",
    "- Maintained accurcy for the first 2 pruning steps\n",
    "\n",
    "[normalized-train_cw]: ../img/crisplit_20220801h000_normalized-train_cw.png\n",
    "[train_cw]: ../img/crisplit_20220801h000_train_cw.png\n",
    "[ss_00_cw]: ../img/crisplit_20220801h000_search_structure_00_cw.png\n",
    "[ss_01_cw]: ../img/crisplit_20220801h000_search_structure_01_cw.png\n",
    "[ss_02_cw]: ../img/crisplit_20220801h000_search_structure_02_cw.png\n",
    "|  | normalized-train | train | Search 0 | Search 1 | Search 2 |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|Cross Entropy Loss|0.2146|0.0046 |0.0054 |0.002837|0.00763|\n",
    "|MIOU              |      |0.8762 |0.9055 |0.891740|0.85680|\n",
    "|Reamining Ratio   |1.0   |1.0    |0.0594 |0.033540|0.02967|\n",
    "| Prune Weights |![][normalized-train_cw]|![][train_cw] |![][ss_00_cw] |![][ss_01_cw]|![][ss_02_cw]|\n",
    "\n",
    "- Failed to prune effectively: k_structure: 0.05 k_prune_basis: 1.0 \n",
    "- Failed to prune effectively: k_structure: 0.01 k_prune_basis: 1.0\n",
    "- Failed to prune effectively: k_structure: 0.01 k_prune_basis: 0.5\n",
    "- Add  k_accuracy to increase cross entropy loss weight without decreasing k_structure and k_prune_sigma to the point they have a vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 July 2022\n",
    "- Yesterday's training froze at the end of the step \"search-structure-01\".  There was no error message and the pod was in the executing state\n",
    "- There were several failed S3 image reads during the run.\n",
    "- I killed the pod and restarted the run to see if the failure repeats.\n",
    "- I added ptflops flops counting library to the project\n",
    "- get_model_complexity_info is called from a few points and printed to stdout\n",
    "- Investigate the output in the debugger and include the network flops in the model output\n",
    "- Investigate replacing current model size counter with flops counter for loss computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 July 2022\n",
    "- Implemented multi-step training\n",
    "- Much faster - 5 days to 10 hours\n",
    "- Each step is 2x faster because I enable 1 device reading thread\n",
    "- About the same number of total steps\n",
    "- Accuracy maintained until final train\n",
    "- Final step removed sigmoid scaling without compensation\n",
    "- Initializing sigmoid to 0 and sech not pushing output to 1\n",
    "- Final train accuracy would likely match other accuracy with additional training\n",
    "- Infer time did not drop\n",
    "- Torch passes data to and from GPU on each layer - this will limit infer time\n",
    "- The batch size remained fixed through training - this will limit infer time\n",
    "\n",
    "![crisplit_20220723i01_tb](../img/crisplit_20220723i01_tb.png)\n",
    "![crisplit_20220723i01_tb_time](../img/crisplit_20220723i01_tb_time.png)\n",
    "\n",
    "- The search strcture image are from the previous step's pruning\n",
    "- crisplit_20220723i010_search_structure_00 remaining parameters 10288177/31037517 = 0.33147551719423946\n",
    "- crisplit_20220723i010_search_structure_01 remaining parameters 1749302/10288177 = 0.1700303173244395\n",
    "- crisplit_20220723i010_search_structure_02 remaining parameters 394959/1749302 = 0.2257809114721186\n",
    "- Pruned quickly even with k_prune_basis=0.1, k_structure=0.1, convMaskThreshold=0.01\n",
    "- Rarely push retained class weights to red\n",
    "- Only cross entropy loss settled\n",
    "- Architecture loss still dropping\n",
    "- prune loss still dripping\n",
    "![train](../img/crisplit_20220723i010_train_cw.png)\n",
    "![search_structure_00](../img/crisplit_20220723i010_search_structure_00_cw.png)\n",
    "![search_structure_01](../img/crisplit_20220723i010_search_structure_01_cw.png)\n",
    "![search_structure_02](../img/crisplit_20220723i010_search_structure_02_cw.png)\n",
    "![final](../img/crisplit_20220723i010_cw.png)\n",
    "\n",
    "- Maintain size comparison to initial network size, not previous pruned size\n",
    "- Push sigmoid output to 1\n",
    "- If not, compensate for sigmoid scale when pruning\n",
    "- Dynamically adjust batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 July 2022\n",
    "- Initialized pruning weight to 0 so architecture loss has maximum gradient and prune loss gradient is small\n",
    "- Quick convergence of architecture loss\n",
    "- Slow convergence of prune loss\n",
    "- Maintained good cross entropy loss throughout optimization \n",
    "\n",
    "![Tensorboard](../img/crisplit_20220709i_tb.png)\n",
    "\n",
    "- There is is a significant blue-shift for the prune_weights plot\n",
    "- This isn't necessarily a problem because there is a clear separation between the dark blue and other weights\n",
    "\n",
    "![Class Weights](../img/crisplit_20220709i0_train-fine_cw.png)\n",
    "\n",
    "- prune_weight moved quickly to blue (off) but not to red(on)\n",
    "- Setting target_structure to 0 means there is always a reward to reduce architecture loss\n",
    "- Try early pruning at ~ -2 sigma to speed training\n",
    "- Need to autoscale batch size to get full GPU speedup:L \n",
    "- Scale learning rate based on batch size:  [Learning Rates as a Function of Batch Size: A Random\n",
    "- Experiment with replacing the sigmoid function with a scale function for retained weights to improve the similarity between relaxed and pruned network \n",
    "- Push scale into previous network element to speed execution (e.g. convolution weights, batch norm scale)\n",
    "Matrix Theory Approach to Neural Network Training](https://arxiv.org/pdf/2006.09092.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 July 2022\n",
    "- Can prune_basis be applied 100% throughout train_fine relaxation? No\n",
    "- When I applied prune loss at epoch 1, any progress in architecture loss was reversed\n",
    "- I expect the gradient of prune loss is greater then the gradient of architecture reduction when they are applied simultaniously\n",
    "- I dropped k_prune_basis from 1.0 to 0.1 and had the same results where the architecure_loss and arcitecture_reduciton reverses\n",
    "![crisplit_20220708i0_tb](../img/crisplit_20220708i0_tb.png)\n",
    "\n",
    "- From [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980.pdf),\n",
    "\n",
    "![Adam Optimizer](../img/AdamAlgorithm.png)\n",
    "\n",
    "- Step is dependendend on gradient\n",
    "- The total gradient is a product of the gradients of each objective/regularization function\n",
    "- I could compute/display the gradient of each objective function independently\n",
    "- I expect the gradient of cross entropy loss to near 0 since this is a pre-trained network\n",
    "- The gradient of architecture loss is dependent on k_structure and the change in sigmoid(prune weights)\n",
    "- Since we start all prune weights at 1, this will be a small gradient\n",
    "- The gradient of prune_loss will be dependent on k_prune_basis and the gradient of gaussian(prune weights)\n",
    "- Initializing prune wights at 1 and k_prune_sigma=1, there will be a clear gradient pushing prune weights more positive\n",
    "- What happens if we initialize prune_weights to 0 rather then 1?\n",
    "- We start with architecture loss gradient at its maximum\n",
    "- We start with prune_loss with a small gradient but near an unstable equilibrium point\n",
    "- Lets see what happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from pymlutil.functions import Sigmoid, GaussianBasis\n",
    "x = np.arange(-2.0, 2.0, 0.01) \n",
    "y1 = Sigmoid(x, scale = 1.0, offset=0.0, k_exp = 5.0)\n",
    "plt.plot(x, y1)\n",
    "y2 = GaussianBasis(torch.tensor(x), sigma=0.5)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 July 2022\n",
    "- Removed batch norm layers from UNET\n",
    "- Training and pruning results were at least as good to network with batch norm\n",
    "- No clear advantage provided by batch norm for weight magnitude from impacting relaxation magnitude\n",
    "- crisplit_20220630i0_train\n",
    "    ```yaml\n",
    "    cross_entropy_loss: 0.0030129256192594767 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9977446895166078 \n",
    "            1: 0.8221591634441512 \n",
    "        average time: 0.003201360332294909 \n",
    "        miou: 0.9099519264803795 \n",
    "        num images: 963\n",
    "    ```\n",
    "- crisplit_20220630i0_train-fine\n",
    "    ```yaml\n",
    "    cross_entropy_loss: 0.002983461134135723 \n",
    "    architecture_loss: 0.005337885580956936 \n",
    "    prune_loss: 0.1341208666563034 \n",
    "    architecture_reduction: 0.05337885394692421 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9975066577436897 \n",
    "            1: 0.8136417113713045 \n",
    "        average time: 0.007769013499480788 \n",
    "        miou: 0.9055741845574972 \n",
    "        num images: 963\n",
    "    ```\n",
    "- crisplit_20220630i0_prune \n",
    "    ```yaml\n",
    "    prune: \n",
    "        final parameters: 593296 \n",
    "        initial parameters: 31037517 \n",
    "        remaining ratio: 0.019115446638337724 \n",
    "    training: \n",
    "        cross_entropy_loss: 0.0037698952946811914 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9975929543526758 \n",
    "            1: 0.8155541153342405 \n",
    "        average time: 0.0032667279335410193 \n",
    "        miou: 0.9065735348434582 \n",
    "        num images: 963\n",
    "    ```\n",
    "- No full-layer collapse of UNET which may have caused a deviation between full and relaxed network\n",
    "\n",
    "![crisplit_20220630i0_train-fine_cw.pn](../img/crisplit_20220630i0_train-fine_cw.png)\n",
    "![crisplit_20220630i0_prune_cw.png](../img/crisplit_20220630i0_prune_cw.png)\n",
    "\n",
    "- Architecture reduction never became 0\n",
    "- Possibility of additional pruning while maintaining accuracy\n",
    "- Prune loss continued to fall but with a falling rate\n",
    "- Possibility of improved relaxation with increased time in train_fine\n",
    "- increasing prune_basis did not appear to negatively impact architecture loss or cross_entropy loss\n",
    "- How sloppy can I be with the prune_basis?  \n",
    "- Can prune_basis be applied 100% throughout train_fine relaxation? \n",
    "- Would this slow architecture search?\n",
    "- This would eliminate several parameters\n",
    "- This would provide additional time to converge prune_loss\n",
    "- When relaxation is removed, the scale factor is not applied to the convolution weights.\n",
    "- If the convolution weight is 1 or 0, then there is a resulting shift that must be removed in post prune training\n",
    "- Can post prune training be shortened or removed?\n",
    "- Investigate changing prune epochs from 15 to 7 or 5\n",
    "- Investigate converting sigmoid function to a fixed scale multiple\n",
    "\n",
    "![crisplit_20220630i_tb.png](../img/crisplit_20220630i_tb.png)\n",
    "\n",
    "### From literature review\n",
    "- Can I demonstrate a linear reduction in time for structured pruning size reduction?\n",
    "- This time reduction will need to adjust batch size to benefit from very heavy pruning.\n",
    "- Can I demonstrate a different time reduction for unstructured pruning?\n",
    "- Is this independent from other neural network optimizations: NAS, parameter optimization, quantization, target hardware optimization\n",
    "- Structured pruning may be achieving similar compression ratios and accuracies as unstructured pruning.  \n",
    "- If so, how?\n",
    "- Quantify performance advantages of structured pruning\n",
    "- What differences are there to the Lottery Ticket Hypothesis?\n",
    "- Is there a more appropriate theoretical basis?\n",
    "- Is there a theoretical basis independent of an approximation function?\n",
    "- Are there any useful conclusions that can be made from this?\n",
    "- Is the final pruning dependent on the initial state as is commonly assumed in the lottery ticket hypothesis\n",
    "- What are the advantages/disadvantages of relaxation vs magnitude pruning?\n",
    "- Explore simultaneous training/pruning\n",
    "- Training/pruning from random variables\n",
    "- Training/pruning from pretrained network\n",
    "- Training/pruning from transfer learning\n",
    "- How to speed pytorch training?  \n",
    "- It seems much slower than Tensorflow training.  \n",
    "- This may be due to FCN vs UNET not PyTorch vs Tensorflow or Tensorflow optimized data pipeline vs my Pytorch S3 data pulling.\n",
    "- How robust is a pruned model in retraining?\n",
    "- How robust is a pruned model to training in a new domain?\n",
    "- Does the model converge to an equivalent solution?  \n",
    "- How to deter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
