{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLLib Development Notes\n",
    "\n",
    "Reverse time order - Newest on top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 July 2022\n",
    "- Removed batch norm layers from UNET\n",
    "- Training and pruning results were at least as good to network with batch norm\n",
    "- No clear advantage provided by batch norm for weight magnitude from impacting relaxation magnitude\n",
    "- crisplit_20220630i0_train\n",
    "    ```yaml\n",
    "    cross_entropy_loss: 0.0030129256192594767 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9977446895166078 \n",
    "            1: 0.8221591634441512 \n",
    "        average time: 0.003201360332294909 \n",
    "        miou: 0.9099519264803795 \n",
    "        num images: 963\n",
    "    ```\n",
    "- crisplit_20220630i0_train-fine\n",
    "    ```yaml\n",
    "    cross_entropy_loss: 0.002983461134135723 \n",
    "    architecture_loss: 0.005337885580956936 \n",
    "    prune_loss: 0.1341208666563034 \n",
    "    architecture_reduction: 0.05337885394692421 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9975066577436897 \n",
    "            1: 0.8136417113713045 \n",
    "        average time: 0.007769013499480788 \n",
    "        miou: 0.9055741845574972 \n",
    "        num images: 963\n",
    "    ```\n",
    "- crisplit_20220630i0_prune \n",
    "    ```yaml\n",
    "    prune: \n",
    "        final parameters: 593296 \n",
    "        initial parameters: 31037517 \n",
    "        remaining ratio: 0.019115446638337724 \n",
    "    training: \n",
    "        cross_entropy_loss: 0.0037698952946811914 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9975929543526758 \n",
    "            1: 0.8155541153342405 \n",
    "        average time: 0.0032667279335410193 \n",
    "        miou: 0.9065735348434582 \n",
    "        num images: 963\n",
    "    ```\n",
    "- No full-layer collapse of UNET which may have caused a deviation between full and relaxed network\n",
    "![crisplit_20220630i0_train-fine_cw.pn](../img/crisplit_20220630i0_train-fine_cw.png)\n",
    "![crisplit_20220630i0_prune_cw.png](../img/crisplit_20220630i0_prune_cw.png)\n",
    "- Architecture reduction never became 0\n",
    "- Possibility of additional pruning while maintaining accuracy\n",
    "- Prune loss continued to fall but with a falling rate\n",
    "- Possibility of improved relaxation with increased time in train_fine\n",
    "- increasing prune_basis did not appear to negatively impact architecture loss or cross_entropy loss\n",
    "- How sloppy can I be with the prune_basis?  \n",
    "- Can prune_basis be applied 100% throughout train_fine relaxation? \n",
    "- Would this slow architecture search?\n",
    "- This would eliminate several parameters\n",
    "- This would provide additional time to converge prune_loss\n",
    "- When relaxation is removed, the scale factor is not applied to the convolution weights.\n",
    "- If the convolution weight is 1 or 0, then there is a resulting shift that must be removed in post prune training\n",
    "- Can post prune training be shortened or removed?\n",
    "- Investigate changing prune epochs from 15 to 7 or 5\n",
    "- Investigate converting sigmoid function to a fixed scale multiple\n",
    "\n",
    "![crisplit_20220630i_tb.png](../img/crisplit_20220630i_tb.png)\n",
    "\n",
    "### From literature review\n",
    "- Can I demonstrate a linear reduction in time for structured pruning size reduction?\n",
    "- This time reduction will need to adjust batch size to benefit from very heavy pruning.\n",
    "- Can I demonstrate a different time reduction for unstructured pruning?\n",
    "- Is this independent from other neural network optimizations: NAS, parameter optimization, quantization, target hardware optimization\n",
    "- Structured pruning may be achieving similar compression ratios and accuracies as unstructured pruning.  \n",
    "- If so, how?\n",
    "- Quantify performance advantages of structured pruning\n",
    "- What differences are there to the Lottery Ticket Hypothesis?\n",
    "- Is there a more appropriate theoretical basis?\n",
    "- Is there a theoretical basis independent of an approximation function?\n",
    "- Are there any useful conclusions that can be made from this?\n",
    "- Is the final pruning dependent on the initial state as is commonly assumed in the lottery ticket hypothesis\n",
    "- What are the advantages/disadvantages of relaxation vs magnitude pruning?\n",
    "- Explore simultaneous training/pruning\n",
    "- Training/pruning from random variables\n",
    "- Training/pruning from pretrained network\n",
    "- Training/pruning from transfer learning\n",
    "- How to speed pytorch training?  \n",
    "- It seems much slower than Tensorflow training.  \n",
    "- This may be due to FCN vs UNET not PyTorch vs Tensorflow or Tensorflow optimized data pipeline vs my Pytorch S3 data pulling.\n",
    "- How robust is a pruned model in retraining?\n",
    "- How robust is a pruned model to training in a new domain?\n",
    "- Does the model converge to an equivalent solution?  \n",
    "- How to deter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
