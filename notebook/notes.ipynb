{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b409663-308c-43dc-8eb6-0cd92927c070",
   "metadata": {},
   "source": [
    "# Test Notes in Jupyter\n",
    "\n",
    "I am looking for a method to keep an electronic design notebook where I can keep a design log, live evqations, plots, and drawings.  Here I am trying to do this in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a352506-c467-4dc5-a886-b16d9562ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1300c909-1929-4de1-87b2-37d22803d2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\int e^{x} \\cos{\\left(x \\right)}\\, dx = \\frac{e^{x} \\sin{\\left(x \\right)}}{2} + \\frac{e^{x} \\cos{\\left(x \\right)}}{2}$"
      ],
      "text/plain": [
       "Eq(Integral(exp(x)*cos(x), x), exp(x)*sin(x)/2 + exp(x)*cos(x)/2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = symbols('x')\n",
    "a = Integral(cos(x)*exp(x), x)\n",
    "Eq(a, a.doit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06d3b938-66a1-43a2-8690-4032e78311c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 2*np.pi, 0.01) \n",
    "y = np.sin(x) \n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1c670",
   "metadata": {},
   "source": [
    "4 January 2022\n",
    "- cell2d is no longer optimizing the network to minimize convolution norm\n",
    "- I suspect that summing convolution weights is losing the tensor gradient \n",
    "- See [Pytorch: backpropagating from sum of matrix elements to leaf variable answer](https://stackoverflow.com/questions/55942423/pytorch-backpropagating-from-sum-of-matrix-elements-to-leaf-variable)\n",
    "- Next: build up architecture loss concatenating tensors from lower networl level: c = torch.cat([a,b])\n",
    "- Next: solve architecture level using torch sum: d = torch.sum(c)\n",
    "- Next: verify gradient throughout operation\n",
    "- Next: test convolution minimization\n",
    "- Next: enable residual bypass of a specific level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695d9f0-6f06-4fa4-979a-55b2ddc389d7",
   "metadata": {},
   "source": [
    "6 January 2022\n",
    "- Pruning successful\n",
    "- Failied to run with pruned network nas_20220104_01:\\\n",
    "- Exception has occurred: RuntimeError       (note: full exception trace is shown but execution is paused at: <module>)\n",
    "Given groups=1, weight of size [982, 1023, 1, 1], expected input[256, 1024, 8, 8] to have 1023 channels, but got 1024 channels instead\n",
    "- The pruned convolution size is not fully propegated to the next convolution from Cell->ConvBR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d169c1a",
   "metadata": {},
   "source": [
    "7 January 2022\n",
    "- Test cross entropy loss is ~ 0.01\n",
    "- Architecture loss is ~ 0.01\n",
    "- The architecture reduction pushing from 0.1 to 0.01 requests a much smaller model but has a very small loss\n",
    "- Change from mean squared error to absolute error\n",
    "<br /> ![Tensorboard](../img/Tensorboard_nas_20220106_01.png) <br />\n",
    "- How to boost baseline accuracy to the state of the art? \n",
    "- [Cutmix](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.pdf)\n",
    "- [Attentive CutMix](https://arxiv.org/pdf/2003.13048.pdf)\n",
    "- [Label smoothing](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf)\n",
    "- [Mish activation function](https://arxiv.org/pdf/1908.08681v3.pdf)\n",
    "- Comminting out Cell::ApplyStructure enabled training following pruning: <br />\n",
    "            #if self.convolutions[-1]['out_channels'] == self.in1_channels+self.in2_channels:\n",
    "            #    self.conv_residual = None\n",
    "- On second ApplyStructure call, I get the following error: <br />\n",
    "  File \"networks/cell2d.py\", line 164, in ApplyStructure\n",
    "    raise ValueError(\"len(out_channel_mask)={} must be equal to self.out_channels={}\".format(len(out_channel_mask), self.out_channels))\n",
    "ValueError: len(out_channel_mask)=253 must be equal to self.out_channels=256\n",
    "- Find why self.out_channels is not updated in second training?\n",
    "- Found and fixed incorrect self.out_channels propagation on pruned cells\n",
    "- Training after prune resulted in all of the higher level convolutions being pruned out.\n",
    "- With higher levels pruned, the test accuracy peeked at about 50% rather than ~ 80%\n",
    "<br /> ![nas_20220106_03 training Tensorboard](../img/class_weights_nas_20220106_03.png) <br />\n",
    "<br /> ![nas_20220106_03 training Tensorboard](../img/TrainingAfterPrune_nas_20220106_03.png) <br />\n",
    "- Why did the test accuracy peek earlier in a pruned network while the training accuracy continued to climb with just the lower level convolutions?\n",
    "- What in the deeper network structure enables the training and test accuracies to track better?\n",
    "- Can I capture that good to keep the training and test accuracy together for longer?\n",
    "- Try reducing size by reducing convolutions but not removing layers.\n",
    "- Removing layer removal resulted in a significant decay of the middle layers and a similar test accuracy before pruning.  Test loss diverged from training loss after about iteration 4000\n",
    "<br /> ![nas_20220107_00 training NAS sweights](../img/nas_20220107_00_cw.png <br />\n",
    "<br /> ![nas_20220107_00 training Tensorboard](../img/nas_20220107_00_tb.png) <br />\n",
    "- Runtime error:\n",
    "```console\n",
    "Total Trainable Params: 22467463\n",
    "Reduced parameters 22467463/38108762 = 0.5895616079052896\n",
    "Train steps:   0%|                                                                                                                                                             | 0/250.0 [00:01<?, ?it/s]\n",
    "Train epochs:   0%|                                                                                                                                                               | 0/50 [00:01<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 987, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 886, in Test\n",
    "    outputs = classify(inputs)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 634, in forward\n",
    "    x = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 466, in forward\n",
    "    y = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
    "    return F.linear(input, self.weight, self.bias)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 1947, in linear\n",
    "    return torch._C._nn.linear(input, weight, bias)\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x1449 and 2048x10)\n",
    "PlotSearch finish\n",
    "```\n",
    "- Added dropout improved cross entropy loss compared with nas_20220107_00\n",
    "<br /> ![nas_20220107_dropout_00 training weights](../img/nas_20220107_dropout_00_cw.png) <br />\n",
    "<br /> ![nas_20220107_dropout_00 Tensorboard](../img/nas_20220107_dropout_00_tb.png) <br />\n",
    "- Try pruning and training.\n",
    "- Failed running following pruning\n",
    "```cmd\n",
    "Total Trainable Params: 27624300\n",
    "Reduced parameters 27624300/38108762 = 0.7248805405958871\n",
    "Train steps:   0%|                                                                                                                                                             | 0/250.0 [00:01<?, ?it/s]\n",
    "Train epochs:   0%|                                                                                                                                                               | 0/50 [00:01<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 993, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 892, in Test\n",
    "    outputs = classify(inputs, isTraining=True)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 639, in forward\n",
    "    x = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 471, in forward\n",
    "    y = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
    "    return F.linear(input, self.weight, self.bias)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 1947, in linear\n",
    "    return torch._C._nn.linear(input, weight, bias)\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x1712 and 2048x10)\n",
    "PlotSearch finish\n",
    "```\n",
    "- Pass the input size to FC::ApplyStructure from Classify::ApplyStructure\n",
    "- Initial training following pruning preserved accuracy:\n",
    "```cmd\n",
    "Total Trainable Params: 27620940\n",
    "Reduced parameters 27620940/38108762 = 0.724792371895996\n",
    "Train epochs:   0%|                                                                                                                                                                                        | 0/50 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.l1_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.945000 test accuracy=0.845000 training loss=1.87416e+00, test loss=2.21001e+00 arcitecture_reduction: 5.44080e-01                                                                          \n",
    "Test [1, 10.000000] training accuracy=0.905000 test accuracy=0.845000 training loss=1.83940e+00, test loss=2.13547e+00 arcitecture_reduction: 5.43983e-01 \n",
    "```\n",
    "- Will test cross entropy loss continue to improve?\n",
    "- Can this be repeated with future training and cropping?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034e5b3",
   "metadata": {},
   "source": [
    "8 January 2022\n",
    "- Training for ~ 200 epochs with droput did not result in a rise in training loss but it did result in a small increase in cross-entropy loss\n",
    "<br /> ![nas_20220107_dropout_00 training weights](../img/nas_20220107_dropout_03_tb.png) <br />\n",
    "- There was only a small decrease in network size.  Cross entropy loss is half the magnitude of architecture loss.\n",
    "<br /> ![nas_20220107_dropout_00 Tensorboard](../img/nas_20220107_dropout_03_cw.png) <br />\n",
    "- I will prune and restart training with 0.1 architecture loss vs the current 0.2.\n",
    "- Will this change the the network pruning?\n",
    "- Will this cause test accuracy to decrease?\n",
    "- Starting nas_20220108_00 with dropout rate of 0.1 and batch norm enabled\n",
    "- Initial network:\n",
    "```cmd\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|         cells.0.cnn.0.conv.weight         |    192     |\n",
    "|          cells.0.cnn.0.conv.bias          |     64     |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|         cells.0.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.0.cnn.1.conv.bias          |     64     |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|         cells.0.cnn.2.conv.weight         |   16384    |\n",
    "|          cells.0.cnn.2.conv.bias          |    256     |\n",
    "|      cells.0.cnn.2.batchnorm2d.weight     |    256     |\n",
    "|       cells.0.cnn.2.batchnorm2d.bias      |    256     |\n",
    "|     cells.0.conv_residual.conv.weight     |    768     |\n",
    "|      cells.0.conv_residual.conv.bias      |    256     |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|         cells.1.cnn.0.conv.weight         |   16384    |\n",
    "|          cells.1.cnn.0.conv.bias          |     64     |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.1.conv.bias          |     64     |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|         cells.1.cnn.2.conv.weight         |   16384    |\n",
    "|          cells.1.cnn.2.conv.bias          |    256     |\n",
    "|      cells.1.cnn.2.batchnorm2d.weight     |    256     |\n",
    "|       cells.1.cnn.2.batchnorm2d.bias      |    256     |\n",
    "|     cells.1.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.1.conv_residual.conv.bias      |    256     |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|         cells.2.cnn.0.conv.weight         |   16384    |\n",
    "|          cells.2.cnn.0.conv.bias          |     64     |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|         cells.2.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.2.cnn.1.conv.bias          |     64     |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|         cells.2.cnn.2.conv.weight         |   16384    |\n",
    "|          cells.2.cnn.2.conv.bias          |    256     |\n",
    "|      cells.2.cnn.2.batchnorm2d.weight     |    256     |\n",
    "|       cells.2.cnn.2.batchnorm2d.bias      |    256     |\n",
    "|     cells.2.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.2.conv_residual.conv.bias      |    256     |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|         cells.3.cnn.0.conv.weight         |   32768    |\n",
    "|          cells.3.cnn.0.conv.bias          |    128     |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.3.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.3.cnn.1.conv.bias          |    128     |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.3.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.3.cnn.2.conv.bias          |    512     |\n",
    "|      cells.3.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.3.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.3.conv_residual.conv.weight     |   131072   |\n",
    "|      cells.3.conv_residual.conv.bias      |    512     |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|         cells.4.cnn.0.conv.weight         |   65536    |\n",
    "|          cells.4.cnn.0.conv.bias          |    128     |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.4.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.4.cnn.1.conv.bias          |    128     |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.4.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.4.cnn.2.conv.bias          |    512     |\n",
    "|      cells.4.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.4.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.4.conv_residual.conv.weight     |   262144   |\n",
    "|      cells.4.conv_residual.conv.bias      |    512     |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|         cells.5.cnn.0.conv.weight         |   65536    |\n",
    "|          cells.5.cnn.0.conv.bias          |    128     |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.5.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.5.cnn.1.conv.bias          |    128     |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.5.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.5.cnn.2.conv.bias          |    512     |\n",
    "|      cells.5.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.5.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.5.conv_residual.conv.weight     |   262144   |\n",
    "|      cells.5.conv_residual.conv.bias      |    512     |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|         cells.6.cnn.0.conv.weight         |   65536    |\n",
    "|          cells.6.cnn.0.conv.bias          |    128     |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.6.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.6.cnn.1.conv.bias          |    128     |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.6.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.6.cnn.2.conv.bias          |    512     |\n",
    "|      cells.6.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.6.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.6.conv_residual.conv.weight     |   262144   |\n",
    "|      cells.6.conv_residual.conv.bias      |    512     |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|         cells.7.cnn.0.conv.weight         |   131072   |\n",
    "|          cells.7.cnn.0.conv.bias          |    256     |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.7.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.7.cnn.1.conv.bias          |    256     |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.7.cnn.2.conv.weight         |   262144   |\n",
    "|          cells.7.cnn.2.conv.bias          |    1024    |\n",
    "|      cells.7.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|       cells.7.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.7.conv_residual.conv.weight     |   524288   |\n",
    "|      cells.7.conv_residual.conv.bias      |    1024    |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|         cells.8.cnn.0.conv.weight         |   262144   |\n",
    "|          cells.8.cnn.0.conv.bias          |    256     |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.8.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.8.cnn.1.conv.bias          |    256     |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.8.cnn.2.conv.weight         |   262144   |\n",
    "|          cells.8.cnn.2.conv.bias          |    1024    |\n",
    "|      cells.8.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|       cells.8.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.8.conv_residual.conv.weight     |  1048576   |\n",
    "|      cells.8.conv_residual.conv.bias      |    1024    |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|         cells.9.cnn.0.conv.weight         |   262144   |\n",
    "|          cells.9.cnn.0.conv.bias          |    256     |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.9.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.9.cnn.1.conv.bias          |    256     |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.9.cnn.2.conv.weight         |   262144   |\n",
    "|          cells.9.cnn.2.conv.bias          |    1024    |\n",
    "|      cells.9.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|       cells.9.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.9.conv_residual.conv.weight     |  1048576   |\n",
    "|      cells.9.conv_residual.conv.bias      |    1024    |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|         cells.10.cnn.0.conv.weight        |   262144   |\n",
    "|          cells.10.cnn.0.conv.bias         |    256     |\n",
    "|     cells.10.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.10.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.10.cnn.1.conv.bias         |    256     |\n",
    "|     cells.10.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.10.cnn.2.conv.weight        |   262144   |\n",
    "|          cells.10.cnn.2.conv.bias         |    1024    |\n",
    "|     cells.10.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|      cells.10.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.10.conv_residual.conv.weight    |  1048576   |\n",
    "|      cells.10.conv_residual.conv.bias     |    1024    |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|         cells.11.cnn.0.conv.weight        |   262144   |\n",
    "|          cells.11.cnn.0.conv.bias         |    256     |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.11.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.11.cnn.1.conv.bias         |    256     |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.11.cnn.2.conv.weight        |   262144   |\n",
    "|          cells.11.cnn.2.conv.bias         |    1024    |\n",
    "|     cells.11.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|      cells.11.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.11.conv_residual.conv.weight    |  1048576   |\n",
    "|      cells.11.conv_residual.conv.bias     |    1024    |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|         cells.12.cnn.0.conv.weight        |   262144   |\n",
    "|          cells.12.cnn.0.conv.bias         |    256     |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.12.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.12.cnn.1.conv.bias         |    256     |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.12.cnn.2.conv.weight        |   262144   |\n",
    "|          cells.12.cnn.2.conv.bias         |    1024    |\n",
    "|     cells.12.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|      cells.12.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.12.conv_residual.conv.weight    |  1048576   |\n",
    "|      cells.12.conv_residual.conv.bias     |    1024    |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|         cells.13.cnn.0.conv.weight        |   524288   |\n",
    "|          cells.13.cnn.0.conv.bias         |    512     |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|         cells.13.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.13.cnn.1.conv.bias         |    512     |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|         cells.13.cnn.2.conv.weight        |  1048576   |\n",
    "|          cells.13.cnn.2.conv.bias         |    2048    |\n",
    "|     cells.13.cnn.2.batchnorm2d.weight     |    2048    |\n",
    "|      cells.13.cnn.2.batchnorm2d.bias      |    2048    |\n",
    "|     cells.13.conv_residual.conv.weight    |  2097152   |\n",
    "|      cells.13.conv_residual.conv.bias     |    2048    |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |    2048    |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |    2048    |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|         cells.14.cnn.0.conv.weight        |  1048576   |\n",
    "|          cells.14.cnn.0.conv.bias         |    512     |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|         cells.14.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.14.cnn.1.conv.bias         |    512     |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|         cells.14.cnn.2.conv.weight        |  1048576   |\n",
    "|          cells.14.cnn.2.conv.bias         |    2048    |\n",
    "|     cells.14.cnn.2.batchnorm2d.weight     |    2048    |\n",
    "|      cells.14.cnn.2.batchnorm2d.bias      |    2048    |\n",
    "|     cells.14.conv_residual.conv.weight    |  4194304   |\n",
    "|      cells.14.conv_residual.conv.bias     |    2048    |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |    2048    |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |    2048    |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|         cells.15.cnn.0.conv.weight        |  1048576   |\n",
    "|          cells.15.cnn.0.conv.bias         |    512     |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|         cells.15.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.15.cnn.1.conv.bias         |    512     |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|         cells.15.cnn.2.conv.weight        |  1048576   |\n",
    "|          cells.15.cnn.2.conv.bias         |    2048    |\n",
    "|     cells.15.cnn.2.batchnorm2d.weight     |    2048    |\n",
    "|      cells.15.cnn.2.batchnorm2d.bias      |    2048    |\n",
    "|     cells.15.conv_residual.conv.weight    |  4194304   |\n",
    "|      cells.15.conv_residual.conv.bias     |    2048    |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |    2048    |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |    2048    |\n",
    "|                fc.fc.weight               |   20480    |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 38108762\n",
    "Test [1, 5.000000] training accuracy=0.095000 test accuracy=0.100000 training loss=7.45281e+00, test loss=4.40786e+00 arcitecture_reduction: 5.36742e-01 \n",
    "```\n",
    "- At 0.1 dropout probability, cross entropy loss rises after 4000k similar to without dropout\n",
    "- Arcitecture reduction may be more effective at 0.1 dropout probability that 0.2\n",
    "<br /> ![nas_20220107_dropout_00 Tensorboard](../img/nas_20220108_00_tb.png) <br />\n",
    "<br /> ![nas_20220107_dropout_00 cell weights](../img/nas_20220108_00_cw.png) <br />\n",
    "- Try again with 0.3 dropout probability \n",
    "- I think minimizing convolutions is more effective for 3x3 convolutions and less on 1x1 convolutions.  Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0e34e",
   "metadata": {},
   "source": [
    "10 January 2022\n",
    "- Add a scaler function to enable/disable each feature-space convolution so the size minimization can be performed by minimizing a single value\n",
    "- sigmoid(value)*convolution channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3438b5b-2a15-4a92-b631-83af29e74147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhXklEQVR4nO3deXxU9b3/8dcnCQmQBMjGThJQFpGdiKh1q1bBWrDXpdBa1xbbW6y9aivWX63X29tr9Vpva23VW1e8grjTisVa1/aqEPbNQNgTtmyE7Ov398dMvGMMZCCTnJnJ+/l45JEz55yZvDkzeXNy5sz5mnMOERGJfDFeBxARkdBQoYuIRAkVuohIlFChi4hECRW6iEiUiPPqB6enp7vs7GyvfryISERatWpVsXMuo61lnhV6dnY2ubm5Xv14EZGIZGa7j7ZMh1xERKKECl1EJEqo0EVEokS7hW5mT5rZITPbeJTlZma/NbN8M1tvZlNCH1NERNoTzB7608CMYyyfCYz0f80D/tDxWCIicrzaLXTn3AdA6TFWmQ0863w+BvqZ2aBQBRQRkeCE4hj6EGBvwO0C/7wvMLN5ZpZrZrlFRUUh+NEiItKiS89Dd849DjwOkJOTo+v2ikjUcs5RUddIcUUdJVX1FFfUUez/fsEp/ZkwtF/If2YoCr0QGBZwe6h/nohIVHHOUVpVz6GKOoor6yiprKe4so5i//cS/3RJpa+86xub23ycjOSEsC30pcB8M1sMnA6UO+f2h+BxRUS6nHOOoso6dhVXs6ukit0lVQHT1VTWNX7hPj1ijbTEBNKT40lLTGDUgGTSk+JJT0ogLeB7RlICKYnx9IjtnDPG2y10M1sEnAekm1kB8HOgB4Bz7lFgGXAJkA9UA9d3SlIRkRBxzlFUUcfOYl9J7wwo7t0lVVTVN322bmyMMSylF1lpiZyWnUpmam8G9u1JWmI86ckJpCcm0KdXHGbm4b/Ip91Cd87NbWe5A34QskQiIiHknGNvaQ2r9pSSu6uMtXsPs7O4iuqA0o6LMYal9iY7rTfThqcyPD2RrLTeZKclMiSlV6ftUYeaZxfnEhHpDHWNTWwsPMLq3WXk7i5l1e7DFFfWAZCUEMekYf2YdlpLaScyPC2Rwf16EhchpX0sKnQRiWjFlXWs2l3mL/AyNhSUU9/kezMyM7U354xMZ0pWClOzUhg1IJnYGO8PjXQWFbqIRAznHPmHKlmxq/SzEt9VUg1AfGwM44b04bqzspmSmcKUrH70T+7pceKupUIXkbBXUFbN62v38dqaQrYdqgQgPSmeKZkpfPP0TKZmpXDq4L707BHrcVJvqdBFJCwdrq7njQ37eX3NPlbs8l195LTsFP7tsnGcMzKdzNTeYXFmSThRoYtI2KhtaOJvWw7x2tpC3ss7REOT4+T+Sfz44tHMmjiYYam9vY4Y1lToIuKppmbHJztKeHVNIX/ZeICKukb6Jydw3ZnZzJ40hFMH99GeeJBU6CLS5ZxzbNp3hNfXFrJ03T4OHqkjKSGOGeMG8vXJQ5g+Ii2qz0bpLCp0EekyFbUNPPfxHl5ZXcC2Q5XExRjnje7Pzy4dzIWnDOj2b2p2lApdRDpdfWMzz3+ym9++k09pVT05WSn84rJxfHX8IFIS472OFzVU6CLSaZxzvLFhPw8sz2N3STVnnpTGnTNPYfzQvl5Hi0oqdBHpFB9tL+G+N7ewrqCcMQOTefr60zh3VIbe4OxEKnQRCam8AxXc9+YW3s0rYlDfnvznlRP5+uQhepOzC6jQRSQk9pfX8Ou3tvLy6gISE+JYMHMM152ZrTc6u5AKXUQ6pLymgUff386Tf9+Jc3DDWcP5wfkn681OD6jQReSE1DU28dzHe3j4nW0crm7gskmDue2i0fo0p4dU6CJyXJqbHX9av48HludRUFbD2SPTuWPGGMYN0ZkrXlOhi0jQ8g9V8i8vrGVDYTmnDOrDszeM55xRGV7HEj8VuogE5a1NB7h1yToS4mJ46BsTmT1xCDE6cyWsqNBF5Jiamh3/9fZWHn4nnwlD+/Lo1VMZ3K+X17GkDSp0ETmq8uoGbnlhDe/lFXFVzlDunT1OpyGGMRW6iLQp70AF8xbmsu9wDb+4bBzfOj1Tn/IMcyp0EfmCP6/fx49fXE9SzzgWz5vO1KxUryNJEFToIvKZxqZmHliex2Mf7GBqVgp/+NYU+vfpXgMtRzIVuogAUFpVz82LVvOP/BK+PT2Ln106lvi4GK9jyXFQoYsIGwvLuWnhKooq67j/iglclTPM60hyAlToIt3cK6sLuPOVDaQlxvPiTWcwcVg/ryPJCVKhi3RTDU3N/PsbW3j6f3cxfUQqv/vmFNKTEryOJR2gQhfphooq6vjB86tZsbOUG780nDtnjiEuVsfLI50KXaSbWbOnjO8/t5rDNfX8Zs4kZk8a4nUkCREVukg38l7eIeY9u4oBfRN45ftnMXZwH68jSQgF9TeWmc0wszwzyzezBW0szzSzd81sjZmtN7NLQh9VRDpi1e4yvvfcKk7un8TSH3xJZR6F2i10M4sFHgFmAmOBuWY2ttVq/w9Y4pybDMwBfh/qoCJy4vIOVHDD0ysZ2Kcnz9wwTaMJRalg9tCnAfnOuR3OuXpgMTC71ToOaPnvvi+wL3QRRaQj9pZWc82Tn5AQF8PCG08nI1lnskSrYAp9CLA34HaBf16ge4CrzawAWAbc3NYDmdk8M8s1s9yioqITiCsix6Oooo5vP/EJNfVNLLzxdA0PF+VCdZ7SXOBp59xQ4BJgoZl94bGdc48753KcczkZGRrlRKQzHalt4LqnVnDgSC1PXX8aowcmex1JOlkwhV4IBH4OeKh/XqAbgSUAzrmPgJ5AeigCisjxq21o4rvP5JJ3oII/XD1VV0vsJoIp9JXASDMbbmbx+N70XNpqnT3ABQBmdgq+QtcxFREPNDY1c/OiNazYVcqDV03k/NH9vY4kXaTdQnfONQLzgeXAFnxns2wys3vNbJZ/tduA75rZOmARcJ1zznVWaBFpm3OOO1/ZwF83H+Ser52qDw11M0F9sMg5twzfm52B8+4OmN4MnBXaaCJyvO5781NeXFXALReM5Nozs72OI11MF28QiRKPvr+dxz7YwTVnZPGjC0d6HUc8oEIXiQJLVu7lvjc/5WsTB3PP107V2J/dlApdJMIt33SABa+s55xRGTx45URiYlTm3ZUKXSSCfbS9hJsXrWHisH48evUUDRnXzenZF4lQGwvL+e6zuWSn9eap606jd7wuntrdqdBFItCOokqufXIFfXv14NkbTqdfb11sS1ToIhHnQHkt335iBQALb5zGwL49PU4k4UJ/o4lEkPKaBq558hPKaxpYPG86IzKSvI4kYUR76CIRwjnHT1/ZwI6iKh6/ZirjhvT1OpKEGRW6SIRYtGIvb2zYz+0Xj+bMk3TtO/kiFbpIBMg7UMG//mkTZ49MZ97ZI7yOI2FKhS4S5mrqm7h50WqSe/bg11dN0geH5Kj0pqhImLv3z5vZerCShTdO0/BxckzaQxcJY2+s38+iFXv43rkncfZIjfIlx6ZCFwlTe0urWfDKeiYN68dtF43yOo5EABW6SBhq8I86hIOH506mR6x+VaV9OoYuEoYefGsra/ce5pFvTmFYam+v40iE0H/7ImHmg61FPPr+duZOy+SrEwZ5HUciiApdJIwcqqjl1iVrGTUgibsvHet1HIkwOuQiEiaamx23LVlHRW0j//Od6fSKj/U6kkQY7aGLhInHP9zBh9uK+fnXTmX0wGSv40gEUqGLhIHVe8r4z+V5fHX8IOZOG+Z1HIlQKnQRj5XXNPDDRWsY0Kcnv/yn8RrgWU6YjqGLeKjlkrj7y2tZctMZ9O3Vw+tIEsG0hy7iocUrfZfEve2iUUzNSvE6jkQ4FbqIR7Ye9F0S90snp/O9c07yOo5EARW6iAdqG5qY//xqkhLi+PU3JuqSuBISOoYu4oGWS+I+c8M0+idrkGcJDe2hi3SxZRv28/wne7jp3BGcO0qXxJXQUaGLdKHCwzXc8fJ6Jg7rx+0XjfY6jkQZFbpIF2k5RbGp2fHwHF0SV0IvqFeUmc0wszwzyzezBUdZ5yoz22xmm8zs+dDGFIl8r6wu5P2tRfzk4tFkpumSuBJ67b4pamaxwCPAV4ACYKWZLXXObQ5YZyRwJ3CWc67MzPp3VmCRSFRUUce9f97M1KwUrjkj2+s4EqWC2UOfBuQ753Y45+qBxcDsVut8F3jEOVcG4Jw7FNqYIpHtnqWbqKlv4leXT9ApitJpgin0IcDegNsF/nmBRgGjzOwfZvaxmc0IVUCRSPeXjQd4Y8N+brlwJCf3T/I6jkSxUJ2HHgeMBM4DhgIfmNl459zhwJXMbB4wDyAzMzNEP1okfJVXN/Cz1zcydlAf5p0zwus4EuWC2UMvBAKv5znUPy9QAbDUOdfgnNsJbMVX8J/jnHvcOZfjnMvJyND5txL9fvHGZkqr6rn/igk6q0U6XTCvsJXASDMbbmbxwBxgaat1XsO3d46ZpeM7BLMjdDFFIs+H24p4cVUB884Zwbghfb2OI91Au4XunGsE5gPLgS3AEufcJjO718xm+VdbDpSY2WbgXeDHzrmSzgotEu6q6hpZ8PIGRqQncssFX/hjVaRTBHUM3Tm3DFjWat7dAdMOuNX/JdLtPbA8j33lNSy56Qx69tDYoNI1dFBPJMRW7S7lmY92cc30LE7LTvU6jnQjKnSREKptaOInL61ncN9e/HjGGK/jSDejy+eKhNDv3slne1EVz9wwjaQE/XpJ19IeukiIbNpXzh/e387lU4bqsrjiCRW6SAg0NjXzk5fWk9I7np9deorXcaSb0t+EIiHw3x/uZNO+I/zhW1Po1zve6zjSTWkPXaSDthdV8tDbW5lx6kBmjh/kdRzpxlToIh3Q3OxY8PJ6esbFcO/sU72OI92cCl2kA577ZDcrd5Xxs0vH0r+PBnsWb6nQRU5QQVk1v3rzU84emc4VU4d6HUdEhS5yIpxz/PTVjTjgl18fj5kGrRDvqdBFTsArqwv5wD8+6LBUjQ8q4UGFLnKcWsYHzdH4oBJmVOgix+nnSzdSU9/EfRofVMKMCl3kOPxl4wGWbTig8UElLKnQRYKk8UEl3Omj/yJBahkf9KnrTtP4oBKW9KoUCcIHW33jg96k8UEljKnQRdpRVdfIna/4xgf9ocYHlTCmQy4i7dD4oBIptIcucgy5uzQ+qEQOFbrIUdQ2NHHHyxofVCKHDrmIHMXD72zT+KASUbSHLtKGTfvKefT9HRofVCKKCl2kFY0PKpFKf0eKtPL4hzs0PqhEJO2hiwTYXlTJf729TeODSkRSoYv4fW580Ms0PqhEHhW6iN/nxgdN1vigEnlU6CJofFCJDip06fY0PqhECxW6dHst44PeMWOMxgeViBZUoZvZDDPLM7N8M1twjPUuNzNnZjmhiyjSeQ5V1H42Pui3p2d5HUekQ9otdDOLBR4BZgJjgblmNraN9ZKBW4BPQh1SpLPcs3QTNQ0aH1SiQzB76NOAfOfcDudcPbAYmN3Gev8G/AqoDWE+kU7zl437feODXqDxQSU6BFPoQ4C9AbcL/PM+Y2ZTgGHOuTeO9UBmNs/Mcs0st6io6LjDioRKeXUD/++1TRofVKJKh98UNbMY4NfAbe2t65x73DmX45zLycjQBY/EO794YzNl1fXcf8UEjQ8qUSOYV3IhMCzg9lD/vBbJwDjgPTPbBUwHluqNUQlXGh9UolUwhb4SGGlmw80sHpgDLG1Z6Jwrd86lO+eynXPZwMfALOdcbqckFumAz8YHzdD4oBJ92i1051wjMB9YDmwBljjnNpnZvWY2q7MDioTSz5duYl95Db+6fILGB5WoE9Tlc51zy4BlrebdfZR1z+t4LJHQe21NIS+tKuCHXz5Z44NKVNK7QdIt7Cqu4q5XN3BadooOtUjUUqFL1KtrbGL+otXExcbwmzmTidNZLRKlNGKRRL37/5LHxsIjPPbtqQzu18vrOCKdRrsqEtXe+fQgT/x9J9eekcXFpw70Oo5Ip1KhS9Q6eKSW219czymD+nDnJRrsWaKfCl2iUlOz40eL11JT38TDcyfrFEXpFnQMXaLS79/N56MdJTxwxQRdeEu6De2hS9RZuauUh97eyuxJgzWcnHQrKnSJKoer67ll0RqGpfbmF5eN03By0q3okItEDeccP3lpPUWVdbz8/TNJ7tnD60giXUp76BI1Fn68m7c2H+SOGWOYMLSf13FEupwKXaLC5n1H+MUbWzh/dAY3nDXc6zginlChS8Srrm9k/qLV9OvVg/+8cqLGBpVuS8fQJeLds3QTO4ur+J/vnE5aUoLXcUQ8oz10iWivry1kSW4B888/mTNPSvc6joinVOgSsXaXVHHXqxvJyUrhFl0SV0SFLpGpvrGZmxetITbG+M1cXRJXBHQMXSLUA8s/ZX1BOY9ePZUhuiSuCKA9dIlA7+Yd4r8/3Mk1Z2QxY5wuiSvSQoUuEWV/eQ23L1nHmIHJ/FSXxBX5HBW6RIyyqnqueWIFdY3N/O6bU3RJXJFWdAxdIkJVXSPXP72S3aXVPHP9NF0SV6QN2kOXsFff2Mz3nlvF+oLDPDx3MmeclOZ1JJGwpD10CWtNzY5bl6zlw23F3H/5BI0LKnIM2kOXsOWc456lm/jz+v3cOXMMV502zOtIImFNhS5h66G3t7Hw493cdM4Ibjr3JK/jiIQ9FbqEpaf/sZPf/m0bV+UMZcHMMV7HEYkIKnQJO6+vLeSeP23morED+OXXx2sYOZEgqdAlrLybd4jblqzj9OGp/FbXaBE5LvptkbCxancp339uFaMHJvPHa3P0wSGR46RCl7Dw6YEjXP/USgb17cUzN0zTAM8iJyCoQjezGWaWZ2b5ZragjeW3mtlmM1tvZn8zs6zQR5Votbe0mmueWEGv+FievWEa6Rp1SOSEtFvoZhYLPALMBMYCc81sbKvV1gA5zrkJwEvA/aEOKtGpqKKOq5/4hLrGZp694XSGpfb2OpJIxApmD30akO+c2+GcqwcWA7MDV3DOveucq/bf/BgYGtqYEo2O1DZw7ZMrOHSkjievO43RA5O9jiQS0YIp9CHA3oDbBf55R3Mj8GZbC8xsnpnlmlluUVFR8Ckl6tQ2NPGdZ3LZerCCP1w9halZKV5HEol4IX1T1MyuBnKAB9pa7px73DmX45zLycjICOWPlgjS2NTM/OfXsHJXKQ9eNZHzRvf3OpJIVAjm4lyFQOBFNIb6532OmV0I3AWc65yrC008iTbNzY47Xt7A21sOcu/sU5k96Vh/7InI8QhmD30lMNLMhptZPDAHWBq4gplNBh4DZjnnDoU+pkSDyrpG5i9azcurC/jRhSO55oxsryOJRJV299Cdc41mNh9YDsQCTzrnNpnZvUCuc24pvkMsScCL/o9p73HOzerE3BJhdhZXMe/ZXLYXVfLTS8bw3bNHeB1JJOoEdT1059wyYFmreXcHTF8Y4lwSRf625SA/emEtcTHGwhtP56yT072OJBKVNMCFdJrmZsdv39nGf729jVMH9+Gxb09laIrOMxfpLCp06RRHahu49YW1vL3lEP80ZQi//Pp4XZtFpJOp0CXkth2s4KaFq9hTWs2/zjqVa87I0iVwRbqACl1C6s0N+7n9xXX0io/l+e9OZ9rwVK8jiXQbKnQJiaZmx4Nv5fH797YzaVg/Hr16KgP79vQ6lki3okKXDjtcXc8PF6/lg61FzJ2WyT2zxpIQp+PlIl1NhS4dsnnfEW56LpeD5XX8xz+NZ+60TK8jiXRbKnQ5Ya+vLeSOl9fTr1c8i2+azpRMXWBLxEsqdDlujU3N3Pfmp/zx7zuZlp3KI9+aQkayBqUQ8ZoKXY5L4eEabl+yjo92lHDdmdnc9dVT6KGBnEXCggpdglJe3cAj7+Xz9P/uwoAHr5zI5VM1jolIOFGhyzHVNjTx7Ee7+N07+VTUNXL5lKHc+pVRDO7Xy+toItKKCl3a1NzseG1tIQ++tZXCwzWcNzqDO2aM4ZRBfbyOJiJHoUKXL/hgaxH/8eanbNl/hPFD+vLAFRM4U1dIFAl7KnT5zMbCcu5781P+nl/MsNRe/HbuZC4dP4iYGF2HRSQSqNCFvaXVPPhWHq+t3UdK7x7cfelYvjU9U5/2FIkwKvRurKyqnkfezefZj3YTEwP/fN5JfO+8k+jTs4fX0UTkBKjQu6Hahiae+scufv9ePlV1jVw5dRj/8pVRupiWSIRToXcje0ureW1NIc+v2MP+8louGNOfO2aOYdSAZK+jiUgIqNCjXGlVPW9s2M9rawpZtbsMgOkjUnnoG5OYPiLN43QiEkoq9ChUU9/E21sO8tqaQt7fWkRjs2P0gGTumDGGWZMGM0QfChKJSir0KNHU7Pjf7cW8tmYff9m4n6r6Jgb26cmNXxrOZZOH6ANBIt2ACj2COefYtO8Ir64p5E/r9nGooo7khDgunTCY2ZMHc/rwNGJ1DrlIt6FCj0B7S6t5fW0hr64pZHtRFfGxMZw/JoPLJg3h/DH96dlD54+LdEcq9DDX3OzYdqiSVbvLWLW7jNV7ythZXAXAtOGpfOfsEVwybhB9e+vccZHuToUeZirrGlm39/DnCryithGA9KR4pmSm8M1pmcwcP5ChKb09Tisi4USF7iHnHAVlNaze4yvv3F1lfHrgCM0OzGD0gGS+NnEwUzNTyMlOITO1N2Y6Ji4ibVOhd5HGpmb2Ha5lZ0kV2w5WfLYHfqiiDoDE+FgmZ6Yw/8sjmZqVwuTMfvoIvogcFxV6CDU2NVN4uIadxVXsLqn2f/dN7y2rpqHJfbbu0JRenHlSGlOzUpiSlcKYgX10RoqIdIgK/TjV1Ddx8Egtu0qq2FVcxa6Sana1lHZpNY3N/1faveNjyUpLZMygZC4eN5DhaYlkpfVmREaSBlUWkZDr9oXunKO8poHiyjqKK+sprqyjxP/9/277pksq66iqb/rc/RP9pT12UB9mjhtIdnoi2WmJZKf1JiM5Qce8RaTLBFXoZjYD+A0QC/zROXdfq+UJwLPAVKAE+IZzbldoo7atqdlRWdfo+6ptpLKugYraRqrqmj6bbllWVd9IRW0jR2obKa6oo6TKV96Be9UtYgxSE+NJT0ogLSmeyZn9SEtMID05noykBLLTfXvbGUkqbREJD+0WupnFAo8AXwEKgJVmttQ5tzlgtRuBMufcyWY2B/gV8I3OCPzCyj089v4OKvwlXdPQ1P6d8B3+SEqIIykhjuSecQzq25NxQ/r4CzuB9CRfebcUeErveB3TFpGIEswe+jQg3zm3A8DMFgOzgcBCnw3c459+CfidmZlz7ou7vh2UmpjA2MF9SO4Z5y/oHiQmxPpv9yCp5/+VdqK/wBPjY4mLjQl1FBGRsBJMoQ8B9gbcLgBOP9o6zrlGMysH0oDiwJXMbB4wDyAzM/OEAn9l7AC+MnbACd1XRCSadeluq3PucedcjnMuJyMjoyt/tIhI1Aum0AuBYQG3h/rntbmOmcUBffG9OSoiIl0kmEJfCYw0s+FmFg/MAZa2WmcpcK1/+grgnc44fi4iIkfX7jF0/zHx+cByfKctPumc22Rm9wK5zrmlwBPAQjPLB0rxlb6IiHShoM5Dd84tA5a1mnd3wHQtcGVoo4mIyPHQuXwiIlFChS4iEiVU6CIiUcK8OhnFzIqA3Sd493RafWgpzChfxyhfx4V7RuU7cVnOuTY/yONZoXeEmeU653K8znE0ytcxytdx4Z5R+TqHDrmIiEQJFbqISJSI1EJ/3OsA7VC+jlG+jgv3jMrXCSLyGLqIiHxRpO6hi4hIKyp0EZEoEbaFbmZXmtkmM2s2s6OePmRmM8wsz8zyzWxBwPzhZvaJf/4L/itFhjJfqpn91cy2+b+ntLHO+Wa2NuCr1swu8y972sx2Biyb1NX5/Os1BWRYGjA/HLbfJDP7yP86WG9m3whY1inb72ivp4DlCf7tke/fPtkBy+70z88zs4tDkecE8t1qZpv92+tvZpYVsKzN57qL811nZkUBOb4TsOxa/+thm5ld2/q+XZTvoYBsW83scMCyTt9+HeacC8sv4BRgNPAekHOUdWKB7cAIIB5YB4z1L1sCzPFPPwp8P8T57gcW+KcXAL9qZ/1UfFei7O2//TRwRSduv6DyAZVHme/59gNGASP904OB/UC/ztp+x3o9Bazzz8Cj/uk5wAv+6bH+9ROA4f7HifUg3/kBr7Hvt+Q71nPdxfmuA37Xxn1TgR3+7yn+6ZSuztdq/ZvxXV22S7ZfKL7Cdg/dObfFOZfXzmqfjXfqnKsHFgOzzcyAL+Mb3xTgGeCyEEec7X/cYB//CuBN51x1iHMczfHm+0y4bD/n3Fbn3Db/9D7gENCZQ121+XpqtU5g7peAC/zbazaw2DlX55zbCeT7H69L8znn3g14jX2Mb0CarhLM9juai4G/OudKnXNlwF+BGR7nmwssCnGGThW2hR6ktsY7HYJvPNPDzrnGVvNDaYBzbr9/+gDQ3kCnc/jii+Pf/X8aP2RmCR7l62lmuWb2ccvhIMJw+5nZNHx7VdsDZod6+x3t9dTmOv7t0zJ+bjD37Yp8gW4E3gy43dZz7UW+y/3P20tm1jIaWlhtP/+hquHAOwGzO3v7dVhQ10PvLGb2NjCwjUV3Oede7+o8rR0rX+AN55wzs6Oe/2lmg4Dx+AYJaXEnviKLx3fO6x3AvR7ky3LOFZrZCOAdM9uAr6Q6LMTbbyFwrXOu2T+7w9svmpnZ1UAOcG7A7C8818657W0/Qqf5E7DIOVdnZjfh+2vny12cIRhzgJecc00B88Jh+x2Tp4XunLuwgw9xtPFOS4B+Zhbn34tqaxzUDuUzs4NmNsg5t99fOIeO8VBXAa865xoCHrtl77TOzJ4Cbvcin3Ou0P99h5m9B0wGXiZMtp+Z9QHewPef/McBj93h7deG4xk/t8A+P35uMPftinyY2YX4/tM81zlX1zL/KM91KAup3XzOucCxhv+I772Ulvue1+q+74UwW1D5AswBfhA4owu2X4dF+iGXNsc7db53MN7Fd9wafOOdhnqPP3Ac1fYe/wvH4vwl1nK8+jJgY1fnM7OUlkMVZpYOnAVsDpft539OXwWedc691GpZZ2y/joyfuxSY4z8LZjgwElgRgkzHlc/MJgOPAbOcc4cC5rf5XHuQb1DAzVnAFv/0cuAif84U4CI+/xdtl+TzZxyD743ZjwLmdcX26ziv35U92hfwdXzHuOqAg8By//zBwLKA9S4BtuL7n/KugPkj8P1C5QMvAgkhzpcG/A3YBrwNpPrn5wB/DFgvG99eQEyr+78DbMBXRM8BSV2dDzjTn2Gd//uN4bT9gKuBBmBtwNekztx+bb2e8B3KmeWf7unfHvn+7TMi4L53+e+XB8zspN+L9vK97f99adleS9t7rrs4338Am/w53gXGBNz3Bv92zQeu9yKf//Y9wH2t7tcl26+jX/rov4hIlIj0Qy4iIuKnQhcRiRIqdBGRKKFCFxGJEip0EZEooUIXEYkSKnQRkSjx/wHmEA6XzRjJ3QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x, k=5.0):\n",
    "    return 1.0/(1.0+np.exp(-k*x))\n",
    "\n",
    "x = np.arange(-1.0, 1.0, 0.1) \n",
    "y = sigmoid(x) \n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5856527-0a6a-40de-83e4-968062a87ba0",
   "metadata": {},
   "source": [
    "55- Added channel_scale so the convolution can be minimized by searching a single value rather the norm of the convolution\n",
    "- ConvBR::Forward, rather than scaling the convlution weights, I can scale the convolution output of all batches and convolutions by sigmoid(sigmoid_scale*cannel_scale)\n",
    "```python\n",
    "        x = self.conv(x)\n",
    "        if self.search_structure: #scale channels based on \n",
    "            weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)[None,:,None,None]\n",
    "            x *= weight_scale\n",
    "```\n",
    "- CpmvBR::ArchitectureWeights scales the architecture weight convolution norm by (sigmoid_scale*channel_scale):\n",
    "```python\n",
    "        weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        conv_weights = torch.tanh(self.weight_gain*weight_scale*torch.linalg.norm(self.conv.weight, dim=(1,2,3)))\n",
    "```\n",
    "- sigmoid scale = 5 transitions from ~0 at -1 to ~1 at 1.  Not necessary but it it makes me happy to have the weights normalized in that range\n",
    "- This resulted in a much more agressive pruning\n",
    "<br /> ![nas_20220110_00 cell weights](../img/nas_20220110_00_cw.png) <br />\n",
    "- Accuracy converged well with test tracking training effectively\n",
    "<br /> ![nas_20220110_00 Tensorboard](../img/nas_20220110_00_tb.png) <br />\n",
    "- Pruning was too effective but not reflected in search weight.  Add pruning information to ConvBR::ApplyStructure so the convoluation weight an pruning is clear that results in everything being pruned.\n",
    "- Why didn't the plot show everything was being pruned?  \n",
    "```cmd\n",
    "Total Trainable Params: 38146522\n",
    "ConvBR::ApplyStructure Cell 0 convolution 0/3 1.0=64/64 in_channels=3 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 0 convolution 1/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 0 convolution 2/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 0 cell residual 1.0=256/256 in_channels=3 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 convolution 0/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 convolution 1/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 convolution 2/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 cell residual 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 convolution 0/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 convolution 1/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 convolution 2/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 cell residual 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 convolution 0/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 convolution 1/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 convolution 2/3 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 cell residual 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 convolution 0/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 convolution 1/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 convolution 2/3 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 cell residual 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 convolution 0/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 convolution 1/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 convolution 2/3 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 cell residual 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "FC::ApplyStructure in 1.0=2048/2048 out 0.0=0/10 convolutions in_channels=0 out_channels=10\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|        cells.0.cnn.0.channel_scale        |     64     |\n",
    "|         cells.0.cnn.0.conv.weight         |     0      |\n",
    "|          cells.0.cnn.0.conv.bias          |     0      |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.0.cnn.1.channel_scale        |     64     |\n",
    "|         cells.0.cnn.1.conv.weight         |     0      |\n",
    "|          cells.0.cnn.1.conv.bias          |     0      |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.0.cnn.2.channel_scale        |    256     |\n",
    "|         cells.0.cnn.2.conv.weight         |     0      |\n",
    "|          cells.0.cnn.2.conv.bias          |     0      |\n",
    "|      cells.0.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.0.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.0.conv_residual.channel_scale    |    256     |\n",
    "|     cells.0.conv_residual.conv.weight     |     0      |\n",
    "|      cells.0.conv_residual.conv.bias      |     0      |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|        cells.1.cnn.0.channel_scale        |     64     |\n",
    "|         cells.1.cnn.0.conv.weight         |     0      |\n",
    "|          cells.1.cnn.0.conv.bias          |     0      |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.1.cnn.1.channel_scale        |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |     0      |\n",
    "|          cells.1.cnn.1.conv.bias          |     0      |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.1.cnn.2.channel_scale        |    256     |\n",
    "|         cells.1.cnn.2.conv.weight         |     0      |\n",
    "|          cells.1.cnn.2.conv.bias          |     0      |\n",
    "|      cells.1.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.1.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.1.conv_residual.channel_scale    |    256     |\n",
    "|     cells.1.conv_residual.conv.weight     |     0      |\n",
    "|      cells.1.conv_residual.conv.bias      |     0      |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|        cells.2.cnn.0.channel_scale        |     64     |\n",
    "|         cells.2.cnn.0.conv.weight         |     0      |\n",
    "|          cells.2.cnn.0.conv.bias          |     0      |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.2.cnn.1.channel_scale        |     64     |\n",
    "|         cells.2.cnn.1.conv.weight         |     0      |\n",
    "|          cells.2.cnn.1.conv.bias          |     0      |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.2.cnn.2.channel_scale        |    256     |\n",
    "|         cells.2.cnn.2.conv.weight         |     0      |\n",
    "|          cells.2.cnn.2.conv.bias          |     0      |\n",
    "|      cells.2.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.2.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.2.conv_residual.channel_scale    |    256     |\n",
    "|     cells.2.conv_residual.conv.weight     |     0      |\n",
    "|      cells.2.conv_residual.conv.bias      |     0      |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|        cells.3.cnn.0.channel_scale        |    128     |\n",
    "|         cells.3.cnn.0.conv.weight         |     0      |\n",
    "|          cells.3.cnn.0.conv.bias          |     0      |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.3.cnn.1.channel_scale        |    128     |\n",
    "|         cells.3.cnn.1.conv.weight         |     0      |\n",
    "|          cells.3.cnn.1.conv.bias          |     0      |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.3.cnn.2.channel_scale        |    512     |\n",
    "|         cells.3.cnn.2.conv.weight         |     0      |\n",
    "|          cells.3.cnn.2.conv.bias          |     0      |\n",
    "|      cells.3.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.3.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.3.conv_residual.channel_scale    |    512     |\n",
    "|     cells.3.conv_residual.conv.weight     |     0      |\n",
    "|      cells.3.conv_residual.conv.bias      |     0      |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|        cells.4.cnn.0.channel_scale        |    128     |\n",
    "|         cells.4.cnn.0.conv.weight         |     0      |\n",
    "|          cells.4.cnn.0.conv.bias          |     0      |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.4.cnn.1.channel_scale        |    128     |\n",
    "|         cells.4.cnn.1.conv.weight         |     0      |\n",
    "|          cells.4.cnn.1.conv.bias          |     0      |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.4.cnn.2.channel_scale        |    512     |\n",
    "|         cells.4.cnn.2.conv.weight         |     0      |\n",
    "|          cells.4.cnn.2.conv.bias          |     0      |\n",
    "|      cells.4.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.4.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.4.conv_residual.channel_scale    |    512     |\n",
    "|     cells.4.conv_residual.conv.weight     |     0      |\n",
    "|      cells.4.conv_residual.conv.bias      |     0      |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|        cells.5.cnn.0.channel_scale        |    128     |\n",
    "|         cells.5.cnn.0.conv.weight         |     0      |\n",
    "|          cells.5.cnn.0.conv.bias          |     0      |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.5.cnn.1.channel_scale        |    128     |\n",
    "|         cells.5.cnn.1.conv.weight         |     0      |\n",
    "|          cells.5.cnn.1.conv.bias          |     0      |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.5.cnn.2.channel_scale        |    512     |\n",
    "|         cells.5.cnn.2.conv.weight         |     0      |\n",
    "|          cells.5.cnn.2.conv.bias          |     0      |\n",
    "|      cells.5.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.5.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.5.conv_residual.channel_scale    |    512     |\n",
    "|     cells.5.conv_residual.conv.weight     |     0      |\n",
    "|      cells.5.conv_residual.conv.bias      |     0      |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|        cells.6.cnn.0.channel_scale        |    128     |\n",
    "|         cells.6.cnn.0.conv.weight         |     0      |\n",
    "|          cells.6.cnn.0.conv.bias          |     0      |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.6.cnn.1.channel_scale        |    128     |\n",
    "|         cells.6.cnn.1.conv.weight         |     0      |\n",
    "|          cells.6.cnn.1.conv.bias          |     0      |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.6.cnn.2.channel_scale        |    512     |\n",
    "|         cells.6.cnn.2.conv.weight         |     0      |\n",
    "|          cells.6.cnn.2.conv.bias          |     0      |\n",
    "|      cells.6.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.6.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.6.conv_residual.channel_scale    |    512     |\n",
    "|     cells.6.conv_residual.conv.weight     |     0      |\n",
    "|      cells.6.conv_residual.conv.bias      |     0      |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|        cells.7.cnn.0.channel_scale        |    256     |\n",
    "|         cells.7.cnn.0.conv.weight         |     0      |\n",
    "|          cells.7.cnn.0.conv.bias          |     0      |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.7.cnn.1.channel_scale        |    256     |\n",
    "|         cells.7.cnn.1.conv.weight         |     0      |\n",
    "|          cells.7.cnn.1.conv.bias          |     0      |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.7.cnn.2.channel_scale        |    1024    |\n",
    "|         cells.7.cnn.2.conv.weight         |     0      |\n",
    "|          cells.7.cnn.2.conv.bias          |     0      |\n",
    "|      cells.7.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.7.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.7.conv_residual.channel_scale    |    1024    |\n",
    "|     cells.7.conv_residual.conv.weight     |     0      |\n",
    "|      cells.7.conv_residual.conv.bias      |     0      |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|        cells.8.cnn.0.channel_scale        |    256     |\n",
    "|         cells.8.cnn.0.conv.weight         |     0      |\n",
    "|          cells.8.cnn.0.conv.bias          |     0      |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.8.cnn.1.channel_scale        |    256     |\n",
    "|         cells.8.cnn.1.conv.weight         |     0      |\n",
    "|          cells.8.cnn.1.conv.bias          |     0      |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.8.cnn.2.channel_scale        |    1024    |\n",
    "|         cells.8.cnn.2.conv.weight         |     0      |\n",
    "|          cells.8.cnn.2.conv.bias          |     0      |\n",
    "|      cells.8.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.8.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.8.conv_residual.channel_scale    |    1024    |\n",
    "|     cells.8.conv_residual.conv.weight     |     0      |\n",
    "|      cells.8.conv_residual.conv.bias      |     0      |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|        cells.9.cnn.0.channel_scale        |    256     |\n",
    "|         cells.9.cnn.0.conv.weight         |     0      |\n",
    "|          cells.9.cnn.0.conv.bias          |     0      |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.9.cnn.1.channel_scale        |    256     |\n",
    "|         cells.9.cnn.1.conv.weight         |     0      |\n",
    "|          cells.9.cnn.1.conv.bias          |     0      |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.9.cnn.2.channel_scale        |    1024    |\n",
    "|         cells.9.cnn.2.conv.weight         |     0      |\n",
    "|          cells.9.cnn.2.conv.bias          |     0      |\n",
    "|      cells.9.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.9.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.9.conv_residual.channel_scale    |    1024    |\n",
    "|     cells.9.conv_residual.conv.weight     |     0      |\n",
    "|      cells.9.conv_residual.conv.bias      |     0      |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|        cells.10.cnn.0.channel_scale       |    256     |\n",
    "|         cells.10.cnn.0.conv.weight        |     0      |\n",
    "|          cells.10.cnn.0.conv.bias         |     0      |\n",
    "|     cells.10.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.10.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.10.cnn.1.channel_scale       |    256     |\n",
    "|         cells.10.cnn.1.conv.weight        |     0      |\n",
    "|          cells.10.cnn.1.conv.bias         |     0      |\n",
    "|     cells.10.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.10.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.10.cnn.2.channel_scale       |    1024    |\n",
    "|         cells.10.cnn.2.conv.weight        |     0      |\n",
    "|          cells.10.cnn.2.conv.bias         |     0      |\n",
    "|     cells.10.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.10.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.10.conv_residual.channel_scale   |    1024    |\n",
    "|     cells.10.conv_residual.conv.weight    |     0      |\n",
    "|      cells.10.conv_residual.conv.bias     |     0      |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|        cells.11.cnn.0.channel_scale       |    256     |\n",
    "|         cells.11.cnn.0.conv.weight        |     0      |\n",
    "|          cells.11.cnn.0.conv.bias         |     0      |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.11.cnn.1.channel_scale       |    256     |\n",
    "|         cells.11.cnn.1.conv.weight        |     0      |\n",
    "|          cells.11.cnn.1.conv.bias         |     0      |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.11.cnn.2.channel_scale       |    1024    |\n",
    "|         cells.11.cnn.2.conv.weight        |     0      |\n",
    "|          cells.11.cnn.2.conv.bias         |     0      |\n",
    "|     cells.11.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.11.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.11.conv_residual.channel_scale   |    1024    |\n",
    "|     cells.11.conv_residual.conv.weight    |     0      |\n",
    "|      cells.11.conv_residual.conv.bias     |     0      |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|        cells.12.cnn.0.channel_scale       |    256     |\n",
    "|         cells.12.cnn.0.conv.weight        |     0      |\n",
    "|          cells.12.cnn.0.conv.bias         |     0      |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.12.cnn.1.channel_scale       |    256     |\n",
    "|         cells.12.cnn.1.conv.weight        |     0      |\n",
    "|          cells.12.cnn.1.conv.bias         |     0      |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.12.cnn.2.channel_scale       |    1024    |\n",
    "|         cells.12.cnn.2.conv.weight        |     0      |\n",
    "|          cells.12.cnn.2.conv.bias         |     0      |\n",
    "|     cells.12.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.12.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.12.conv_residual.channel_scale   |    1024    |\n",
    "|     cells.12.conv_residual.conv.weight    |     0      |\n",
    "|      cells.12.conv_residual.conv.bias     |     0      |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|        cells.13.cnn.0.channel_scale       |    512     |\n",
    "|         cells.13.cnn.0.conv.weight        |     0      |\n",
    "|          cells.13.cnn.0.conv.bias         |     0      |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.13.cnn.1.channel_scale       |    512     |\n",
    "|         cells.13.cnn.1.conv.weight        |     0      |\n",
    "|          cells.13.cnn.1.conv.bias         |     0      |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.13.cnn.2.channel_scale       |    2048    |\n",
    "|         cells.13.cnn.2.conv.weight        |     0      |\n",
    "|          cells.13.cnn.2.conv.bias         |     0      |\n",
    "|     cells.13.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.13.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.13.conv_residual.channel_scale   |    2048    |\n",
    "|     cells.13.conv_residual.conv.weight    |     0      |\n",
    "|      cells.13.conv_residual.conv.bias     |     0      |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|        cells.14.cnn.0.channel_scale       |    512     |\n",
    "|         cells.14.cnn.0.conv.weight        |     0      |\n",
    "|          cells.14.cnn.0.conv.bias         |     0      |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.14.cnn.1.channel_scale       |    512     |\n",
    "|         cells.14.cnn.1.conv.weight        |     0      |\n",
    "|          cells.14.cnn.1.conv.bias         |     0      |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.14.cnn.2.channel_scale       |    2048    |\n",
    "|         cells.14.cnn.2.conv.weight        |     0      |\n",
    "|          cells.14.cnn.2.conv.bias         |     0      |\n",
    "|     cells.14.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.14.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.14.conv_residual.channel_scale   |    2048    |\n",
    "|     cells.14.conv_residual.conv.weight    |     0      |\n",
    "|      cells.14.conv_residual.conv.bias     |     0      |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|        cells.15.cnn.0.channel_scale       |    512     |\n",
    "|         cells.15.cnn.0.conv.weight        |     0      |\n",
    "|          cells.15.cnn.0.conv.bias         |     0      |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.15.cnn.1.channel_scale       |    512     |\n",
    "|         cells.15.cnn.1.conv.weight        |     0      |\n",
    "|          cells.15.cnn.1.conv.bias         |     0      |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.15.cnn.2.channel_scale       |    2048    |\n",
    "|         cells.15.cnn.2.conv.weight        |     0      |\n",
    "|          cells.15.cnn.2.conv.bias         |     0      |\n",
    "|     cells.15.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.15.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.15.conv_residual.channel_scale   |    2048    |\n",
    "|     cells.15.conv_residual.conv.weight    |     0      |\n",
    "|      cells.15.conv_residual.conv.bias     |     0      |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|                fc.fc.weight               |     0      |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 37786\n",
    "Reduced parameters 37786/38146522 = 0.0009905490204323215\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 1049, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 929, in Test\n",
    "    plotsearch = PlotSearch(classify)\n",
    "  File \"networks/cell2d.py\", line 785, in __init__\n",
    "    architecture_weights, total_trainable_weights, cell_weights = network.ArchitectureWeights()\n",
    "  File \"networks/cell2d.py\", line 703, in ArchitectureWeights\n",
    "    cell_archatecture_weights, cell_total_trainable_weights, cell_weight = in_cell.ArchitectureWeights()\n",
    "  File \"networks/cell2d.py\", line 463, in ArchitectureWeights\n",
    "    layer_weight, _, conv_weights  = l.ArchitectureWeights()\n",
    "  File \"networks/cell2d.py\", line 151, in ArchitectureWeights\n",
    "    conv_weights = torch.tanh(self.weight_gain*weight_scale*torch.linalg.norm(self.conv.weight, dim=(1,2,3)))\n",
    "RuntimeError: The size of tensor a (64) must match the size of tensor b (0) at non-singleton dimension 0\n",
    "PlotSearch finish\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef0930-7cf9-4028-8619-c08bcf7ffaac",
   "metadata": {},
   "source": [
    "12 January 2022\n",
    "- Error in normalized weights is because L2 norm is not normzlized based on the number of elments in norm.  The norm of large convolutions resulted in a larger norm than smaller convolutions.\n",
    "- L2 norm devided by the square root of the number of elements results in consistent bahavior across different tensor sizes:\n",
    "``` python\n",
    "def ArchitectureWeights(self):\n",
    "    weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "    norm = torch.linalg.norm(self.conv.weight, dim=(1,2,3))/np.sqrt(np.product(self.conv.weight.shape[1:]))\n",
    "    conv_weights = torch.tanh(self.weight_gain*weight_scale*norm)\n",
    "```\n",
    "- Next, include a by-stable function in the objective for channel enable/disable so the optomizer will be rewarded for either turning the channel on or off and avoid a middle value\n",
    "- How to handel zeroing out all convolutions?\n",
    "- Disabled batch norm to prevent compensating for a zeroed out channel by brining it back with batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90363603-3329-4cf4-99f1-0f2facc999ce",
   "metadata": {},
   "source": [
    "13 January 2022\n",
    "- Training was resulting in zero-sized convolutions\n",
    "- Cause - initialized channel pruning to 0.5.  \n",
    "- Architecture wights was product of convolution norm (~ 0.6) * channel scale (~0.5) = ~0.3\n",
    "- Cutoff was 0.5 = results in pruning all of the convolutions\n",
    "- \n",
    "``` python\n",
    "class ConvBR(nn.Module):\n",
    "    def __init__(self, \n",
    "       self.channel_scale = nn.Parameter(torch.zeros(self.out_channels, dtype=torch.float))\n",
    "    def ArchitectureWeights(self):\n",
    "        weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        conv_weights = torch.tanh(self.weight_gain*weight_scale*torch.linalg.norm(self.conv.weight, dim=(1,2,3)))\n",
    "\n",
    "```\n",
    "- Fix: change con_weights average of channel_scale and convolution norm\n",
    "```python\n",
    "    def ArchitectureWeights(self):\n",
    "        weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        norm = torch.linalg.norm(self.conv.weight, dim=(1,2,3))/np.sqrt(np.product(self.conv.weight.shape[1:]))\n",
    "        conv_weights = (torch.tanh(self.weight_gain*norm)+weight_scale)/2.0\n",
    "\n",
    "```\n",
    "- I then needed to reduce the -k_structure default argument (trying 1e-3 now) to accuracy is searched preferentially.\n",
    "- Add stabilization normalized gaussian function to push ConvBR::channel_scale to 0 or 1.  Add to loss so channels preferentially are on or off\n",
    "- record class weights as video\n",
    "- Add class weighs to tensorboard\n",
    "- What d cleass weights do when search_structure us false?\n",
    "- Add fully connected layer to model erosion\n",
    "- How to esnsure the eroded model is feasible?\n",
    "- Eroded weights should not be contributing significantly to model accuracy.  \n",
    "- Is relaxation of the convolution outputs a poor model of the removed weight?  \n",
    "- Do max pooling and batch normalizaton compensate too effectively for the weight change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c87bb-6fd8-47b4-bb9b-e2a9cdcf98ba",
   "metadata": {},
   "source": [
    "14 January 2022\n",
    "- The trained networks were very stubborn about minimizing network size\n",
    "- After initial training without or with very little pruning, I then tried to increase the pruning with very little affect\n",
    "- Added a gradient plot following the structure of the class_weights plot\n",
    "- Found that the architecture gradent was in the trange of 1e-6 and the classification gradient was in the range of 1e-2\n",
    "- By increasing k_structure, I was able to proportionally incrase the class weight norms to a similar value to classificaiton norms\n",
    "- At this point, classificaiton loss on the test set minimized at a bout 1000 batches and then incrased.  \n",
    "- Set structural loss to target a specific architecture level and  trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6b85e-be14-44a2-bc1a-4be625ec380a",
   "metadata": {},
   "source": [
    "15 January 2022\n",
    "- Added gradient norm visualization\n",
    "- Needed to increase k_structure to 1.0e2 or 1.0e3 to get the structure minimization.  1.0e3 convers structure well\n",
    "- Need to handle 0 size convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01269e6c",
   "metadata": {},
   "source": [
    "17 January 2022\n",
    "- Added image augmentation, removed dropout\n",
    "- CIFAR-10 Resnet 18, 75 epochs, -target_structure=1.0e0, final test accuracy ~0.8583, nas_20220117_rn18_00\n",
    "- CIFAR-10 Resnet 18, 20 epochs, -target_structure=1.0e0, -learning_rate=0.001, final test accuracy ~0.8743, nas_20220117_rn18_01\n",
    "- CIFSR-10 -target_structure=1.0e-1, 10 epochs, -learning_rate=0.01, -target_structure=1.0e-1, final test accuracy ~.8496, nas_20220117_rn18_02\n",
    "- CIFAR-10 Resnet 18, 20 epochs, Prune, Reduced parameters 5145795/11498898 = 0.4475033172744032, -learning_rate=0.001,\n",
    "learning_rate=0.01, -target_structure=1.0e-1, final test accuracy ~.8496, nas_20220117_rn18_02\n",
    "- nas_20220117_rn18_04 zero's out several channels.  Use this to figure out what I should do when this happens.  I would like to burn out the convolution and burn open the residual but that would mean growing the residual channel.  Maybe freezing the residual would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7cc36",
   "metadata": {},
   "source": [
    "18 January 2022\n",
    "- nas_20220118_rn101_00, -resnet_len=101, -learning_rate=0.01, -batch_size=200, -epochs=50, -model_class=CIFAR10, -model_src=None, -k_structure=1.0e2, -target_structure=1.0e0\n",
    "    ``` cmd\n",
    "    Test [50, 245.000000] training accuracy=0.900000 test accuracy=0.870000 training loss=5.92753e-01, test loss=6.63403e-01 arcitecture_reduction: 9.48538e-01                        \n",
    "    Test [50, 250.000000] training accuracy=0.880000 test accuracy=0.885000 training loss=5.86504e-01, test loss=6.20232e-01 arcitecture_reduction: 9.48538e-01                        \n",
    "    Train steps:  250/250.0 [04:04<00:00,  1.02it/s]\n",
    "    Train epochs: 100% 50/50 [3:25:56<00:00, 247.13s/it]\n",
    "    ```\n",
    "-   nas_20220118_rn101_01  -learning_rate=1e-3, -epochs=25, -model_src=nas_20220118_rn101_00, test accuracy ~.8835\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fbc5",
   "metadata": {},
   "source": [
    "19 January 2022\n",
    "- Resnet 101 test accuracy capped at ~ 87%\n",
    "- Going back and preparing data at Resnet 50\n",
    "- py networks/cell2d.py -target_structure=1.0 -epochs=50 -prune=False -learning_rate=1e-2 -model_dest=\"nas_20220119_rn50_00\" - Stopped when test accuracy was ~ 85\n",
    "- py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=False -learning_rate=1e-3 -model_src=\"nas_20220119_rn50_00\" -model_dest=\"nas_20220119_rn50_01\"\n",
    "- After data, simplify residual path.\n",
    "- Nvidia pruning software: [Transfer learning toolkit](https://developer.nvidia.com/blog/transfer-learning-toolkit-pruning-intelligent-video-analytics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e9daa",
   "metadata": {
    "tags": []
   },
   "source": [
    "20 January 2022\n",
    "- Initial training of resnet 50 model\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=False -learning_rate=1e-3 -model_src=\"nas_20220119_rn50_00\" -model_dest=\"nas_20220119_rn50_01\"\n",
    "```\n",
    "- Set target structure and train for 25 epochs\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=25 -prune=False -learning_rate=1e-3 -model_src=\"nas_20220119_rn50_01\" -model_dest=\"nas_20220119_rn50_ts5_02\"\n",
    "```\n",
    "- Learning rate was too low.  Prune and train again\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=25 -prune=True -learning_rate=1e-2 -model_src=\"nas_20220119_rn50_ts5_02\" -model_dest=\"nas_20220119_rn50_ts5_03\"\n",
    "...\n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [43:57<00:00, 105.52s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  3.96it/s]\n",
    "test_accuracy=0.8608\n",
    "```\n",
    "- Final fine tuning at a lower learning rate\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=True -learning_rate=1e-3 -model_src=nas_20220119_rn50_ts5_03 -model_dest=nas_20220119_rn50_ts5_03\n",
    "...\n",
    "Total Trainable Params: 21300619\n",
    "Reduced parameters 21300619/22183066 = 0.960219791078474\n",
    "...\n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:31<00:00,  1.37it/s]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [38:25<00:00, 92.23s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:05<00:00,  4.35it/s]\n",
    "test_accuracy=0.869\n",
    "```\n",
    "- The fainal pruned network is:\n",
    "![Network chanel gradient norms](../img/nas_20220119_rn50_ts5_03_gn.png)\n",
    "- Retrain from the pretraining with a reducted target structure:\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.25 -epochs=25 -prune=False -learning_rate=1e-2 -model_src=nas_20220119_rn50_01 -model_dest=nas_20220119_rn50_ts25_02\n",
    "...\n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [44:53<00:00, 107.75s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  4.16it/s]\n",
    "test_accuracy=0.8641\n",
    "``` \n",
    "- Prune and final training:\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=True -learning_rate=1e-2 -model_src=nas_20220119_rn50_ts25_02 -model_dest=nas_20220119_rn50_ts25_03\n",
    "...\n",
    "Total Trainable Params: 21557456\n",
    "Reduced parameters 21557456/22183066 = 0.9717978569779309\n",
    "Train epochs:   0%|                                                                                                                                 | 0/25 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.747500 test accuracy=0.742500 training loss=1.06741e+02, test loss=6.18376e+01 arcitecture_reduction: 7.52974e-01                   \n",
    "Test [1, 10.000000] training accuracy=0.755000 test accuracy=0.777500 training loss=4.46887e+01, test loss=2.69561e+01 arcitecture_reduction: 8.37857e-01                  \n",
    "Test [1, 15.000000] training accuracy=0.845000 test accuracy=0.792500 training loss=2.14885e+01, test loss=1.61794e+01 arcitecture_reduction: 8.75440e-01                  \n",
    "Test [1, 20.000000] training accuracy=0.847500 test accuracy=0.807500 training loss=1.42387e+01, test loss=1.23288e+01 arcitecture_reduction: 8.91240e-01                  \n",
    "\n",
    "...\n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:32<00:00,  1.35it/s]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [39:47<00:00, 95.48s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:05<00:00,  4.36it/s]\n",
    "test_accuracy=0.8716\n",
    "```\n",
    "- Train from scratch to a target structure\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.25 -epochs=50 -prune=False -learning_rate=1e-2 -model_dest=nas_20220119_none_rn50_ts25_00\n",
    "```\n",
    "- Pruning too aggresively & reporting an incorrect size.  Switch conv_weights back to product of norm and weight scale rather than average.  Both should be values from 0 to 1.  \n",
    "``` python\n",
    "def ArchitectureWeights(self):\n",
    "        #conv_weights = (torch.tanh(self.weight_gain*norm)+weight_scale)/2.0\n",
    "        conv_weights = torch.tanh(self.weight_gain*norm)*weight_scale\n",
    "```\n",
    "- Optimizing to 0 size network at a small architecture weight\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.0 -epochs=50 -prune=False -model_dest=nas_20220119_rn50_ts0_0- -search_structure=True -k_structure=1.0e1\n",
    "...\n",
    "Test [50, 115.000000] training accuracy=0.877500 test accuracy=0.847500 training loss=4.16946e-01, test loss=4.57426e-01 arcitecture_reduction: 3.66704e-02                                                                  \n",
    "Test [50, 120.000000] training accuracy=0.837500 test accuracy=0.830000 training loss=4.59586e-01, test loss=4.99954e-01 arcitecture_reduction: 3.67033e-02                                                                  \n",
    "Test [50, 125.000000] training accuracy=0.867500 test accuracy=0.797500 training loss=4.28933e-01, test loss=5.90774e-01 arcitecture_reduction: 3.66914e-02                                                                  \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [1:28:45<00:00, 106.51s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  4.08it/s]\n",
    "test_accuracy=0.8242\n",
    "```\n",
    "- The tensorboard shows that the training and test are still well aligned.  Because the network minimization will be so drastic, I want to continue training at a smaller learning rate before pruning\n",
    "![Training tensorboard](../img/nas_20220119_rn50_ts0_0-tb.png)\n",
    "![Training final class weights](../img/nas_20220119_rn50_ts0_0-cw.png)\n",
    "\n",
    "- Continued training\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.0 -epochs=25 -model_src=nas_20220119_rn50_ts0_0- -model_dest=nas_20220119_rn50_ts0_1- -search_structure=True -learning_rate=1e-3\n",
    "```\n",
    "- Stop after 10 epochs because test magnitude increase has stalled. \n",
    "``` cmd\n",
    "Train epochs:  40%|███████████████████████████████████████████████████████████████████▌                                                                                                     | 10/25 [18:02<26:42, 106.86s/it^Train steps:  15%|█████████████████████████▌                                                                                                                                              | 19/125.0 [00:16<01:30,  1.17it/s]\n",
    "Train epochs:  40%|███████████████████████████████████████████████████████████████████▌                                                                                                     | 10/25 [18:06<27:09, 108.65s/it]\n",
    "```\n",
    "- There has been a slight increase in \"architecture_loss\" which is why I performed the finer training - Add the weights that are needed for higher accuracy.  \n",
    "![Class Weights](../img/nas_20220119_rn50_ts0_1-cw.png)\n",
    "-  Try it a second time with learning reat 1e-4:\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.0 -epochs=10 -model_src=nas_20220119_rn50_ts0_1- -model_dest=nas_20220119_rn50_ts0_2- -search_structure=True -learning_rate=1e-4\n",
    "...\n",
    "Total Trainable Params: 22183066\n",
    "...\n",
    "Test [10, 125.000000] training accuracy=0.890000 test accuracy=0.865000 training loss=2.65042e-01, test loss=4.38974e-01 arcitecture_reduction: 3.75011e-02                                                                  \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:46<00:00,  1.18it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [17:46<00:00, 106.66s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  3.89it/s]\n",
    "test_accuracy=0.8573\n",
    "\n",
    "```\n",
    "- Lots of time, not much improvement.  Lets prune and see what happens\n",
    "``` cmd\n",
    "py networks/cell2d.py -prune=True -epochs=25 -model_src=nas_20220119_rn50_ts0_2- -model_dest=nas_20220119_rn50_ts0_3- -search_structure=False -learning_rate=1e-2\n",
    "...\n",
    "Total Trainable Params: 1712585\n",
    "Reduced parameters 1712585/22183066 = 0.0772023578706388\n",
    "...\n",
    "Test [1, 5.000000] training accuracy=0.402500 test accuracy=0.465000 training loss=1.18123e+01, test loss=1.10280e+01 arcitecture_reduction: 2.43444e-02                                                                     \n",
    "Test [1, 10.000000] training accuracy=0.475000 test accuracy=0.515000 training loss=1.09074e+01, test loss=1.08152e+01 arcitecture_reduction: 2.43531e-02                                                                    \n",
    "Test [1, 15.000000] training accuracy=0.577500 test accuracy=0.585000 training loss=1.07588e+01, test loss=1.06765e+01 arcitecture_reduction: 2.43858e-02                                                                    \n",
    "Test [1, 20.000000] training accuracy=0.555000 test accuracy=0.607500 training loss=1.06855e+01, test loss=1.05820e+01 arcitecture_reduction: 2.44622e-02                                                                    \n",
    "Test [1, 25.000000] training accuracy=0.637500 test accuracy=0.667500 training loss=1.05531e+01, test loss=1.04909e+01 arcitecture_reduction: 2.45635e-02                                                                    \n",
    "Test [1, 30.000000] training accuracy=0.672500 test accuracy=0.670000 training loss=1.05048e+01, test loss=1.05151e+01 arcitecture_reduction: 2.46570e-02                                                                    \n",
    "Test [1, 35.000000] training accuracy=0.712500 test accuracy=0.605000 training loss=1.04531e+01, test loss=1.06228e+01 arcitecture_reduction: 2.47624e-02                                                                    \n",
    "Test [1, 40.000000] training accuracy=0.690000 test accuracy=0.720000 training loss=1.04491e+01, test loss=1.03660e+01 arcitecture_reduction: 2.48824e-02                                                                    \n",
    "Test [1, 45.000000] training accuracy=0.715000 test accuracy=0.705000 training loss=1.03513e+01, test loss=1.03664e+01 arcitecture_reduction: 2.50210e-02                                                                    \n",
    "Test [1, 50.000000] training accuracy=0.700000 test accuracy=0.677500 training loss=1.03175e+01, test loss=1.03993e+01 arcitecture_reduction: 2.51657e-02                                                                    \n",
    "Test [1, 55.000000] training accuracy=0.727500 test accuracy=0.740000 training loss=1.02908e+01, test loss=1.03112e+01 arcitecture_reduction: 2.53102e-02                                                                    \n",
    "Test [1, 60.000000] training accuracy=0.770000 test accuracy=0.750000 training loss=1.02227e+01, test loss=1.02677e+01 arcitecture_reduction: 2.54572e-02\n",
    "...\n",
    "Test [25, 120.000000] training accuracy=0.890000 test accuracy=0.837500 training loss=8.92025e+00, test loss=9.08880e+00 arcitecture_reduction: 7.18184e-02                                                                  \n",
    "Test [25, 125.000000] training accuracy=0.862500 test accuracy=0.835000 training loss=8.97310e+00, test loss=9.11956e+00 arcitecture_reduction: 7.18398e-02                                                                  \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:11<00:00,  1.75it/s]\n",
    "Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [29:44<00:00, 71.37s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:04<00:00,  5.47it/s]\n",
    "test_accuracy=0.8394\n",
    "```\n",
    "\n",
    "- Looks like we maintained reasonable accuracy are may climb to a similar accuracy\n",
    "- Even in the test, we got an inference speedup.  \n",
    "- Train a little at 1e-3 learning rate to see if that gives us the final little boost to the same as before\n",
    "- Increae the batch size to 800 since we now have a much smaller network\n",
    "![Tensorboard](../img/nas_20220119_rn50_ts0_3-tb.png)\n",
    "![Gradient norm](../img/nas_20220119_rn50_ts0_3-gn.png)\n",
    "\n",
    "``` cmd\n",
    "py networks/cell2d.py -prune=False -epochs=10 -model_src=nas_20220119_rn50_ts0_3- -model_dest=nas_20220119_rn50_ts0_4- -search_structure=False -learning_rate=1e-3 -batch_size=800\n",
    "...\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|        cells.0.cnn.0.channel_scale        |     64     |\n",
    "|         cells.0.cnn.0.conv.weight         |    1728    |\n",
    "|          cells.0.cnn.0.conv.bias          |     64     |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.0.cnn.1.channel_scale        |     64     |\n",
    "|         cells.0.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.0.cnn.1.conv.bias          |     64     |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.0.conv_residual.channel_scale    |     64     |\n",
    "|     cells.0.conv_residual.conv.weight     |    192     |\n",
    "|      cells.0.conv_residual.conv.bias      |     64     |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|        cells.1.cnn.0.channel_scale        |     64     |\n",
    "|         cells.1.cnn.0.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.0.conv.bias          |     64     |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.1.cnn.1.channel_scale        |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.1.conv.bias          |     64     |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.1.conv_residual.channel_scale    |     64     |\n",
    "|     cells.1.conv_residual.conv.weight     |    4096    |\n",
    "|      cells.1.conv_residual.conv.bias      |     64     |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|        cells.2.cnn.0.channel_scale        |     64     |\n",
    "|         cells.2.cnn.0.conv.weight         |   36864    |\n",
    "|          cells.2.cnn.0.conv.bias          |     64     |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.2.cnn.1.channel_scale        |     64     |\n",
    "|         cells.2.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.2.cnn.1.conv.bias          |     64     |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.2.conv_residual.channel_scale    |     64     |\n",
    "|     cells.2.conv_residual.conv.weight     |    4096    |\n",
    "|      cells.2.conv_residual.conv.bias      |     64     |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|        cells.3.cnn.0.channel_scale        |    128     |\n",
    "|         cells.3.cnn.0.conv.weight         |   73728    |\n",
    "|          cells.3.cnn.0.conv.bias          |    128     |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.3.cnn.1.channel_scale        |    128     |\n",
    "|         cells.3.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.3.cnn.1.conv.bias          |    128     |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.3.conv_residual.channel_scale    |    128     |\n",
    "|     cells.3.conv_residual.conv.weight     |    8192    |\n",
    "|      cells.3.conv_residual.conv.bias      |    128     |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|        cells.4.cnn.0.channel_scale        |    128     |\n",
    "|         cells.4.cnn.0.conv.weight         |   147456   |\n",
    "|          cells.4.cnn.0.conv.bias          |    128     |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.4.cnn.1.channel_scale        |    128     |\n",
    "|         cells.4.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.4.cnn.1.conv.bias          |    128     |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.4.conv_residual.channel_scale    |    128     |\n",
    "|     cells.4.conv_residual.conv.weight     |   16384    |\n",
    "|      cells.4.conv_residual.conv.bias      |    128     |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|        cells.5.cnn.0.channel_scale        |    128     |\n",
    "|         cells.5.cnn.0.conv.weight         |   147456   |\n",
    "|          cells.5.cnn.0.conv.bias          |    128     |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.5.cnn.1.channel_scale        |    128     |\n",
    "|         cells.5.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.5.cnn.1.conv.bias          |    128     |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.5.conv_residual.channel_scale    |    128     |\n",
    "|     cells.5.conv_residual.conv.weight     |   16384    |\n",
    "|      cells.5.conv_residual.conv.bias      |    128     |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|        cells.6.cnn.0.channel_scale        |    128     |\n",
    "|         cells.6.cnn.0.conv.weight         |   147456   |\n",
    "|          cells.6.cnn.0.conv.bias          |    128     |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.6.cnn.1.channel_scale        |    128     |\n",
    "|         cells.6.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.6.cnn.1.conv.bias          |    128     |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.6.conv_residual.channel_scale    |    128     |\n",
    "|     cells.6.conv_residual.conv.weight     |   16384    |\n",
    "|      cells.6.conv_residual.conv.bias      |    128     |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|        cells.7.cnn.0.channel_scale        |    256     |\n",
    "|         cells.7.cnn.0.conv.weight         |   294912   |\n",
    "|          cells.7.cnn.0.conv.bias          |    256     |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.7.cnn.1.channel_scale        |    256     |\n",
    "|         cells.7.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.7.cnn.1.conv.bias          |    256     |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.7.conv_residual.channel_scale    |    256     |\n",
    "|     cells.7.conv_residual.conv.weight     |   32768    |\n",
    "|      cells.7.conv_residual.conv.bias      |    256     |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|        cells.8.cnn.0.channel_scale        |    256     |\n",
    "|         cells.8.cnn.0.conv.weight         |   589824   |\n",
    "|          cells.8.cnn.0.conv.bias          |    256     |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.8.cnn.1.channel_scale        |    256     |\n",
    "|         cells.8.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.8.cnn.1.conv.bias          |    256     |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.8.conv_residual.channel_scale    |    256     |\n",
    "|     cells.8.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.8.conv_residual.conv.bias      |    256     |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|        cells.9.cnn.0.channel_scale        |    256     |\n",
    "|         cells.9.cnn.0.conv.weight         |   589824   |\n",
    "|          cells.9.cnn.0.conv.bias          |    256     |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.9.cnn.1.channel_scale        |    256     |\n",
    "|         cells.9.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.9.cnn.1.conv.bias          |    256     |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.9.conv_residual.channel_scale    |    256     |\n",
    "|     cells.9.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.9.conv_residual.conv.bias      |    256     |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|        cells.10.cnn.0.channel_scale       |    256     |\n",
    "|         cells.10.cnn.0.conv.weight        |   589824   |\n",
    "|          cells.10.cnn.0.conv.bias         |    256     |\n",
    "|     cells.10.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.10.cnn.1.channel_scale       |    256     |\n",
    "|         cells.10.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.10.cnn.1.conv.bias         |    256     |\n",
    "|     cells.10.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.10.conv_residual.channel_scale   |    256     |\n",
    "|     cells.10.conv_residual.conv.weight    |   65536    |\n",
    "|      cells.10.conv_residual.conv.bias     |    256     |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|        cells.11.cnn.0.channel_scale       |    256     |\n",
    "|         cells.11.cnn.0.conv.weight        |   589824   |\n",
    "|          cells.11.cnn.0.conv.bias         |    256     |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.11.cnn.1.channel_scale       |    256     |\n",
    "|         cells.11.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.11.cnn.1.conv.bias         |    256     |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.11.conv_residual.channel_scale   |    256     |\n",
    "|     cells.11.conv_residual.conv.weight    |   65536    |\n",
    "|      cells.11.conv_residual.conv.bias     |    256     |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|        cells.12.cnn.0.channel_scale       |    256     |\n",
    "|         cells.12.cnn.0.conv.weight        |   589824   |\n",
    "|          cells.12.cnn.0.conv.bias         |    256     |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.12.cnn.1.channel_scale       |    256     |\n",
    "|         cells.12.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.12.cnn.1.conv.bias         |    256     |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.12.conv_residual.channel_scale   |    256     |\n",
    "|     cells.12.conv_residual.conv.weight    |   65536    |\n",
    "|      cells.12.conv_residual.conv.bias     |    256     |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|        cells.13.cnn.0.channel_scale       |    512     |\n",
    "|         cells.13.cnn.0.conv.weight        |  1179648   |\n",
    "|          cells.13.cnn.0.conv.bias         |    512     |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|        cells.13.cnn.1.channel_scale       |    512     |\n",
    "|         cells.13.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.13.cnn.1.conv.bias         |    512     |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|    cells.13.conv_residual.channel_scale   |    512     |\n",
    "|     cells.13.conv_residual.conv.weight    |   131072   |\n",
    "|      cells.13.conv_residual.conv.bias     |    512     |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |    512     |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|        cells.14.cnn.0.channel_scale       |    512     |\n",
    "|         cells.14.cnn.0.conv.weight        |  2359296   |\n",
    "|          cells.14.cnn.0.conv.bias         |    512     |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|        cells.14.cnn.1.channel_scale       |    512     |\n",
    "|         cells.14.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.14.cnn.1.conv.bias         |    512     |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|    cells.14.conv_residual.channel_scale   |    512     |\n",
    "|     cells.14.conv_residual.conv.weight    |   262144   |\n",
    "|      cells.14.conv_residual.conv.bias     |    512     |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |    512     |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|        cells.15.cnn.0.channel_scale       |    512     |\n",
    "|         cells.15.cnn.0.conv.weight        |  2359296   |\n",
    "|          cells.15.cnn.0.conv.bias         |    512     |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|        cells.15.cnn.1.channel_scale       |    512     |\n",
    "|         cells.15.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.15.cnn.1.conv.bias         |    512     |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|    cells.15.conv_residual.channel_scale   |    512     |\n",
    "|     cells.15.conv_residual.conv.weight    |   262144   |\n",
    "|      cells.15.conv_residual.conv.bias     |    512     |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |    512     |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|                fc.fc.weight               |    5120    |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 22183066\n",
    "\n",
    "Pruned to:\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|        cells.0.cnn.0.channel_scale        |     62     |\n",
    "|         cells.0.cnn.0.conv.weight         |    1674    |\n",
    "|          cells.0.cnn.0.conv.bias          |     62     |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     62     |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     62     |\n",
    "|        cells.0.cnn.1.channel_scale        |     63     |\n",
    "|         cells.0.cnn.1.conv.weight         |   35154    |\n",
    "|          cells.0.cnn.1.conv.bias          |     63     |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     63     |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     63     |\n",
    "|    cells.0.conv_residual.channel_scale    |     63     |\n",
    "|     cells.0.conv_residual.conv.weight     |    189     |\n",
    "|      cells.0.conv_residual.conv.bias      |     63     |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |     63     |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |     63     |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|        cells.1.cnn.0.channel_scale        |     64     |\n",
    "|         cells.1.cnn.0.conv.weight         |   36288    |\n",
    "|          cells.1.cnn.0.conv.bias          |     64     |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.1.cnn.1.channel_scale        |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.1.conv.bias          |     64     |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.1.conv_residual.channel_scale    |     64     |\n",
    "|     cells.1.conv_residual.conv.weight     |    4032    |\n",
    "|      cells.1.conv_residual.conv.bias      |     64     |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|        cells.2.cnn.0.channel_scale        |     62     |\n",
    "|         cells.2.cnn.0.conv.weight         |   35712    |\n",
    "|          cells.2.cnn.0.conv.bias          |     62     |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     62     |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     62     |\n",
    "|        cells.2.cnn.1.channel_scale        |     63     |\n",
    "|         cells.2.cnn.1.conv.weight         |   35154    |\n",
    "|          cells.2.cnn.1.conv.bias          |     63     |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     63     |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     63     |\n",
    "|    cells.2.conv_residual.channel_scale    |     63     |\n",
    "|     cells.2.conv_residual.conv.weight     |    4032    |\n",
    "|      cells.2.conv_residual.conv.bias      |     63     |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |     63     |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |     63     |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|        cells.3.cnn.0.channel_scale        |    105     |\n",
    "|         cells.3.cnn.0.conv.weight         |   59535    |\n",
    "|          cells.3.cnn.0.conv.bias          |    105     |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |    105     |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |    105     |\n",
    "|        cells.3.cnn.1.channel_scale        |    100     |\n",
    "|         cells.3.cnn.1.conv.weight         |   94500    |\n",
    "|          cells.3.cnn.1.conv.bias          |    100     |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |    100     |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |    100     |\n",
    "|    cells.3.conv_residual.channel_scale    |    100     |\n",
    "|     cells.3.conv_residual.conv.weight     |    6300    |\n",
    "|      cells.3.conv_residual.conv.bias      |    100     |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |    100     |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |    100     |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|        cells.4.cnn.0.channel_scale        |     70     |\n",
    "|         cells.4.cnn.0.conv.weight         |   63000    |\n",
    "|          cells.4.cnn.0.conv.bias          |     70     |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |     70     |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |     70     |\n",
    "|        cells.4.cnn.1.channel_scale        |     83     |\n",
    "|         cells.4.cnn.1.conv.weight         |   52290    |\n",
    "|          cells.4.cnn.1.conv.bias          |     83     |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |     83     |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |     83     |\n",
    "|    cells.4.conv_residual.channel_scale    |     83     |\n",
    "|     cells.4.conv_residual.conv.weight     |    8300    |\n",
    "|      cells.4.conv_residual.conv.bias      |     83     |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |     83     |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |     83     |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|        cells.5.cnn.0.channel_scale        |     52     |\n",
    "|         cells.5.cnn.0.conv.weight         |   38844    |\n",
    "|          cells.5.cnn.0.conv.bias          |     52     |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |     52     |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |     52     |\n",
    "|        cells.5.cnn.1.channel_scale        |     54     |\n",
    "|         cells.5.cnn.1.conv.weight         |   25272    |\n",
    "|          cells.5.cnn.1.conv.bias          |     54     |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |     54     |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |     54     |\n",
    "|    cells.5.conv_residual.channel_scale    |     54     |\n",
    "|     cells.5.conv_residual.conv.weight     |    4482    |\n",
    "|      cells.5.conv_residual.conv.bias      |     54     |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |     54     |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |     54     |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|        cells.6.cnn.0.channel_scale        |     12     |\n",
    "|         cells.6.cnn.0.conv.weight         |    5832    |\n",
    "|          cells.6.cnn.0.conv.bias          |     12     |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |     12     |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |     12     |\n",
    "|        cells.6.cnn.1.channel_scale        |     18     |\n",
    "|         cells.6.cnn.1.conv.weight         |    1944    |\n",
    "|          cells.6.cnn.1.conv.bias          |     18     |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |     18     |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |     18     |\n",
    "|    cells.6.conv_residual.channel_scale    |     18     |\n",
    "|     cells.6.conv_residual.conv.weight     |    972     |\n",
    "|      cells.6.conv_residual.conv.bias      |     18     |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |     18     |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |     18     |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|        cells.7.cnn.0.channel_scale        |     16     |\n",
    "|         cells.7.cnn.0.conv.weight         |    2592    |\n",
    "|          cells.7.cnn.0.conv.bias          |     16     |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |     16     |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |     16     |\n",
    "|        cells.7.cnn.1.channel_scale        |     26     |\n",
    "|         cells.7.cnn.1.conv.weight         |    3744    |\n",
    "|          cells.7.cnn.1.conv.bias          |     26     |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |     26     |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |     26     |\n",
    "|    cells.7.conv_residual.channel_scale    |     26     |\n",
    "|     cells.7.conv_residual.conv.weight     |    468     |\n",
    "|      cells.7.conv_residual.conv.bias      |     26     |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |     26     |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |     26     |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|        cells.8.cnn.0.channel_scale        |     97     |\n",
    "|         cells.8.cnn.0.conv.weight         |   22698    |\n",
    "|          cells.8.cnn.0.conv.bias          |     97     |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |     97     |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |     97     |\n",
    "|        cells.8.cnn.1.channel_scale        |     47     |\n",
    "|         cells.8.cnn.1.conv.weight         |   41031    |\n",
    "|          cells.8.cnn.1.conv.bias          |     47     |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |     47     |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |     47     |\n",
    "|    cells.8.conv_residual.channel_scale    |     47     |\n",
    "|     cells.8.conv_residual.conv.weight     |    1222    |\n",
    "|      cells.8.conv_residual.conv.bias      |     47     |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |     47     |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |     47     |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|        cells.9.cnn.0.channel_scale        |     10     |\n",
    "|         cells.9.cnn.0.conv.weight         |    4230    |\n",
    "|          cells.9.cnn.0.conv.bias          |     10     |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |     10     |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |     10     |\n",
    "|        cells.9.cnn.1.channel_scale        |     17     |\n",
    "|         cells.9.cnn.1.conv.weight         |    1530    |\n",
    "|          cells.9.cnn.1.conv.bias          |     17     |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |     17     |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |     17     |\n",
    "|    cells.9.conv_residual.channel_scale    |     17     |\n",
    "|     cells.9.conv_residual.conv.weight     |    799     |\n",
    "|      cells.9.conv_residual.conv.bias      |     17     |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |     17     |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |     17     |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|    cells.10.conv_residual.channel_scale   |    256     |\n",
    "|     cells.10.conv_residual.conv.weight    |    4352    |\n",
    "|      cells.10.conv_residual.conv.bias     |    256     |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|        cells.11.cnn.0.channel_scale       |     8      |\n",
    "|         cells.11.cnn.0.conv.weight        |   18432    |\n",
    "|          cells.11.cnn.0.conv.bias         |     8      |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |     8      |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |     8      |\n",
    "|        cells.11.cnn.1.channel_scale       |     6      |\n",
    "|         cells.11.cnn.1.conv.weight        |    432     |\n",
    "|          cells.11.cnn.1.conv.bias         |     6      |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |     6      |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |     6      |\n",
    "|    cells.11.conv_residual.channel_scale   |     6      |\n",
    "|     cells.11.conv_residual.conv.weight    |    1536    |\n",
    "|      cells.11.conv_residual.conv.bias     |     6      |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |     6      |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |     6      |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|        cells.12.cnn.0.channel_scale       |     42     |\n",
    "|         cells.12.cnn.0.conv.weight        |    2268    |\n",
    "|          cells.12.cnn.0.conv.bias         |     42     |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |     42     |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |     42     |\n",
    "|        cells.12.cnn.1.channel_scale       |     23     |\n",
    "|         cells.12.cnn.1.conv.weight        |    8694    |\n",
    "|          cells.12.cnn.1.conv.bias         |     23     |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |     23     |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |     23     |\n",
    "|    cells.12.conv_residual.channel_scale   |     23     |\n",
    "|     cells.12.conv_residual.conv.weight    |    138     |\n",
    "|      cells.12.conv_residual.conv.bias     |     23     |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |     23     |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |     23     |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|        cells.13.cnn.0.channel_scale       |    299     |\n",
    "|         cells.13.cnn.0.conv.weight        |   61893    |\n",
    "|          cells.13.cnn.0.conv.bias         |    299     |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |    299     |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |    299     |\n",
    "|        cells.13.cnn.1.channel_scale       |     78     |\n",
    "|         cells.13.cnn.1.conv.weight        |   209898   |\n",
    "|          cells.13.cnn.1.conv.bias         |     78     |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |     78     |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |     78     |\n",
    "|    cells.13.conv_residual.channel_scale   |     78     |\n",
    "|     cells.13.conv_residual.conv.weight    |    1794    |\n",
    "|      cells.13.conv_residual.conv.bias     |     78     |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |     78     |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |     78     |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|        cells.14.cnn.0.channel_scale       |    320     |\n",
    "|         cells.14.cnn.0.conv.weight        |   224640   |\n",
    "|          cells.14.cnn.0.conv.bias         |    320     |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |    320     |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |    320     |\n",
    "|        cells.14.cnn.1.channel_scale       |     34     |\n",
    "|         cells.14.cnn.1.conv.weight        |   97920    |\n",
    "|          cells.14.cnn.1.conv.bias         |     34     |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |     34     |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |     34     |\n",
    "|    cells.14.conv_residual.channel_scale   |     34     |\n",
    "|     cells.14.conv_residual.conv.weight    |    2652    |\n",
    "|      cells.14.conv_residual.conv.bias     |     34     |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |     34     |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |     34     |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|        cells.15.cnn.0.channel_scale       |    286     |\n",
    "|         cells.15.cnn.0.conv.weight        |   87516    |\n",
    "|          cells.15.cnn.0.conv.bias         |    286     |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |    286     |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |    286     |\n",
    "|        cells.15.cnn.1.channel_scale       |    133     |\n",
    "|         cells.15.cnn.1.conv.weight        |   342342   |\n",
    "|          cells.15.cnn.1.conv.bias         |    133     |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |    133     |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |    133     |\n",
    "|    cells.15.conv_residual.channel_scale   |    133     |\n",
    "|     cells.15.conv_residual.conv.weight    |    4522    |\n",
    "|      cells.15.conv_residual.conv.bias     |    133     |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |    133     |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |    133     |\n",
    "|                fc.fc.weight               |    1330    |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 1712585\n",
    "Reduced parameters 1712585/22183066 = 0.0772023578706388\n",
    "Train epochs:   0%|                                                                                                                                                                                   | 0/10 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 6.000000] training accuracy=0.896250 test accuracy=0.852500 training loss=8.91533e+00, test loss=9.03551e+00 arcitecture_reduction: 7.18547e-02                                                                     \n",
    "Test [1, 12.000000] training accuracy=0.916250 test accuracy=0.848750 training loss=8.88420e+00, test loss=9.04996e+00 arcitecture_reduction: 7.18693e-02 \n",
    "...\n",
    "Test [10, 54.000000] training accuracy=0.930000 test accuracy=0.862500 training loss=8.80388e+00, test loss=9.03733e+00 arcitecture_reduction: 7.27267e-02                                                                   \n",
    "Test [10, 60.000000] training accuracy=0.926250 test accuracy=0.851250 training loss=8.80574e+00, test loss=9.05619e+00 arcitecture_reduction: 7.27321e-02                                                                   \n",
    "Train steps: 63it [00:54,  1.15it/s]████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 9/10 [09:12<00:55, 55.28s/it]\n",
    "Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [09:14<00:00, 55.48s/it]\n",
    "Test steps: 13it [00:04,  2.82it/s]                                                                                                                                                                                          \n",
    "test_accuracy=0.9022916666666667\n",
    "```\n",
    "- Test accuracy of 90% is unexpected!  I thought I needed to switch to multi-stage training with sgd and pretraining before I would pass into the 90% test accuracy on CIFAR-10, not prune a network to nearly nothing.\n",
    "- Training gradients are distributed throughout the pruned network with a fairly uniform magnitude\n",
    "- Test accuracy and cross entropy loss began to degrade after iteration 400\n",
    "![Final pruned network gradient norm](../img/nas_20220119_rn50_ts0_4-gn.png)\n",
    "![Final training Tensorboard](../img/nas_20220119_rn50_ts0_4-tb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b30790-ccb5-4eb5-9449-9ce3e1023517",
   "metadata": {},
   "source": [
    "21 January 2022\n",
    "- \n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=75 -prune=False -search_structure=True -learning_rate=1e-2 -model_dest=crisp20220121_t50_00\n",
    "...\n",
    "Test [75, 120.000000] training accuracy=0.900000 test accuracy=0.845000 training loss=2.32706e-01, test loss=4.64866e-01 arcitecture_reduction: 5.00009e-01                                            \n",
    "Test [75, 125.000000] training accuracy=0.925000 test accuracy=0.832500 training loss=2.29566e-01, test loss=5.13253e-01 arcitecture_reduction: 4.99894e-01                                            \n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [2:12:41<00:00, 106.15s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  4.02it/s]\n",
    "test_accuracy=0.8537\n",
    "\n",
    "```\n",
    "- Training to 0.5 class weights\n",
    "\n",
    "![](../img/crisp20220121_t50_00_cw.png)\n",
    "![](../img/crisp20220121_t50_00_tb.png)\n",
    "\n",
    "- Prune \n",
    "\n",
    "```\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=25 -prune=True -search_structure=False -learning_rate=1e-2 -model_src=crisp20220121_t50_00 -model_dest=crisp20220121_t50_01\n",
    "...\n",
    "Total Trainable Params: 13376476\n",
    "Reduced parameters 13376476/22183066 = 0.6030039310165691\n",
    "Train epochs:   0%|                                                                                                                                                             | 0/25 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.887500 test accuracy=0.837500 training loss=4.37770e-01, test loss=5.90980e-01 arcitecture_reduction: 4.01812e-01                                               \n",
    "Test [1, 10.000000] training accuracy=0.917500 test accuracy=0.862500 training loss=3.57989e-01, test loss=5.13867e-01 arcitecture_reduction: 4.08487e-01                                              \n",
    "Test [1, 15.000000] training accuracy=0.902500 test accuracy=0.822500 training loss=3.51265e-01, test loss=6.14315e-01 arcitecture_reduction: 4.15208e-01\n",
    "...\n",
    "Test [25, 115.000000] training accuracy=0.947500 test accuracy=0.872500 training loss=1.28096e-01, test loss=4.63626e-01 arcitecture_reduction: 4.99841e-01                                            \n",
    "Test [25, 120.000000] training accuracy=0.957500 test accuracy=0.815000 training loss=1.44876e-01, test loss=5.65249e-01 arcitecture_reduction: 4.99830e-01                                            \n",
    "Test [25, 125.000000] training accuracy=0.917500 test accuracy=0.862500 training loss=1.69795e-01, test loss=4.57527e-01 arcitecture_reduction: 4.99909e-01                                            \n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:33<00:00,  1.34it/s]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [39:54<00:00, 95.79s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:05<00:00,  4.40it/s]\n",
    "test_accuracy=0.8586\n",
    "Finished cell2d Test\n",
    "\n",
    "```\n",
    "\n",
    "- Training and test accuracy staied practiclly unchanged\n",
    "- Cut short training when test accuracy and lost begins to rise and then final training with 1e-3 learning rate\n",
    "\n",
    "```\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=10 -prune=True -search_structure=False -learning_rate=1e-3 -model_src=crisp20220121_t50_01 -model_dest=crisp20220121_t50_02 \n",
    "...\n",
    "Test [10, 120.000000] training accuracy=0.987500 test accuracy=0.862500 training loss=6.64550e-02, test loss=5.17078e-01 arcitecture_reduction: 4.99998e-01                                            \n",
    "Test [10, 125.000000] training accuracy=0.972500 test accuracy=0.875000 training loss=9.10177e-02, test loss=4.40394e-01 arcitecture_reduction: 5.00002e-01                                            \n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:33<00:00,  1.33it/s]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [15:58<00:00, 95.88s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:05<00:00,  4.41it/s]\n",
    "test_accuracy=0.8773\n",
    "```\n",
    "\n",
    "![60% pruned class weights](../img/crisp20220121_t50_02_gn.png)\n",
    "\n",
    "- Pruned middle channels so much that instead of an increase in channels, there is a decrease.\n",
    "- Kept the full width on the final (far right section especially near the jump in channels.\n",
    "- Repeat same process for pruning 75%\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.25 -epochs=75 -prune=False -search_structure=True -learning_rate=1e-2 -model_dest=crisp20220121_t25_00\n",
    "...\n",
    "Test [75, 120.000000] training accuracy=0.902500 test accuracy=0.865000 training loss=2.17066e-01, test loss=4.69373e-01 arcitecture_reduction: 2.51623e-01                                            \n",
    "Test [75, 125.000000] training accuracy=0.907500 test accuracy=0.877500 training loss=2.20994e-01, test loss=4.40662e-01 arcitecture_reduction: 2.51564e-01                                            \n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:44<00:00,  1.19it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [2:17:58<00:00, 110.38s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  4.06it/s]\n",
    "test_accuracy=0.8637\n",
    "```\n",
    "- Clean convergence\n",
    "- Will prune out the middle of the last two resnet levels\n",
    "![Target .25 Gradient Norm](../img/crisp20220121_t25_00_gn.png)\n",
    "![Target .25 Class Weights](../img/crisp20220121_t25_00_cw.png)\n",
    "![Target .25 Tensorboard](../img/crisp20220121_t25_00_tb.png)\n",
    "- Prune and train for 25 epochs at 1e-2 learning rate\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=25 -prune=True -search_structure=False -learning_rate=1e-2 -model_src=crisp20220121_t25_00 -model_dest=crisp20220121_t25_01\n",
    "...\n",
    "Total Trainable Params: 9047900\n",
    "Reduced parameters 9047900/22183066 = 0.40787418655293184\n",
    "Train epochs:   0%|                                                                                                                                                             | 0/25 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.920000 test accuracy=0.845000 training loss=6.92640e+00, test loss=7.11933e+00 arcitecture_reduction: 1.84899e-01                                               \n",
    "Train epochs:   0%|                                                                                                                                                             | 0/25 [00:06<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 10.000000] training accuracy=0.922500 test accuracy=0.847500 training loss=6.87471e+00, test loss=7.03949e+00 arcitecture_reduction: 1.89374e-01                                              \n",
    "Test [1, 15.000000] training accuracy=0.927500 test accuracy=0.847500 training loss=6.77528e+00, test loss=6.99641e+00 arcitecture_reduction: 1.94217e-01                                              \n",
    "Test [1, 20.000000] training accuracy=0.932500 test accuracy=0.870000 training loss=6.69325e+00, test loss=6.78030e+00 arcitecture_reduction: 1.99556e-01                                              \n",
    "T\n",
    "```\n",
    "- 60% compression, preserved accuracy follwing pruning\n",
    "``` cmd\n",
    "est [25, 115.000000] training accuracy=0.952500 test accuracy=0.895000 training loss=3.89919e+00, test loss=4.12011e+00 arcitecture_reduction: 3.86886e-01                                            \n",
    "Test [25, 120.000000] training accuracy=0.937500 test accuracy=0.845000 training loss=3.90302e+00, test loss=4.37877e+00 arcitecture_reduction: 3.86887e-01                                            \n",
    "Test [25, 125.000000] training accuracy=0.930000 test accuracy=0.880000 training loss=3.90966e+00, test loss=4.23690e+00 arcitecture_reduction: 3.86887e-01                                            \n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [02:05<00:00,  1.00s/it]\n",
    "Train epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [52:55<00:00, 127.03s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:32<00:00,  1.30s/it]\n",
    "test_accuracy=0.8694\n",
    "```\n",
    "- 10 Epchs training with 1e-3 learning rate\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=10 -prune=True -search_structure=False -learning_rate=1e-3 -model_src=crisp20220121_t25_01 -model_dest=crisp20220121_t25_02 -debug\n",
    "...\n",
    "Test [10, 115.000000] training accuracy=0.982500 test accuracy=0.900000 training loss=3.82421e+00, test loss=4.13265e+00 arcitecture_reduction: 3.86929e-01                                            \n",
    "Test [10, 120.000000] training accuracy=0.977500 test accuracy=0.875000 training loss=3.81356e+00, test loss=4.28123e+00 arcitecture_reduction: 3.86929e-01                                            \n",
    "Test [10, 125.000000] training accuracy=0.987500 test accuracy=0.895000 training loss=3.82077e+00, test loss=4.24202e+00 arcitecture_reduction: 3.86929e-01                                            \n",
    "Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:59<00:00,  1.05it/s]\n",
    "Train epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [20:18<00:00, 121.84s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:06<00:00,  3.94it/s]\n",
    "test_accuracy=0.8831\n",
    "```\n",
    "- Nice to get a couple of percentage boost from this\n",
    "- Pruning created a second bottleneck in the same position\n",
    "![](../img/crisp20220121_t25_02_gn.png)\n",
    "- Next prune even more to find the point the performance degrades\n",
    "- Set -target_structure to 0.01\n",
    "- Boost -k_structure from 1e1 to 1e3 so it forces the network size over model accuracy\n",
    "```cmd\n",
    "py networks/cell2d.py -epochs=75 -prune=False -search_structure=True -target_structure=0.01 -k_structure=1e3  -learning_rate=1e-2 -model_dest=crisp20220121_t01_00\n",
    "...\n",
    "Test [75, 120.000000] training accuracy=0.902500 test accuracy=0.845000 training loss=2.76101e-01, test loss=4.96990e-01 arcitecture_reduction: 1.10572e-02                                                                                 \n",
    "Test [75, 125.000000] training accuracy=0.887500 test accuracy=0.845000 training loss=2.96646e-01, test loss=4.88102e-01 arcitecture_reduction: 1.10523e-02                                                                                 \n",
    "Train steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [02:21<00:00,  1.13s/it]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [2:54:07<00:00, 139.30s/it]█| 125/125.0 [02:21<00:00,  1.48s/it]\n",
    "Test steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:07<00:00,  3.38it/s]\n",
    "test_accuracy=0.8552\n",
    "```\n",
    "- The traing was suprisngly normal.\n",
    "![1% strcture unpruned class weigthts](../img/crisp20220121_t01_00_cw.png)\n",
    "![1% strcture unpruned gradient norm](../img/crisp20220121_t01_00_gn.png)\n",
    "![1% strcture unpruned tensorboard](../img/crisp20220121_t01_00_tb.png)\n",
    "- Reduce learning rate from 1e-2 to 1e-3 and to a final training before pruning\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=10 -prune False -search_structure True -target_structure=0.01 -k_structure=1e3  -learning_rate=1e-3 -model_src=crisp20220121_t01_00 -model_dest=crisp20220121_t01_01\n",
    "...\n",
    "Test [10, 120.000000] training accuracy=0.957500 test accuracy=0.887500 training loss=1.41753e-01, test loss=4.24293e-01 arcitecture_reduction: 1.13174e-02                                                                                 \n",
    "Test [10, 125.000000] training accuracy=0.932500 test accuracy=0.867500 training loss=1.61560e-01, test loss=4.10597e-01 arcitecture_reduction: 1.13180e-02                                                                                 \n",
    "Train steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [02:20<00:00,  1.12s/it]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [23:40<00:00, 142.10s/it]\n",
    "Test steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:07<00:00,  3.45it/s]\n",
    "test_accuracy=0.8734\n",
    "```\n",
    "- Prune and train after pruning:\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=10 -prune True -search_structure False -target_structure=0.01 -k_structure=1e3  -learning_rate=1e-2 -model_src=crisp20220121_t01_01 -model_dest=crisp20220121_t01_02\n",
    "...\n",
    "Total Trainable Params: 3071136\n",
    "Reduced parameters 3071136/22183066 = 0.13844506435674853\n",
    "...\n",
    "Total Trainable Params: 3071136\n",
    "Reduced parameters 3071136/22183066 = 0.13844506435674853\n",
    "Train epochs:   0%|                                                                                                                                                                                                  | 0/10 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.330000 test accuracy=0.347500 training loss=2.50454e+00, test loss=1.86002e+00 arcitecture_reduction: 9.01897e-03                                                                                    \n",
    "Test [1, 10.000000] training accuracy=0.367500 test accuracy=0.405000 training loss=1.79165e+00, test loss=1.65403e+00 arcitecture_reduction: 8.95732e-03                                                                                   \n",
    "Test [1, 15.000000] training accuracy=0.462500 test accuracy=0.520000 training loss=1.55283e+00, test loss=1.42762e+00 arcitecture_reduction: 8.91929e-03                                                                                   \n",
    "Test [1, 20.000000] training accuracy=0.552500 test accuracy=0.515000 training loss=1.40417e+00, test loss=1.30682e+00 arcitecture_reduction: 8.90896e-03                                                                                   \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae4c22-202a-465e-876d-874ea3fdafb3",
   "metadata": {},
   "source": [
    "25 Jan, 2022\n",
    "- Gave algorim name: Continuous Relaxation in Structured Pruning - CRISP\n",
    "- The plot below shows structured Resenet 50 on CIFAR-10 data set\n",
    "- Pruning accurcy improves to 93% compression then a rapid drop.\n",
    "- Poor convergence in my resnet 152 structure,  I believe this is because I have 1x1 convolutions in the residual path.  Restructure the residual path\n",
    "1. Get training to work on hiocnn\n",
    "1. Try algorithms claiming improved results vs adam\n",
    "1. Try pretraining (coco/imagenet) before CIFAR-10\n",
    "1. Move to segment/denoise network\n",
    "1. Low-light image enhancement\n",
    "1. Will mlflow help to understand and track experaments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09df6399-b1c6-4cdd-82ca-6efad054b99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Baseline\n",
    "x = [.50, 1.0] \n",
    "y = [.77,.77] \n",
    "plt.plot(x, y, color='purple')\n",
    "\n",
    "#Variational Dropout\n",
    "x = [.50, 0.8, .90, .979, .98] \n",
    "y = [.769,.76, .74, .61, .60] \n",
    "plt.plot(x, y, color='blue', label='variational dropout')\n",
    "\n",
    "#Magnitude Pruning\n",
    "x = [.50, 0.8, .90, .975] \n",
    "y = [.769,.76, .74, .59] \n",
    "plt.plot(x, y, color='Green', label='magnitude pruning')\n",
    "\n",
    "# Random Pruning\n",
    "x = [.50,.70, .80,.90,.95,.97] \n",
    "y = [.74,.725,.70,.65,.55,.45] \n",
    "plt.plot(x, y, color='yellow', label='random pruning')\n",
    "\n",
    "# CRISP\n",
    "y = [0.8773, 0.8831, 0.887, 0.8943, 0.8798, 0.8829, 0.8767, 0.8582, 0.8367, 0.7214,0.5562] \n",
    "x = [1-13376476/22183066, 1-9047900/22183066, 1-6686420/22183066 ,1-3484738/22183066, 1-2207584/22183066,1-1863924/22183066 , 1-1712585/22183066, 1-1392685/22183066, 1-1214003/22183066, 1-1139234/22183066, 1-1104412/22183066] \n",
    "x1, y1 = [1-13376476/22183066, 1.0], [0.8773, 0.8773]\n",
    "plt.plot(x, y, color='orange', label='CRISP (our method)')\n",
    "plt.title('Sparcity/Accuracy trade-off for Resnet-50')\n",
    "plt.ylabel('Top-1 Accuracy')\n",
    "plt.xlabel('Sparsity')\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832233e-b661-4870-be02-139839a41731",
   "metadata": {},
   "source": [
    "27 January 2022\n",
    "- created classify/crisp.py based on cell2d Test\n",
    "- Executing pytorch resnet classifier so I can have a reference on a common resnet implementaion\n",
    "- Testing performance\n",
    "- Model my prunable resnet on the pytorch resnet implementaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9ff16",
   "metadata": {},
   "source": [
    "29 January 2022\n",
    "- Restructured network to have a straight residual branch execpt for the 1x1 resize convolutions\n",
    "- Pruning weights are not properly accounting for collapsing a branch.  When branch collapses, not just the searchable networks collpase but the whole convolutional branch collapses.\n",
    "- Need to provide a weighting function where the weight of the convolutions become 0 when the channel becomes 0 and grows quickly to the maximum when the channel diverges from 0\n",
    "- Need a differential weight to wight the full collaps of a set of convolutions.\n",
    "- The layer weight provides a norm of each layer\n",
    "- conv_weights provide a maximumn value of each layer weight\n",
    "- The ration is a weighting of how much is left of each cnn\n",
    "- tanh provides a continuous scaling with a small gradient except where the value is near 0\n",
    "- Multiply the produce of these scalings by the architecture weight\n",
    "- \n",
    "\n",
    "\n",
    "``` python\n",
    "architecture_weights = architecture_weights.sum_to_size((1))\n",
    "cnn_weights = torch.tanh(cnn_weights*self.cell_convolution)\n",
    "architecture_weights *= torch.prod(cnn_weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f599447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm=0.08099694781629463 (0.08099694781629463,0.711870739835938), \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 1.0, 0.01) \n",
    "y = np.tanh(11*x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "conv_weights=[0.0806, 0.0232, 0.110, 0.0939, 0.0350, 0.0740, 0.0283, 0.0982, 0.1181]\n",
    "conv_weights = np.linalg.norm(conv_weights)/np.sqrt(len(conv_weights))\n",
    "conv_loss = np.tanh(11*conv_weights)\n",
    "print('norm={} ({},{}), '.format(conv_weights, conv_weights, conv_loss))\n",
    "conv_loc = plt.Circle((conv_weights, conv_loss), 0.01, color='r')\n",
    "ax = plt.gca()\n",
    "ax.add_patch(conv_loc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588f4f0",
   "metadata": {},
   "source": [
    "31 January 2022\n",
    "- Add visualization to convolution weighting function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf45a43",
   "metadata": {},
   "source": [
    "1 February 2020\n",
    "- Restrucured resnet so the residual path only encounters 1x1 convolutions on size changes to match the standard implementation\n",
    "- SVG jointly searched poorly - never pruned out a whole convolution path\n",
    "- Adam jointly searched model and pruning weights\n",
    "- Minimize booth error and size to zero\n",
    "- Propegates gradient norm more uniformaly across network\n",
    "- Need to autoscale gradient norm\n",
    "- Optimizer rate schedule working well: optim.lr_scheduler.MultiStepLR\n",
    "- Not achieving the same accuracy as before but this may be because the 8x downsizing to the beginning of resnet.\n",
    "- See what happens if I pull of the 8x downsizing convolution at the begining\n",
    "    ![](../img/crisp20220201_t00_01_tb.png)\n",
    "    ![](../img/crisp20220201_t00_01_cw.png)\n",
    "    ![](../img/crisp20220201_t00_01_gn.png)\n",
    "    ```cmd\n",
    "    [113/300,      6/62.5]  accuracy: 0.927500|0.826250 loss: 2.23489e-01|5.70975e-01 remaining: 5.51220e-02 (train|test)                                                                         \n",
    "    [113/300,     12/62.5]  accuracy: 0.916250|0.823750 loss: 2.22670e-01|6.13765e-01 remaining: 5.51220e-02 (train|test)                                                                         \n",
    "    [113/300,     18/62.5]  accuracy: 0.913750|0.805000 loss: 2.17565e-01|7.09836e-01 remaining: 5.51221e-02 (train|test)                                                                         \n",
    "    [113/300,     24/62.5]  accuracy: 0.928750|0.832500 loss: 2.18517e-01|6.02128e-01 remaining: 5.51221e-02 (train|test)                                                                         \n",
    "    Train epochs:  37%|█████████████████████████████████████████████████▋                                                                                   | 112/300 [2:15:23<3:46:13, 72.20s/it^Train steps:  37%|██████████████████████████████████████████████████▊                                                                                       | 23/62.5 [00:25<00:44,  1.13s/it]\n",
    "    Train epochs:  37%|█████████████████████████████████████████████████▋                                                                                   | 112/300 [2:15:24<3:47:18, 72.54s/it]\n",
    "    ```\n",
    "- Prune and see how the stats work out\n",
    "    ```cmd\n",
    "    Total Trainable Params: 4668400\n",
    "    Reduced parameters 4668400/23581658 = 0.19796742027214542\n",
    "    [  1/100,      6/62.5]  accuracy: 0.861250|0.768750 loss: 3.77656e-01|8.04519e-01 remaining: 7.85033e-02 (train|test)                                                                         \n",
    "    [  1/100,     12/62.5]  accuracy: 0.858750|0.792500 loss: 3.90868e-01|7.15877e-01 remaining: 7.83786e-02 (train|test)                                                                         \n",
    "    [  1/100,     18/62.5]  accuracy: 0.867500|0.761250 loss: 3.77579e-01|7.25845e-01 remaining: 7.83118e-02 (train|test)                                                                         \n",
    "    [  1/100,     24/62.5]  accuracy: 0.892500|0.791250 loss: 3.56796e-01|6.41781e-01 remaining: 7.82733e-02 (train|test)                                                                         \n",
    "\n",
    "    ```\n",
    "- 5% remaining pruned 80% of network\n",
    "- Accuracy remained good\n",
    "- Full convolution paths not pruned because all channels need to be removed first.  See what happens if there is a lower threshold\n",
    "- If it doesn't prune a full convolution path successfully, I could add and additional set of paramets to prune these out.\n",
    "- Peaked at 82%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04ea87",
   "metadata": {},
   "source": [
    "1 February 2022\n",
    "- Setting up mlflow\n",
    "```cmd\n",
    "$ helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "$ helm install mlf-db bitnami/postgresql --set postgresqlDatabase=mlflow_db --set postgresqlPassword=mlflow --set service.type=NodePort\n",
    "PostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:\n",
    "\n",
    "    mlf-db-postgresql.default.svc.cluster.local - Read/Write connection\n",
    "\n",
    "To get the password for \"postgres\" run:\n",
    "\n",
    "    export POSTGRES_PASSWORD=$(kubectl get secret --namespace default mlf-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n",
    "\n",
    "To connect to your database run the following command:\n",
    "\n",
    "    kubectl run mlf-db-postgresql-client --rm --tty -i --restart='Never' --namespace default --image docker.io/bitnami/postgresql:11.14.0-debian-10-r28 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host mlf-db-postgresql -U postgres -d mlflow_db -p 5432\n",
    "\n",
    "\n",
    "\n",
    "To connect to your database from outside the cluster execute the following commands:\n",
    "\n",
    "    export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n",
    "    export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].nodePort}\" services mlf-db-postgresql)\n",
    "    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host $NODE_IP --port $NODE_PORT -U postgres -d mlflow_db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5623999-b91d-4b73-9aef-9028672596eb",
   "metadata": {},
   "source": [
    "2 February 2022\n",
    "- Resent 152 training is progressing well.  Test accuracy 70% and rising\n",
    "- Removed resnet initial 7x7 convolution\n",
    "- Updated gradient norm plot with autoscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918f711",
   "metadata": {},
   "source": [
    "3 February 2022\n",
    "- With a erodible resnet with a cleaned up residual path\n",
    "- Resnet 152 trains without difficulties\n",
    "- The final convolution of a residual block must output the number of channels required to sum with the residual\n",
    "- We can still prune the entire residual function ([Deep Residual Learning for Image Recognition figure 5](https://arxiv.org/pdf/1512.03385.pdf)) convolution path\n",
    "- To reward the optimizer for removing an entire residual function in order to reduce model size, there must be a reduction of loss that increases as channel pruning approaches 0\n",
    "- Method: \n",
    "    1. store original size of a the ConvBR structure\n",
    "    1. If ConvBR.search_structure is True, weight decay is proportional to the remaining channels\n",
    "    1. If ConvBR.search_structure is False, its weight is apportioned among those that are true\n",
    "    1. If any of the convolutions in the residual function are 0, the whole convolution branch disappears\n",
    "    1. The product of the ration of current channels to total channels achieves this: \n",
    "        $$\n",
    "        \\newcommand{\\rem}{r=\\textrm{remaining channels}}\n",
    "        \\newcommand{\\all}{f=\\textrm{total channels}}\n",
    "        \\newcommand{\\aw}{a=\\textrm{architecture weights}}\n",
    "\n",
    "        aw = \\sum_{i = 1}^{m} x + K_i * tanh(\\prod_{j = 1}^{n} \\frac{ r_{j}}{ t_{j}})\n",
    "        \\\\\n",
    "        \\\\\n",
    "        \\rem{} \\\\\n",
    "        \\all{} \\\\\n",
    "        \\aw{}\n",
    "        $$\n",
    "    1. One different between the target size and the pruned size in this formation is due to the size reduction of the residual function not being accounted for\n",
    "    1. A second difference is the residual is pruned at a fixed point where the function is continuous.  The hyperbolic tangent reduces this differences and enables us to select a specific value to prune at. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ae605",
   "metadata": {},
   "source": [
    "7 February, 2022\n",
    "- SGD optimizer: optim.SGD\n",
    "- CIFAR-10 Resnet 56 default data augmentation max test accuracy 75%\n",
    "- CIFAR-10 Resnet 56 node data augmentation max test accuracy 87%\n",
    "- CIFAR-10 Resnet 56 0 mean, 1 std, translate 0.125, Nearest interpolation, test accuracy 85%\n",
    "    ```cmd\n",
    "    Train steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:37<00:00,  1.28it/s]\n",
    "    Train epochs: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [4:06:52<00:00, 98.75s/it]\n",
    "    Test steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.00it/s]\n",
    "    test_accuracy=0.8495\n",
    "    ```\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b88f72",
   "metadata": {},
   "source": [
    "8 February 2022\n",
    "- CIFAR-10 Resnet 0 mean, 1 std, translate 0.125, BILINEAR interpolation, 54 target architecture 10% max test accuracy 86%\n",
    "    ``` cmd\n",
    "    [134/150,     45/125.0]  accuracy: 0.985000|0.857500 loss: 8.80765e-02|6.11403e-01 remaining: 1.57671e-01 (train|test)                                                                                                                                      \n",
    "    [134/150,     50/125.0]  accuracy: 0.980000|0.825000 loss: 9.38828e-02|6.25705e-01 remaining: 1.57671e-01 (train|test)                                                                                                                                      \n",
    "    Train epochs:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                    | 133/150 [3:42:44<28:26, 100.36s/it]                  \n",
    "    ^Cain steps:  41%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                                                     | 51/125.0 [00:41<01:01,  1.21it/s]\n",
    "\n",
    "    ```\n",
    "- Pruning\n",
    "```cmd\n",
    "Total Trainable Params: 433606\n",
    "Reduced parameters 433606/467186 = 0.9281228461469307\n",
    "```\n",
    "- Getting about the same results with the minimal CIFAR-10 Resenet as the big resnet described in the [Resnet paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- I am getting about 2x the error as the Table 6 in the Resnet paper after applying the same parameters and preprocessing.\n",
    "- Sources of error: unoptimized parameters, error in network, pretraining, corrections in CIFAR, loss function\n",
    "- Pytorch Resnet had similar results \n",
    "- Moved to padding and random cropping.  test_accuracy=0.8231 did not improve:\n",
    "```cmd\n",
    "[ 80/80,    125/125.0]  accuracy: 0.957500|0.837500 loss: 1.65346e-01|6.19811e-01 remaining: 9.74377e-01 (train|test)████████████████████████████▉ | 124/125.0 [01:21<00:00,  1.60it/s]\n",
    "Train epochs:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 79/80 [1:50:07<01:22, 82.5Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:22<00:00,  1.52it/s]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [1:50:08<00:00, 82.5Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [1:50:08<00:00, 82.61s/it]\n",
    "Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.91it/s]\n",
    "test_accuracy=0.8231\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcdc0d",
   "metadata": {},
   "source": [
    "9 February 2022\n",
    "- 100% training \n",
    "    ```cmd\n",
    "        parser.add_argument('-augment_rotation', type=float, default=0.0, help='Input augmentation rotation degrees')\n",
    "        parser.add_argument('-augment_scale_min', type=float, default=1.00, help='Input augmentation scale')\n",
    "        parser.add_argument('-augment_scale_max', type=float, default=1.00, help='Input augmentation scale')\n",
    "        parser.add_argument('-augment_translate_x', type=float, default=0.25, help='Input augmentation translation')\n",
    "        parser.add_argument('-augment_translate_y', type=float, default=0.25, help='Input augmentation translation')\n",
    "        parser.add_argument('-augment_noise', type=float, default=0.0, help='Input augmentation rotation degrees')\n",
    "\n",
    "    [120/120,    125/125.0]  accuracy: 0.977500|0.817500 loss: 1.02125e-01|7.67410e-01 remaining: 9.73807e-01 (train|test)███████████████████████████▉ | 124/125.0 [01:26<00:00,  1.54it/s]\n",
    "    Train epochs:  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:27<00:00,  1.43it/s]\n",
    "    Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [5:24:31<00:00, 162.26s/it]\n",
    "    Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.81it/s]\n",
    "    test_accuracy=0.766\n",
    "    ```\n",
    "- The initial convolution had a residual branch.  I eliminated it and retrained 50 cycles: crisp20220209_t100_00 \\\n",
    "    ![Tensorboard](../img/crisp20220209_t100_00_tb.png)\n",
    "    ```cmd\n",
    "    [ 50/50,    120/125.0]  accuracy: 0.907500|0.810000 loss: 2.89950e-01|6.16024e-01 remaining: 9.74487e-01 (train|test)██████████████████████████████████████████████▋       | 119/125.0 [02:04<00:05,  1.04it/s]\n",
    "    Train epochs:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 49/50 [1:51:15<02:12, 133.00                                                                                                                                                                                                               [ 50/50,    125/125.0]  accuracy: 0.905000|0.837500 loss: 3.08497e-01|4.81070e-01 remaining: 9.74485e-01 (train|test)████████████████████████████████████████████████████▊ | 124/125.0 [02:09<00:00,  1.03it/s]\n",
    "    Train epochs:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊   | 49/50 [1:51:20<02:12, 133.00Train steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [02:11<00:00,  1.05s/it]\n",
    "    Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [1:51:21<00:00, 132.77Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [1:51:21<00:00, 133.63s/it]\n",
    "    Test steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  6.58it/s]\n",
    "    test_accuracy=0.8259\n",
    "    ```\n",
    "- Continue training: crisp20220209_t100_01\n",
    "    ```cmd\n",
    "    [ 40/40,    120/125.0]  accuracy: 0.985000|0.815000 loss: 6.32229e-02|7.06654e-01 remaining: 9.74219e-01 (train|test)          \n",
    "    [ 40/40,    125/125.0]  accuracy: 0.987500|0.857500 loss: 5.66682e-02|5.80368e-01 remaining: 9.74219e-01 (train|test)          \n",
    "    Train steps: 100%|█████████████████████████████████████████████████████████████████████████| 125/125.0 [01:27<00:00,  1.43it/s]\n",
    "    Train epochs: 100%|██████████████████████████████████████████████████████████████████████████| 40/40 [1:02:56<00:00, 94.40s/it]\n",
    "    Test steps: 100%|████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.50it/s]\n",
    "    test_accuracy=0.8676\n",
    "    ```\n",
    "- crisp20220209_t50_02 still has a target size of 100% but had a nice bump in accuracy.  Start with this one.\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.975000|0.837500 loss: 7.52500e-02|5.15717e-01 remaining: 9.74088e-01 (train|test)                                                                                                   \n",
    "[ 20/20,    125/125.0]  accuracy: 0.985000|0.887500 loss: 7.50653e-02|4.58454e-01 remaining: 9.74088e-01 (train|test)                                                                                                   \n",
    "Train steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:29<00:00,  1.40it/s]\n",
    "Train epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [30:06<00:00, 90.33s/it]]\n",
    "Test steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.41it/s]\n",
    "test_accuracy=0.8781\n",
    "Finished cell2d Test\n",
    "```\n",
    "- Train to 50%\n",
    "- Same accuracy\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.965000|0.852500 loss: 7.38883e-02|5.39016e-01 remaining: 5.01593e-01 (train|test)                                                                                                   \n",
    "[ 20/20,    125/125.0]  accuracy: 0.982500|0.887500 loss: 7.11580e-02|4.42782e-01 remaining: 5.01593e-01 (train|test)                                                                                                   \n",
    "Train steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:26<00:00,  1.45it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [30:06<00:00, 90.34s/it]\n",
    "Test steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.55it/s]\n",
    "test_accuracy=0.8775\n",
    "Finished cell2d Test\n",
    "root@0b8e21e8af84:/app# \n",
    "```\n",
    "- Failed to prune and train.  Debug\n",
    "```cmd\n",
    "Total Trainable Params: 466643\n",
    "Reduced parameters 466643/466962 = 0.999316860900887\n",
    "Train steps:   0%|                                                                                                                                                                            | 0/125.0 [00:02<?, ?it/s]\n",
    "Train epochs:   0%|                                                                                                                                                                              | 0/20 [00:02<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 1309, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 1145, in Test\n",
    "    outputs = model(inputs, isTraining=True)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 722, in forward\n",
    "    x = cell(x, isTraining=isTraining)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 426, in forward\n",
    "    y = x + residual\n",
    "RuntimeError: The size of tensor a (32) must match the size of tensor b (31) at non-singleton dimension 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf46c4f",
   "metadata": {},
   "source": [
    "10 February 2022\n",
    "- Corrected errors from \n",
    "- 50% training preserved test accuracy:\n",
    "``` cmd\n",
    "105/105,    120/125.0]  accuracy: 0.970000|0.857500 loss: 9.73530e-02|5.43195e-01 remaining: 5.04247e-01 (train|test)                                                                                                  \n",
    "[105/105,    125/125.0]  accuracy: 0.972500|0.870000 loss: 9.88603e-02|5.21643e-01 remaining: 5.04247e-01 (train|test)                                                                                                  \n",
    "Train steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:27<00:00,  1.43it/s]\n",
    "Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 105/105 [2:34:01<00:00, 88.02s/it] 125/125.0 [01:27<00:00,  1.31it/s]\n",
    "Test steps: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.45it/s]\n",
    "test_accuracy=0.8582\n",
    "```\n",
    "- Prune and test\n",
    "- Pruning was targeting 50% but pruned 88% \\\n",
    "![Before pruning](../img/crisp20220209_t50_01_cw.png) \\\n",
    "![50% pruned to 88%](../img/crisp20220209_t50_03_cw.png) \\\n",
    "- One error in counting the parameters is when the size output of one convolution is reduced, the input size of the next one is reduced.\n",
    "-   This occurs along the residual cells of resnet but only the change in output cells is counted\n",
    "- Continuous relaxation considers weightings and floating point values.  When they are pruned it becomes 0 for what was pruned but remains floating point for what remains for the target weight.\n",
    "- Reduced parameters is the actual change in parameters, not the weighted values\n",
    "- Are the scale, bias, and batch norm weights counted correctly in the target structure?  The are counted as a ratio of the ConvBN parameters.\n",
    "![Parameters Compare](../img/crisp20220209_t50_01_03.png) \\\n",
    "- Trained a base model crisp20220210_t100_00 to quickly train the smaller models:\n",
    "    ``` cmd\n",
    "    [ 90/90,    120/125.0]  accuracy: 0.872500|0.832500 loss: 3.47883e-01|5.04425e-01 remaining: 9.74499e-01 (train|test)                                                                                                                                                                                                               \n",
    "    [ 90/90,    125/125.0]  accuracy: 0.870000|0.837500 loss: 3.81746e-01|4.89213e-01 remaining: 9.74499e-01 (train|test)                                                                                                                                                                                                               \n",
    "    Train steps: 100%|██████████████████████████████████████████████████████████████████████████| 125/125.0 [01:53<00:00,  1.10it/s]\n",
    "    Train epochs: 100%|█████████████████████████████████████████████████████████████████████████| 90/90 [2:51:02<00:00, 114.03s/it]\n",
    "    Test steps: 100%|███████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.91it/s]\n",
    "    test_accuracy=0.8429\n",
    "    Finished cell2d Test\n",
    "    ```\n",
    "- Train 90% \n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.920000|0.830000 loss: 3.38574e-01|4.57155e-01 remaining: 9.08950e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.900000|0.852500 loss: 3.36099e-01|4.53501e-01 remaining: 9.08950e-01 (train|test)                                \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:55<00:00,  1.08it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [40:54<00:00, 122.71s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.46it/s]\n",
    "test_accuracy=0.8403\n",
    "Finished cell2d Test\n",
    "```\n",
    "- Train for 90%\n",
    "```python\n",
    "    parser.add_argument('-model_src', type=str,  default=\"crisp20220210_t100_00\")\n",
    "    parser.add_argument('-model_dest', type=str, default=\"crisp20220210_t90_00\")\n",
    "```\n",
    "- Nothing pruned\n",
    "```cmd\n",
    " 20/20,    120/125.0]  accuracy: 0.882500|0.855000 loss: 3.55629e-01|4.65631e-01 remaining: 9.08965e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.877500|0.845000 loss: 3.38583e-01|4.68672e-01 remaining: 9.08965e-01 (train|test)                                \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:53<00:00,  1.10it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [37:59<00:00, 113.95s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.48it/s]\n",
    "test_accuracy=0.8449\n",
    "```\n",
    "- crisp20220210_t100_00 to crisp20220210_t90_00\n",
    "- Prune and final training\n",
    "- Nothing pruned:\n",
    "```cmd\n",
    "Total Trainable Params: 466962\n",
    "Reduced parameters 466962/466962 = 1.0\n",
    "```\n",
    "- Train crisp20220210_t100_00 to 80%: crisp20220210_t80_00\n",
    "```\n",
    "[ 20/20,    115/125.0]  accuracy: 0.862500|0.857500 loss: 3.62796e-01|4.36736e-01 remaining: 8.01001e-01 (train|test)                                \n",
    "[ 20/20,    120/125.0]  accuracy: 0.907500|0.840000 loss: 3.59787e-01|4.78969e-01 remaining: 8.01001e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.855000|0.862500 loss: 3.51241e-01|4.47174e-01 remaining: 8.01001e-01 (train|test)                                \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:56<00:00,  1.07it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [39:41<00:00, 119.06s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.61it/s]\n",
    "test_accuracy=0.8445\n",
    "```\n",
    "![crisp20220210_t80_00](../img/crisp20220210_t80_00_cw.png)\n",
    "- It hit near 80:\n",
    "```cmd\n",
    "Total Trainable Params: 381702\n",
    "Reduced parameters 381702/466962 = 0.8174155498734372\n",
    "```\n",
    "- 80% Pruning reduced the initial convolutions and left the subsequent ones unchanged\n",
    "```\n",
    "[ 20/20,    120/125.0]  accuracy: 0.860000|0.845000 loss: 3.74923e-01|4.21833e-01 remaining: 7.78066e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.882500|0.862500 loss: 3.42709e-01|4.36758e-01 remaining: 7.78066e-01 (train|test)                                \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:48<00:00,  1.15it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [36:34<00:00, 109.72s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  8.27it/s]\n",
    "test_accuracy=0.8483\n",
    "```\n",
    "![crisp20220210_t80_01](../img/crisp20220210_t80_01_cw.png)\n",
    "- Train crisp20220210_t100_00 for 70% target structure crisp20220210_t70_00\n",
    "- Pruned the initial 32 channel layers\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.855000|0.817500 loss: 3.19838e-01|4.90642e-01 remaining: 7.02769e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.865000|0.845000 loss: 3.64346e-01|4.58762e-01 remaining: 7.02769e-01 (train|test)                                \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [02:03<00:00,  1.01it/s]\n",
    "Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [40:37<00:00, 121.87s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:03<00:00,  7.11it/s]\n",
    "test_accuracy=0.844\n",
    "```\n",
    "![crisp20220210_t70_00](../img/crisp20220210_t70_00_cw.png) \\\n",
    "- prune and retrain\n",
    "70% remaining created 27% remaining\n",
    "```cmd\n",
    "Total Trainable Params: 124534\n",
    "Reduced parameters 124534/466962 = 0.2666897948869501\n",
    "```\n",
    "- Drop in accuracy\n",
    "![crisp20220210_t70_01](../img/crisp20220210_t70_01_cw.png)\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.810000|0.797500 loss: 2.43650e+00|2.45131e+00 remaining: 2.62802e-01 (train|test)                                                                                    \n",
    "[ 20/20,    125/125.0]  accuracy: 0.815000|0.790000 loss: 2.41060e+00|2.49772e+00 remaining: 2.62802e-01 (train|test)                                                                                    \n",
    "Train steps: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125.0 [01:41<00:00,  1.23it/s]\n",
    "Train epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [32:31<00:00, 97.57s/it]\n",
    "Test steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25.0 [00:02<00:00,  8.65it/s]\n",
    "test_accuracy=0.8134\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02727f48",
   "metadata": {},
   "source": [
    "14 February 2022\n",
    "- [U-Net: Convolutional Networks for Biomedical\n",
    "Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2796b",
   "metadata": {},
   "source": [
    "15 February 2022\n",
    "- Use set_extra_state and get_extra_state to extract pruned state along with segment.load_state_dict and torch.save(model.state_dict(), out_buffer)\n",
    "- State dictionary loading and saving rather that full object pickling will allow me to load data into a new object\n",
    "- In order to successfully train UNET with coco, I needed to drop the Adam learning rate to 1e-4\n",
    "- In order to successfully train UNET with coco, I needed to adjust class weights to [0.05,0.5, 1.0, 1.0]\n",
    "- With these settings both crisp and standard UNET are converging slowly\n",
    "- After the cross entropy loss dropped to ~ 0.3, I increased class weights [1.0,1.0, 1.0, 1.0] and learning rate to 1.0e-3\n",
    "- Failed to continue to converge.  Restart with class_weight [0.5,1.0, 1.0, 1.0] and learning_rate=2.5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fab92",
   "metadata": {},
   "source": [
    "16 February, 2022\n",
    "- Overnight trained 3 epochs, cross entropy loss 0.3 - Training 0.3 test\n",
    "- Set class weights [1.0,1.0, 1.0, 1.0] and learning rate to 1.0e-4\n",
    "- ConvBR channel pruning using : self.channel_scale = nn.Parameter(torch.zeros(self.out_channels, dtype=torch.float))\n",
    "- ConvBR channel output:\n",
    "    ```python\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "        if self.search_structure: #scale channels based on self.channel_scale\n",
    "                weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)[None,:,None,None]\n",
    "                x *= weight_scale\n",
    "        ...\n",
    "    ```\n",
    "- ConvBR architecture weights is based just on channel_scale\n",
    "    ``` python\n",
    "    def ArchitectureWeights(self):\n",
    "        conv_weights = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        cell_weights = model_weights(self)\n",
    "        architecture_weights = (cell_weights/ self.out_channels) * conv_weights.sum_to_size((1))\n",
    "\n",
    "        return architecture_weights, cell_weights, conv_weights\n",
    "    ```\n",
    "- Test after 1 training epoch:\n",
    "    ```cmd\n",
    "    [  1/1,  29496/29571.75]  loss: 1.70899e-01|2.09416e-01 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    [  1/1,  29520/29571.75]  loss: 1.90740e-01|3.37091e-02 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    [  1/1,  29544/29571.75]  loss: 1.78525e-01|6.42415e-02 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    [  1/1,  29568/29571.75]  loss: 2.20651e-01|1.39349e-01 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    Train epochs:   0%|                                                                                                                                    | 0/1 [3:49:30<?, ?it/s]\n",
    "    /opt/conda/lib/python3.8/site-packages/tqdm/std.py:533: TqdmWarning: clamping frac to range [0, 1]█████████████████████████████████▉| 29571/29571.75 [3:49:32<00:00,  1.86it/s]\n",
    "    Train steps: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29572/29571.75 [3:49:32<00:00,  2.15it/s]\n",
    "    Train epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [3:49:33<00:00, 13773.56s/it]\n",
    "    Inference steps:   0%|▏                                                                                                                                                  | 2/1250 [00:01<16:26,  1.27it/s][W CPUAllocator.cpp:305] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
    "    Inference steps: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [16:35<00:00,  1.26it/s]\n",
    "    Test results http://198.211.145.1:30990/mllib/test/segmentation/test_results.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220216T183557Z&X-Amz-Expires=7200&X-Amz-SignedHeaders=host&X-Amz-Signature=f68c8b7c5c199f2a65c3ac4dcaa99157ec4b4463ef8916d8ce44723699522f49\n",
    "    Finished network2d Test\n",
    "    ```\n",
    "- Generally the highest gradient norm was near the UNET input and ouptut.  The center was high at the very beginning and now at the very end \\\n",
    "![segment_nas_512x442_20220215_04 gradient norm](../img/segment_nas_512x442_20220215_04_gn.png)\n",
    "- UNET seems to train slowly compared to FCN.\n",
    "- UNET seems more sensitive to training imbalance compared to FCN\n",
    "-  There continues to be improvement in cross entropy loss \n",
    "- Training and test remain close.  \n",
    "- Coco is a has ~ 118K/5K images and it has been trained about 4 epochs with data augmentation so I would expect good correspondence\\\n",
    "![segment_nas_512x442_20220215_04 cross entropy loss](../img/segment_nas_512x442_20220215_04_tb.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38185832",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
