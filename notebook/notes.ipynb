{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b409663-308c-43dc-8eb6-0cd92927c070",
   "metadata": {},
   "source": [
    "# Test Notes in Jupyter\n",
    "\n",
    "I am looking for a method to keep an electronic design notebook where I can keep a design log, live evqations, plots, and drawings.  Here I am trying to do this in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a352506-c467-4dc5-a886-b16d9562ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1300c909-1929-4de1-87b2-37d22803d2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\int e^{x} \\cos{\\left(x \\right)}\\, dx = \\frac{e^{x} \\sin{\\left(x \\right)}}{2} + \\frac{e^{x} \\cos{\\left(x \\right)}}{2}$"
      ],
      "text/plain": [
       "Eq(Integral(exp(x)*cos(x), x), exp(x)*sin(x)/2 + exp(x)*cos(x)/2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = symbols('x')\n",
    "a = Integral(cos(x)*exp(x), x)\n",
    "Eq(a, a.doit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06d3b938-66a1-43a2-8690-4032e78311c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.0, 2*np.pi, 0.01) \n",
    "y = np.sin(x) \n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1c670",
   "metadata": {},
   "source": [
    "4 January 2022\n",
    "- cell2d is no longer optimizing the network to minimize convolution norm\n",
    "- I suspect that summing convolution weights is losing the tensor gradient \n",
    "- See [Pytorch: backpropagating from sum of matrix elements to leaf variable answer](https://stackoverflow.com/questions/55942423/pytorch-backpropagating-from-sum-of-matrix-elements-to-leaf-variable)\n",
    "- Next: build up architecture loss concatenating tensors from lower networl level: c = torch.cat([a,b])\n",
    "- Next: solve architecture level using torch sum: d = torch.sum(c)\n",
    "- Next: verify gradient throughout operation\n",
    "- Next: test convolution minimization\n",
    "- Next: enable residual bypass of a specific level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695d9f0-6f06-4fa4-979a-55b2ddc389d7",
   "metadata": {},
   "source": [
    "6 January 2022\n",
    "- Pruning successful\n",
    "- Failied to run with pruned network nas_20220104_01:\\\n",
    "- Exception has occurred: RuntimeError       (note: full exception trace is shown but execution is paused at: <module>)\n",
    "Given groups=1, weight of size [982, 1023, 1, 1], expected input[256, 1024, 8, 8] to have 1023 channels, but got 1024 channels instead\n",
    "- The pruned convolution size is not fully propegated to the next convolution from Cell->ConvBR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d169c1a",
   "metadata": {},
   "source": [
    "7 January 2022\n",
    "- Test cross entropy loss is ~ 0.01\n",
    "- Architecture loss is ~ 0.01\n",
    "- The architecture reduction pushing from 0.1 to 0.01 requests a much smaller model but has a very small loss\n",
    "- Change from mean squared error to absolute error\n",
    "<br /> ![Tensorboard](../img/Tensorboard_nas_20220106_01.png) <br />\n",
    "- How to boost baseline accuracy to the state of the art? \n",
    "- [Cutmix](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.pdf)\n",
    "- [Attentive CutMix](https://arxiv.org/pdf/2003.13048.pdf)\n",
    "- [Label smoothing](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf)\n",
    "- [Mish activation function](https://arxiv.org/pdf/1908.08681v3.pdf)\n",
    "- Comminting out Cell::ApplyStructure enabled training following pruning: <br />\n",
    "            #if self.convolutions[-1]['out_channels'] == self.in1_channels+self.in2_channels:\n",
    "            #    self.conv_residual = None\n",
    "- On second ApplyStructure call, I get the following error: <br />\n",
    "  File \"networks/cell2d.py\", line 164, in ApplyStructure\n",
    "    raise ValueError(\"len(out_channel_mask)={} must be equal to self.out_channels={}\".format(len(out_channel_mask), self.out_channels))\n",
    "ValueError: len(out_channel_mask)=253 must be equal to self.out_channels=256\n",
    "- Find why self.out_channels is not updated in second training?\n",
    "- Found and fixed incorrect self.out_channels propagation on pruned cells\n",
    "- Training after prune resulted in all of the higher level convolutions being pruned out.\n",
    "- With higher levels pruned, the test accuracy peeked at about 50% rather than ~ 80%\n",
    "<br /> ![nas_20220106_03 training Tensorboard](../img/class_weights_nas_20220106_03.png) <br />\n",
    "<br /> ![nas_20220106_03 training Tensorboard](../img/TrainingAfterPrune_nas_20220106_03.png) <br />\n",
    "- Why did the test accuracy peek earlier in a pruned network while the training accuracy continued to climb with just the lower level convolutions?\n",
    "- What in the deeper network structure enables the training and test accuracies to track better?\n",
    "- Can I capture that good to keep the training and test accuracy together for longer?\n",
    "- Try reducing size by reducing convolutions but not removing layers.\n",
    "- Removing layer removal resulted in a significant decay of the middle layers and a similar test accuracy before pruning.  Test loss diverged from training loss after about iteration 4000\n",
    "<br /> ![nas_20220107_00 training NAS sweights](../img/nas_20220107_00_cw.png <br />\n",
    "<br /> ![nas_20220107_00 training Tensorboard](../img/nas_20220107_00_tb.png) <br />\n",
    "- Runtime error:\n",
    "```console\n",
    "Total Trainable Params: 22467463\n",
    "Reduced parameters 22467463/38108762 = 0.5895616079052896\n",
    "Train steps:   0%|                                                                                                                                                             | 0/250.0 [00:01<?, ?it/s]\n",
    "Train epochs:   0%|                                                                                                                                                               | 0/50 [00:01<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 987, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 886, in Test\n",
    "    outputs = classify(inputs)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 634, in forward\n",
    "    x = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 466, in forward\n",
    "    y = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
    "    return F.linear(input, self.weight, self.bias)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 1947, in linear\n",
    "    return torch._C._nn.linear(input, weight, bias)\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x1449 and 2048x10)\n",
    "PlotSearch finish\n",
    "```\n",
    "- Added dropout improved cross entropy loss compared with nas_20220107_00\n",
    "<br /> ![nas_20220107_dropout_00 training weights](../img/nas_20220107_dropout_00_cw.png) <br />\n",
    "<br /> ![nas_20220107_dropout_00 Tensorboard](../img/nas_20220107_dropout_00_tb.png) <br />\n",
    "- Try pruning and training.\n",
    "- Failed running following pruning\n",
    "```cmd\n",
    "Total Trainable Params: 27624300\n",
    "Reduced parameters 27624300/38108762 = 0.7248805405958871\n",
    "Train steps:   0%|                                                                                                                                                             | 0/250.0 [00:01<?, ?it/s]\n",
    "Train epochs:   0%|                                                                                                                                                               | 0/50 [00:01<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 993, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 892, in Test\n",
    "    outputs = classify(inputs, isTraining=True)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 639, in forward\n",
    "    x = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 471, in forward\n",
    "    y = self.fc(x)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
    "    return F.linear(input, self.weight, self.bias)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\", line 1947, in linear\n",
    "    return torch._C._nn.linear(input, weight, bias)\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x1712 and 2048x10)\n",
    "PlotSearch finish\n",
    "```\n",
    "- Pass the input size to FC::ApplyStructure from Classify::ApplyStructure\n",
    "- Initial training following pruning preserved accuracy:\n",
    "```cmd\n",
    "Total Trainable Params: 27620940\n",
    "Reduced parameters 27620940/38108762 = 0.724792371895996\n",
    "Train epochs:   0%|                                                                                                                                                                                        | 0/50 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.l1_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.945000 test accuracy=0.845000 training loss=1.87416e+00, test loss=2.21001e+00 arcitecture_reduction: 5.44080e-01                                                                          \n",
    "Test [1, 10.000000] training accuracy=0.905000 test accuracy=0.845000 training loss=1.83940e+00, test loss=2.13547e+00 arcitecture_reduction: 5.43983e-01 \n",
    "```\n",
    "- Will test cross entropy loss continue to improve?\n",
    "- Can this be repeated with future training and cropping?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034e5b3",
   "metadata": {},
   "source": [
    "8 January 2022\n",
    "- Training for ~ 200 epochs with droput did not result in a rise in training loss but it did result in a small increase in cross-entropy loss\n",
    "<br /> ![nas_20220107_dropout_00 training weights](../img/nas_20220107_dropout_03_tb.png) <br />\n",
    "- There was only a small decrease in network size.  Cross entropy loss is half the magnitude of architecture loss.\n",
    "<br /> ![nas_20220107_dropout_00 Tensorboard](../img/nas_20220107_dropout_03_cw.png) <br />\n",
    "- I will prune and restart training with 0.1 architecture loss vs the current 0.2.\n",
    "- Will this change the the network pruning?\n",
    "- Will this cause test accuracy to decrease?\n",
    "- Starting nas_20220108_00 with dropout rate of 0.1 and batch norm enabled\n",
    "- Initial network:\n",
    "```cmd\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|         cells.0.cnn.0.conv.weight         |    192     |\n",
    "|          cells.0.cnn.0.conv.bias          |     64     |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|         cells.0.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.0.cnn.1.conv.bias          |     64     |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|         cells.0.cnn.2.conv.weight         |   16384    |\n",
    "|          cells.0.cnn.2.conv.bias          |    256     |\n",
    "|      cells.0.cnn.2.batchnorm2d.weight     |    256     |\n",
    "|       cells.0.cnn.2.batchnorm2d.bias      |    256     |\n",
    "|     cells.0.conv_residual.conv.weight     |    768     |\n",
    "|      cells.0.conv_residual.conv.bias      |    256     |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|         cells.1.cnn.0.conv.weight         |   16384    |\n",
    "|          cells.1.cnn.0.conv.bias          |     64     |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.1.conv.bias          |     64     |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|         cells.1.cnn.2.conv.weight         |   16384    |\n",
    "|          cells.1.cnn.2.conv.bias          |    256     |\n",
    "|      cells.1.cnn.2.batchnorm2d.weight     |    256     |\n",
    "|       cells.1.cnn.2.batchnorm2d.bias      |    256     |\n",
    "|     cells.1.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.1.conv_residual.conv.bias      |    256     |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|         cells.2.cnn.0.conv.weight         |   16384    |\n",
    "|          cells.2.cnn.0.conv.bias          |     64     |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|         cells.2.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.2.cnn.1.conv.bias          |     64     |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|         cells.2.cnn.2.conv.weight         |   16384    |\n",
    "|          cells.2.cnn.2.conv.bias          |    256     |\n",
    "|      cells.2.cnn.2.batchnorm2d.weight     |    256     |\n",
    "|       cells.2.cnn.2.batchnorm2d.bias      |    256     |\n",
    "|     cells.2.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.2.conv_residual.conv.bias      |    256     |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|         cells.3.cnn.0.conv.weight         |   32768    |\n",
    "|          cells.3.cnn.0.conv.bias          |    128     |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.3.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.3.cnn.1.conv.bias          |    128     |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.3.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.3.cnn.2.conv.bias          |    512     |\n",
    "|      cells.3.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.3.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.3.conv_residual.conv.weight     |   131072   |\n",
    "|      cells.3.conv_residual.conv.bias      |    512     |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|         cells.4.cnn.0.conv.weight         |   65536    |\n",
    "|          cells.4.cnn.0.conv.bias          |    128     |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.4.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.4.cnn.1.conv.bias          |    128     |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.4.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.4.cnn.2.conv.bias          |    512     |\n",
    "|      cells.4.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.4.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.4.conv_residual.conv.weight     |   262144   |\n",
    "|      cells.4.conv_residual.conv.bias      |    512     |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|         cells.5.cnn.0.conv.weight         |   65536    |\n",
    "|          cells.5.cnn.0.conv.bias          |    128     |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.5.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.5.cnn.1.conv.bias          |    128     |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.5.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.5.cnn.2.conv.bias          |    512     |\n",
    "|      cells.5.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.5.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.5.conv_residual.conv.weight     |   262144   |\n",
    "|      cells.5.conv_residual.conv.bias      |    512     |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|         cells.6.cnn.0.conv.weight         |   65536    |\n",
    "|          cells.6.cnn.0.conv.bias          |    128     |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|         cells.6.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.6.cnn.1.conv.bias          |    128     |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|         cells.6.cnn.2.conv.weight         |   65536    |\n",
    "|          cells.6.cnn.2.conv.bias          |    512     |\n",
    "|      cells.6.cnn.2.batchnorm2d.weight     |    512     |\n",
    "|       cells.6.cnn.2.batchnorm2d.bias      |    512     |\n",
    "|     cells.6.conv_residual.conv.weight     |   262144   |\n",
    "|      cells.6.conv_residual.conv.bias      |    512     |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |    512     |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|         cells.7.cnn.0.conv.weight         |   131072   |\n",
    "|          cells.7.cnn.0.conv.bias          |    256     |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.7.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.7.cnn.1.conv.bias          |    256     |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.7.cnn.2.conv.weight         |   262144   |\n",
    "|          cells.7.cnn.2.conv.bias          |    1024    |\n",
    "|      cells.7.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|       cells.7.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.7.conv_residual.conv.weight     |   524288   |\n",
    "|      cells.7.conv_residual.conv.bias      |    1024    |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|         cells.8.cnn.0.conv.weight         |   262144   |\n",
    "|          cells.8.cnn.0.conv.bias          |    256     |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.8.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.8.cnn.1.conv.bias          |    256     |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.8.cnn.2.conv.weight         |   262144   |\n",
    "|          cells.8.cnn.2.conv.bias          |    1024    |\n",
    "|      cells.8.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|       cells.8.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.8.conv_residual.conv.weight     |  1048576   |\n",
    "|      cells.8.conv_residual.conv.bias      |    1024    |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|         cells.9.cnn.0.conv.weight         |   262144   |\n",
    "|          cells.9.cnn.0.conv.bias          |    256     |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.9.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.9.cnn.1.conv.bias          |    256     |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.9.cnn.2.conv.weight         |   262144   |\n",
    "|          cells.9.cnn.2.conv.bias          |    1024    |\n",
    "|      cells.9.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|       cells.9.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.9.conv_residual.conv.weight     |  1048576   |\n",
    "|      cells.9.conv_residual.conv.bias      |    1024    |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|         cells.10.cnn.0.conv.weight        |   262144   |\n",
    "|          cells.10.cnn.0.conv.bias         |    256     |\n",
    "|     cells.10.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.10.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.10.cnn.1.conv.bias         |    256     |\n",
    "|     cells.10.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.10.cnn.2.conv.weight        |   262144   |\n",
    "|          cells.10.cnn.2.conv.bias         |    1024    |\n",
    "|     cells.10.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|      cells.10.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.10.conv_residual.conv.weight    |  1048576   |\n",
    "|      cells.10.conv_residual.conv.bias     |    1024    |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|         cells.11.cnn.0.conv.weight        |   262144   |\n",
    "|          cells.11.cnn.0.conv.bias         |    256     |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.11.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.11.cnn.1.conv.bias         |    256     |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.11.cnn.2.conv.weight        |   262144   |\n",
    "|          cells.11.cnn.2.conv.bias         |    1024    |\n",
    "|     cells.11.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|      cells.11.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.11.conv_residual.conv.weight    |  1048576   |\n",
    "|      cells.11.conv_residual.conv.bias     |    1024    |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|         cells.12.cnn.0.conv.weight        |   262144   |\n",
    "|          cells.12.cnn.0.conv.bias         |    256     |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|         cells.12.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.12.cnn.1.conv.bias         |    256     |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|         cells.12.cnn.2.conv.weight        |   262144   |\n",
    "|          cells.12.cnn.2.conv.bias         |    1024    |\n",
    "|     cells.12.cnn.2.batchnorm2d.weight     |    1024    |\n",
    "|      cells.12.cnn.2.batchnorm2d.bias      |    1024    |\n",
    "|     cells.12.conv_residual.conv.weight    |  1048576   |\n",
    "|      cells.12.conv_residual.conv.bias     |    1024    |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |    1024    |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |    1024    |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|         cells.13.cnn.0.conv.weight        |   524288   |\n",
    "|          cells.13.cnn.0.conv.bias         |    512     |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|         cells.13.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.13.cnn.1.conv.bias         |    512     |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|         cells.13.cnn.2.conv.weight        |  1048576   |\n",
    "|          cells.13.cnn.2.conv.bias         |    2048    |\n",
    "|     cells.13.cnn.2.batchnorm2d.weight     |    2048    |\n",
    "|      cells.13.cnn.2.batchnorm2d.bias      |    2048    |\n",
    "|     cells.13.conv_residual.conv.weight    |  2097152   |\n",
    "|      cells.13.conv_residual.conv.bias     |    2048    |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |    2048    |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |    2048    |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|         cells.14.cnn.0.conv.weight        |  1048576   |\n",
    "|          cells.14.cnn.0.conv.bias         |    512     |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|         cells.14.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.14.cnn.1.conv.bias         |    512     |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|         cells.14.cnn.2.conv.weight        |  1048576   |\n",
    "|          cells.14.cnn.2.conv.bias         |    2048    |\n",
    "|     cells.14.cnn.2.batchnorm2d.weight     |    2048    |\n",
    "|      cells.14.cnn.2.batchnorm2d.bias      |    2048    |\n",
    "|     cells.14.conv_residual.conv.weight    |  4194304   |\n",
    "|      cells.14.conv_residual.conv.bias     |    2048    |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |    2048    |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |    2048    |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|         cells.15.cnn.0.conv.weight        |  1048576   |\n",
    "|          cells.15.cnn.0.conv.bias         |    512     |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|         cells.15.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.15.cnn.1.conv.bias         |    512     |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|         cells.15.cnn.2.conv.weight        |  1048576   |\n",
    "|          cells.15.cnn.2.conv.bias         |    2048    |\n",
    "|     cells.15.cnn.2.batchnorm2d.weight     |    2048    |\n",
    "|      cells.15.cnn.2.batchnorm2d.bias      |    2048    |\n",
    "|     cells.15.conv_residual.conv.weight    |  4194304   |\n",
    "|      cells.15.conv_residual.conv.bias     |    2048    |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |    2048    |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |    2048    |\n",
    "|                fc.fc.weight               |   20480    |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 38108762\n",
    "Test [1, 5.000000] training accuracy=0.095000 test accuracy=0.100000 training loss=7.45281e+00, test loss=4.40786e+00 arcitecture_reduction: 5.36742e-01 \n",
    "```\n",
    "- At 0.1 dropout probability, cross entropy loss rises after 4000k similar to without dropout\n",
    "- Arcitecture reduction may be more effective at 0.1 dropout probability that 0.2\n",
    "<br /> ![nas_20220107_dropout_00 Tensorboard](../img/nas_20220108_00_tb.png) <br />\n",
    "<br /> ![nas_20220107_dropout_00 cell weights](../img/nas_20220108_00_cw.png) <br />\n",
    "- Try again with 0.3 dropout probability \n",
    "- I think minimizing convolutions is more effective for 3x3 convolutions and less on 1x1 convolutions.  Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0e34e",
   "metadata": {},
   "source": [
    "10 January 2022\n",
    "- Add a scaler function to enable/disable each feature-space convolution so the size minimization can be performed by minimizing a single value\n",
    "- sigmoid(value)*convolution channel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3438b5b-2a15-4a92-b631-83af29e74147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x, k=5.0):\n",
    "    return 1.0/(1.0+np.exp(-k*x))\n",
    "\n",
    "x = np.arange(-1.0, 1.0, 0.1) \n",
    "y = sigmoid(x) \n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5856527-0a6a-40de-83e4-968062a87ba0",
   "metadata": {},
   "source": [
    "55- Added channel_scale so the convolution can be minimized by searching a single value rather the norm of the convolution\n",
    "- ConvBR::Forward, rather than scaling the convlution weights, I can scale the convolution output of all batches and convolutions by sigmoid(sigmoid_scale*cannel_scale)\n",
    "```python\n",
    "        x = self.conv(x)\n",
    "        if self.search_structure: #scale channels based on \n",
    "            weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)[None,:,None,None]\n",
    "            x *= weight_scale\n",
    "```\n",
    "- CpmvBR::ArchitectureWeights scales the architecture weight convolution norm by (sigmoid_scale*channel_scale):\n",
    "```python\n",
    "        weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        conv_weights = torch.tanh(self.weight_gain*weight_scale*torch.linalg.norm(self.conv.weight, dim=(1,2,3)))\n",
    "```\n",
    "- sigmoid scale = 5 transitions from ~0 at -1 to ~1 at 1.  Not necessary but it it makes me happy to have the weights normalized in that range\n",
    "- This resulted in a much more agressive pruning\n",
    "<br /> ![nas_20220110_00 cell weights](../img/nas_20220110_00_cw.png) <br />\n",
    "- Accuracy converged well with test tracking training effectively\n",
    "<br /> ![nas_20220110_00 Tensorboard](../img/nas_20220110_00_tb.png) <br />\n",
    "- Pruning was too effective but not reflected in search weight.  Add pruning information to ConvBR::ApplyStructure so the convoluation weight an pruning is clear that results in everything being pruned.\n",
    "- Why didn't the plot show everything was being pruned?  \n",
    "```cmd\n",
    "Total Trainable Params: 38146522\n",
    "ConvBR::ApplyStructure Cell 0 convolution 0/3 1.0=64/64 in_channels=3 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 0 convolution 1/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 0 convolution 2/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 0 cell residual 1.0=256/256 in_channels=3 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 convolution 0/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 convolution 1/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 convolution 2/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 1 cell residual 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 convolution 0/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 convolution 1/3 1.0=64/64 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 convolution 2/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 2 cell residual 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 3 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 4 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 5 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 convolution 0/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 convolution 1/3 1.0=128/128 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 convolution 2/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 6 cell residual 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 7 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 8 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 9 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 10 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 11 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 convolution 0/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 convolution 1/3 1.0=256/256 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 convolution 2/3 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 12 cell residual 1.0=1024/1024 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 convolution 0/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 convolution 1/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 convolution 2/3 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 13 cell residual 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 convolution 0/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 convolution 1/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 convolution 2/3 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 14 cell residual 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 convolution 0/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 convolution 1/3 1.0=512/512 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 convolution 2/3 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "ConvBR::ApplyStructure Cell 15 cell residual 1.0=2048/2048 in_channels=0 out_channels=0\n",
    "FC::ApplyStructure in 1.0=2048/2048 out 0.0=0/10 convolutions in_channels=0 out_channels=10\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|        cells.0.cnn.0.channel_scale        |     64     |\n",
    "|         cells.0.cnn.0.conv.weight         |     0      |\n",
    "|          cells.0.cnn.0.conv.bias          |     0      |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.0.cnn.1.channel_scale        |     64     |\n",
    "|         cells.0.cnn.1.conv.weight         |     0      |\n",
    "|          cells.0.cnn.1.conv.bias          |     0      |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.0.cnn.2.channel_scale        |    256     |\n",
    "|         cells.0.cnn.2.conv.weight         |     0      |\n",
    "|          cells.0.cnn.2.conv.bias          |     0      |\n",
    "|      cells.0.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.0.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.0.conv_residual.channel_scale    |    256     |\n",
    "|     cells.0.conv_residual.conv.weight     |     0      |\n",
    "|      cells.0.conv_residual.conv.bias      |     0      |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|        cells.1.cnn.0.channel_scale        |     64     |\n",
    "|         cells.1.cnn.0.conv.weight         |     0      |\n",
    "|          cells.1.cnn.0.conv.bias          |     0      |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.1.cnn.1.channel_scale        |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |     0      |\n",
    "|          cells.1.cnn.1.conv.bias          |     0      |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.1.cnn.2.channel_scale        |    256     |\n",
    "|         cells.1.cnn.2.conv.weight         |     0      |\n",
    "|          cells.1.cnn.2.conv.bias          |     0      |\n",
    "|      cells.1.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.1.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.1.conv_residual.channel_scale    |    256     |\n",
    "|     cells.1.conv_residual.conv.weight     |     0      |\n",
    "|      cells.1.conv_residual.conv.bias      |     0      |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|        cells.2.cnn.0.channel_scale        |     64     |\n",
    "|         cells.2.cnn.0.conv.weight         |     0      |\n",
    "|          cells.2.cnn.0.conv.bias          |     0      |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.2.cnn.1.channel_scale        |     64     |\n",
    "|         cells.2.cnn.1.conv.weight         |     0      |\n",
    "|          cells.2.cnn.1.conv.bias          |     0      |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.2.cnn.2.channel_scale        |    256     |\n",
    "|         cells.2.cnn.2.conv.weight         |     0      |\n",
    "|          cells.2.cnn.2.conv.bias          |     0      |\n",
    "|      cells.2.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.2.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.2.conv_residual.channel_scale    |    256     |\n",
    "|     cells.2.conv_residual.conv.weight     |     0      |\n",
    "|      cells.2.conv_residual.conv.bias      |     0      |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|        cells.3.cnn.0.channel_scale        |    128     |\n",
    "|         cells.3.cnn.0.conv.weight         |     0      |\n",
    "|          cells.3.cnn.0.conv.bias          |     0      |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.3.cnn.1.channel_scale        |    128     |\n",
    "|         cells.3.cnn.1.conv.weight         |     0      |\n",
    "|          cells.3.cnn.1.conv.bias          |     0      |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.3.cnn.2.channel_scale        |    512     |\n",
    "|         cells.3.cnn.2.conv.weight         |     0      |\n",
    "|          cells.3.cnn.2.conv.bias          |     0      |\n",
    "|      cells.3.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.3.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.3.conv_residual.channel_scale    |    512     |\n",
    "|     cells.3.conv_residual.conv.weight     |     0      |\n",
    "|      cells.3.conv_residual.conv.bias      |     0      |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|        cells.4.cnn.0.channel_scale        |    128     |\n",
    "|         cells.4.cnn.0.conv.weight         |     0      |\n",
    "|          cells.4.cnn.0.conv.bias          |     0      |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.4.cnn.1.channel_scale        |    128     |\n",
    "|         cells.4.cnn.1.conv.weight         |     0      |\n",
    "|          cells.4.cnn.1.conv.bias          |     0      |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.4.cnn.2.channel_scale        |    512     |\n",
    "|         cells.4.cnn.2.conv.weight         |     0      |\n",
    "|          cells.4.cnn.2.conv.bias          |     0      |\n",
    "|      cells.4.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.4.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.4.conv_residual.channel_scale    |    512     |\n",
    "|     cells.4.conv_residual.conv.weight     |     0      |\n",
    "|      cells.4.conv_residual.conv.bias      |     0      |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|        cells.5.cnn.0.channel_scale        |    128     |\n",
    "|         cells.5.cnn.0.conv.weight         |     0      |\n",
    "|          cells.5.cnn.0.conv.bias          |     0      |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.5.cnn.1.channel_scale        |    128     |\n",
    "|         cells.5.cnn.1.conv.weight         |     0      |\n",
    "|          cells.5.cnn.1.conv.bias          |     0      |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.5.cnn.2.channel_scale        |    512     |\n",
    "|         cells.5.cnn.2.conv.weight         |     0      |\n",
    "|          cells.5.cnn.2.conv.bias          |     0      |\n",
    "|      cells.5.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.5.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.5.conv_residual.channel_scale    |    512     |\n",
    "|     cells.5.conv_residual.conv.weight     |     0      |\n",
    "|      cells.5.conv_residual.conv.bias      |     0      |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|        cells.6.cnn.0.channel_scale        |    128     |\n",
    "|         cells.6.cnn.0.conv.weight         |     0      |\n",
    "|          cells.6.cnn.0.conv.bias          |     0      |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.6.cnn.1.channel_scale        |    128     |\n",
    "|         cells.6.cnn.1.conv.weight         |     0      |\n",
    "|          cells.6.cnn.1.conv.bias          |     0      |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.6.cnn.2.channel_scale        |    512     |\n",
    "|         cells.6.cnn.2.conv.weight         |     0      |\n",
    "|          cells.6.cnn.2.conv.bias          |     0      |\n",
    "|      cells.6.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.6.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.6.conv_residual.channel_scale    |    512     |\n",
    "|     cells.6.conv_residual.conv.weight     |     0      |\n",
    "|      cells.6.conv_residual.conv.bias      |     0      |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|        cells.7.cnn.0.channel_scale        |    256     |\n",
    "|         cells.7.cnn.0.conv.weight         |     0      |\n",
    "|          cells.7.cnn.0.conv.bias          |     0      |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.7.cnn.1.channel_scale        |    256     |\n",
    "|         cells.7.cnn.1.conv.weight         |     0      |\n",
    "|          cells.7.cnn.1.conv.bias          |     0      |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.7.cnn.2.channel_scale        |    1024    |\n",
    "|         cells.7.cnn.2.conv.weight         |     0      |\n",
    "|          cells.7.cnn.2.conv.bias          |     0      |\n",
    "|      cells.7.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.7.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.7.conv_residual.channel_scale    |    1024    |\n",
    "|     cells.7.conv_residual.conv.weight     |     0      |\n",
    "|      cells.7.conv_residual.conv.bias      |     0      |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|        cells.8.cnn.0.channel_scale        |    256     |\n",
    "|         cells.8.cnn.0.conv.weight         |     0      |\n",
    "|          cells.8.cnn.0.conv.bias          |     0      |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.8.cnn.1.channel_scale        |    256     |\n",
    "|         cells.8.cnn.1.conv.weight         |     0      |\n",
    "|          cells.8.cnn.1.conv.bias          |     0      |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.8.cnn.2.channel_scale        |    1024    |\n",
    "|         cells.8.cnn.2.conv.weight         |     0      |\n",
    "|          cells.8.cnn.2.conv.bias          |     0      |\n",
    "|      cells.8.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.8.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.8.conv_residual.channel_scale    |    1024    |\n",
    "|     cells.8.conv_residual.conv.weight     |     0      |\n",
    "|      cells.8.conv_residual.conv.bias      |     0      |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|        cells.9.cnn.0.channel_scale        |    256     |\n",
    "|         cells.9.cnn.0.conv.weight         |     0      |\n",
    "|          cells.9.cnn.0.conv.bias          |     0      |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.9.cnn.1.channel_scale        |    256     |\n",
    "|         cells.9.cnn.1.conv.weight         |     0      |\n",
    "|          cells.9.cnn.1.conv.bias          |     0      |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.9.cnn.2.channel_scale        |    1024    |\n",
    "|         cells.9.cnn.2.conv.weight         |     0      |\n",
    "|          cells.9.cnn.2.conv.bias          |     0      |\n",
    "|      cells.9.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|       cells.9.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.9.conv_residual.channel_scale    |    1024    |\n",
    "|     cells.9.conv_residual.conv.weight     |     0      |\n",
    "|      cells.9.conv_residual.conv.bias      |     0      |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |     0      |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|        cells.10.cnn.0.channel_scale       |    256     |\n",
    "|         cells.10.cnn.0.conv.weight        |     0      |\n",
    "|          cells.10.cnn.0.conv.bias         |     0      |\n",
    "|     cells.10.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.10.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.10.cnn.1.channel_scale       |    256     |\n",
    "|         cells.10.cnn.1.conv.weight        |     0      |\n",
    "|          cells.10.cnn.1.conv.bias         |     0      |\n",
    "|     cells.10.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.10.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.10.cnn.2.channel_scale       |    1024    |\n",
    "|         cells.10.cnn.2.conv.weight        |     0      |\n",
    "|          cells.10.cnn.2.conv.bias         |     0      |\n",
    "|     cells.10.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.10.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.10.conv_residual.channel_scale   |    1024    |\n",
    "|     cells.10.conv_residual.conv.weight    |     0      |\n",
    "|      cells.10.conv_residual.conv.bias     |     0      |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|        cells.11.cnn.0.channel_scale       |    256     |\n",
    "|         cells.11.cnn.0.conv.weight        |     0      |\n",
    "|          cells.11.cnn.0.conv.bias         |     0      |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.11.cnn.1.channel_scale       |    256     |\n",
    "|         cells.11.cnn.1.conv.weight        |     0      |\n",
    "|          cells.11.cnn.1.conv.bias         |     0      |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.11.cnn.2.channel_scale       |    1024    |\n",
    "|         cells.11.cnn.2.conv.weight        |     0      |\n",
    "|          cells.11.cnn.2.conv.bias         |     0      |\n",
    "|     cells.11.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.11.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.11.conv_residual.channel_scale   |    1024    |\n",
    "|     cells.11.conv_residual.conv.weight    |     0      |\n",
    "|      cells.11.conv_residual.conv.bias     |     0      |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|        cells.12.cnn.0.channel_scale       |    256     |\n",
    "|         cells.12.cnn.0.conv.weight        |     0      |\n",
    "|          cells.12.cnn.0.conv.bias         |     0      |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.12.cnn.1.channel_scale       |    256     |\n",
    "|         cells.12.cnn.1.conv.weight        |     0      |\n",
    "|          cells.12.cnn.1.conv.bias         |     0      |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.12.cnn.2.channel_scale       |    1024    |\n",
    "|         cells.12.cnn.2.conv.weight        |     0      |\n",
    "|          cells.12.cnn.2.conv.bias         |     0      |\n",
    "|     cells.12.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.12.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.12.conv_residual.channel_scale   |    1024    |\n",
    "|     cells.12.conv_residual.conv.weight    |     0      |\n",
    "|      cells.12.conv_residual.conv.bias     |     0      |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|        cells.13.cnn.0.channel_scale       |    512     |\n",
    "|         cells.13.cnn.0.conv.weight        |     0      |\n",
    "|          cells.13.cnn.0.conv.bias         |     0      |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.13.cnn.1.channel_scale       |    512     |\n",
    "|         cells.13.cnn.1.conv.weight        |     0      |\n",
    "|          cells.13.cnn.1.conv.bias         |     0      |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.13.cnn.2.channel_scale       |    2048    |\n",
    "|         cells.13.cnn.2.conv.weight        |     0      |\n",
    "|          cells.13.cnn.2.conv.bias         |     0      |\n",
    "|     cells.13.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.13.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.13.conv_residual.channel_scale   |    2048    |\n",
    "|     cells.13.conv_residual.conv.weight    |     0      |\n",
    "|      cells.13.conv_residual.conv.bias     |     0      |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|        cells.14.cnn.0.channel_scale       |    512     |\n",
    "|         cells.14.cnn.0.conv.weight        |     0      |\n",
    "|          cells.14.cnn.0.conv.bias         |     0      |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.14.cnn.1.channel_scale       |    512     |\n",
    "|         cells.14.cnn.1.conv.weight        |     0      |\n",
    "|          cells.14.cnn.1.conv.bias         |     0      |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.14.cnn.2.channel_scale       |    2048    |\n",
    "|         cells.14.cnn.2.conv.weight        |     0      |\n",
    "|          cells.14.cnn.2.conv.bias         |     0      |\n",
    "|     cells.14.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.14.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.14.conv_residual.channel_scale   |    2048    |\n",
    "|     cells.14.conv_residual.conv.weight    |     0      |\n",
    "|      cells.14.conv_residual.conv.bias     |     0      |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|        cells.15.cnn.0.channel_scale       |    512     |\n",
    "|         cells.15.cnn.0.conv.weight        |     0      |\n",
    "|          cells.15.cnn.0.conv.bias         |     0      |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |     0      |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |     0      |\n",
    "|        cells.15.cnn.1.channel_scale       |    512     |\n",
    "|         cells.15.cnn.1.conv.weight        |     0      |\n",
    "|          cells.15.cnn.1.conv.bias         |     0      |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |     0      |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |     0      |\n",
    "|        cells.15.cnn.2.channel_scale       |    2048    |\n",
    "|         cells.15.cnn.2.conv.weight        |     0      |\n",
    "|          cells.15.cnn.2.conv.bias         |     0      |\n",
    "|     cells.15.cnn.2.batchnorm2d.weight     |     0      |\n",
    "|      cells.15.cnn.2.batchnorm2d.bias      |     0      |\n",
    "|    cells.15.conv_residual.channel_scale   |    2048    |\n",
    "|     cells.15.conv_residual.conv.weight    |     0      |\n",
    "|      cells.15.conv_residual.conv.bias     |     0      |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |     0      |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |     0      |\n",
    "|                fc.fc.weight               |     0      |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 37786\n",
    "Reduced parameters 37786/38146522 = 0.0009905490204323215\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 1049, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 929, in Test\n",
    "    plotsearch = PlotSearch(classify)\n",
    "  File \"networks/cell2d.py\", line 785, in __init__\n",
    "    architecture_weights, total_trainable_weights, cell_weights = network.ArchitectureWeights()\n",
    "  File \"networks/cell2d.py\", line 703, in ArchitectureWeights\n",
    "    cell_archatecture_weights, cell_total_trainable_weights, cell_weight = in_cell.ArchitectureWeights()\n",
    "  File \"networks/cell2d.py\", line 463, in ArchitectureWeights\n",
    "    layer_weight, _, conv_weights  = l.ArchitectureWeights()\n",
    "  File \"networks/cell2d.py\", line 151, in ArchitectureWeights\n",
    "    conv_weights = torch.tanh(self.weight_gain*weight_scale*torch.linalg.norm(self.conv.weight, dim=(1,2,3)))\n",
    "RuntimeError: The size of tensor a (64) must match the size of tensor b (0) at non-singleton dimension 0\n",
    "PlotSearch finish\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef0930-7cf9-4028-8619-c08bcf7ffaac",
   "metadata": {},
   "source": [
    "12 January 2022\n",
    "- Error in normalized weights is because L2 norm is not normzlized based on the number of elments in norm.  The norm of large convolutions resulted in a larger norm than smaller convolutions.\n",
    "- L2 norm devided by the square root of the number of elements results in consistent bahavior across different tensor sizes:\n",
    "``` python\n",
    "def ArchitectureWeights(self):\n",
    "    weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "    norm = torch.linalg.norm(self.conv.weight, dim=(1,2,3))/np.sqrt(np.product(self.conv.weight.shape[1:]))\n",
    "    conv_weights = torch.tanh(self.weight_gain*weight_scale*norm)\n",
    "```\n",
    "- Next, include a by-stable function in the objective for channel enable/disable so the optomizer will be rewarded for either turning the channel on or off and avoid a middle value\n",
    "- How to handel zeroing out all convolutions?\n",
    "- Disabled batch norm to prevent compensating for a zeroed out channel by brining it back with batch norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90363603-3329-4cf4-99f1-0f2facc999ce",
   "metadata": {},
   "source": [
    "13 January 2022\n",
    "- Training was resulting in zero-sized convolutions\n",
    "- Cause - initialized channel pruning to 0.5.  \n",
    "- Architecture wights was product of convolution norm (~ 0.6) * channel scale (~0.5) = ~0.3\n",
    "- Cutoff was 0.5 = results in pruning all of the convolutions\n",
    "- \n",
    "``` python\n",
    "class ConvBR(nn.Module):\n",
    "    def __init__(self, \n",
    "       self.channel_scale = nn.Parameter(torch.zeros(self.out_channels, dtype=torch.float))\n",
    "    def ArchitectureWeights(self):\n",
    "        weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        conv_weights = torch.tanh(self.weight_gain*weight_scale*torch.linalg.norm(self.conv.weight, dim=(1,2,3)))\n",
    "\n",
    "```\n",
    "- Fix: change con_weights average of channel_scale and convolution norm\n",
    "```python\n",
    "    def ArchitectureWeights(self):\n",
    "        weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        norm = torch.linalg.norm(self.conv.weight, dim=(1,2,3))/np.sqrt(np.product(self.conv.weight.shape[1:]))\n",
    "        conv_weights = (torch.tanh(self.weight_gain*norm)+weight_scale)/2.0\n",
    "\n",
    "```\n",
    "- I then needed to reduce the -k_structure default argument (trying 1e-3 now) to accuracy is searched preferentially.\n",
    "- Add stabilization normalized gaussian function to push ConvBR::channel_scale to 0 or 1.  Add to loss so channels preferentially are on or off\n",
    "- record class weights as video\n",
    "- Add class weighs to tensorboard\n",
    "- What d cleass weights do when search_structure us false?\n",
    "- Add fully connected layer to model erosion\n",
    "- How to esnsure the eroded model is feasible?\n",
    "- Eroded weights should not be contributing significantly to model accuracy.  \n",
    "- Is relaxation of the convolution outputs a poor model of the removed weight?  \n",
    "- Do max pooling and batch normalizaton compensate too effectively for the weight change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c87bb-6fd8-47b4-bb9b-e2a9cdcf98ba",
   "metadata": {},
   "source": [
    "14 January 2022\n",
    "- The trained networks were very stubborn about minimizing network size\n",
    "- After initial training without or with very little pruning, I then tried to increase the pruning with very little affect\n",
    "- Added a gradient plot following the structure of the class_weights plot\n",
    "- Found that the architecture gradent was in the trange of 1e-6 and the classification gradient was in the range of 1e-2\n",
    "- By increasing k_structure, I was able to proportionally incrase the class weight norms to a similar value to classificaiton norms\n",
    "- At this point, classificaiton loss on the test set minimized at a bout 1000 batches and then incrased.  \n",
    "- Set structural loss to target a specific architecture level and  trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b6b85e-be14-44a2-bc1a-4be625ec380a",
   "metadata": {},
   "source": [
    "15 January 2022\n",
    "- Added gradient norm visualization\n",
    "- Needed to increase k_structure to 1.0e2 or 1.0e3 to get the structure minimization.  1.0e3 convers structure well\n",
    "- Need to handle 0 size convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01269e6c",
   "metadata": {},
   "source": [
    "17 January 2022\n",
    "- Added image augmentation, removed dropout\n",
    "- CIFAR-10 Resnet 18, 75 epochs, -target_structure=1.0e0, final test accuracy ~0.8583, nas_20220117_rn18_00\n",
    "- CIFAR-10 Resnet 18, 20 epochs, -target_structure=1.0e0, -learning_rate=0.001, final test accuracy ~0.8743, nas_20220117_rn18_01\n",
    "- CIFSR-10 -target_structure=1.0e-1, 10 epochs, -learning_rate=0.01, -target_structure=1.0e-1, final test accuracy ~.8496, nas_20220117_rn18_02\n",
    "- CIFAR-10 Resnet 18, 20 epochs, Prune, Reduced parameters 5145795/11498898 = 0.4475033172744032, -learning_rate=0.001,\n",
    "learning_rate=0.01, -target_structure=1.0e-1, final test accuracy ~.8496, nas_20220117_rn18_02\n",
    "- nas_20220117_rn18_04 zero's out several channels.  Use this to figure out what I should do when this happens.  I would like to burn out the convolution and burn open the residual but that would mean growing the residual channel.  Maybe freezing the residual would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e7cc36",
   "metadata": {},
   "source": [
    "18 January 2022\n",
    "- nas_20220118_rn101_00, -resnet_len=101, -learning_rate=0.01, -batch_size=200, -epochs=50, -model_class=CIFAR10, -model_src=None, -k_structure=1.0e2, -target_structure=1.0e0\n",
    "    ``` cmd\n",
    "    Test [50, 245.000000] training accuracy=0.900000 test accuracy=0.870000 training loss=5.92753e-01, test loss=6.63403e-01 arcitecture_reduction: 9.48538e-01                        \n",
    "    Test [50, 250.000000] training accuracy=0.880000 test accuracy=0.885000 training loss=5.86504e-01, test loss=6.20232e-01 arcitecture_reduction: 9.48538e-01                        \n",
    "    Train steps:  250/250.0 [04:04<00:00,  1.02it/s]\n",
    "    Train epochs: 100% 50/50 [3:25:56<00:00, 247.13s/it]\n",
    "    ```\n",
    "-   nas_20220118_rn101_01  -learning_rate=1e-3, -epochs=25, -model_src=nas_20220118_rn101_00, test accuracy ~.8835\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf10fbc5",
   "metadata": {},
   "source": [
    "19 January 2022\n",
    "- Resnet 101 test accuracy capped at ~ 87%\n",
    "- Going back and preparing data at Resnet 50\n",
    "- py networks/cell2d.py -target_structure=1.0 -epochs=50 -prune=False -learning_rate=1e-2 -model_dest=\"nas_20220119_rn50_00\" - Stopped when test accuracy was ~ 85\n",
    "- py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=False -learning_rate=1e-3 -model_src=\"nas_20220119_rn50_00\" -model_dest=\"nas_20220119_rn50_01\"\n",
    "- After data, simplify residual path.\n",
    "- Nvidia pruning software: [Transfer learning toolkit](https://developer.nvidia.com/blog/transfer-learning-toolkit-pruning-intelligent-video-analytics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e9daa",
   "metadata": {
    "tags": []
   },
   "source": [
    "20 January 2022\n",
    "- Initial training of resnet 50 model\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=False -learning_rate=1e-3 -model_src=\"nas_20220119_rn50_00\" -model_dest=\"nas_20220119_rn50_01\"\n",
    "```\n",
    "- Set target structure and train for 25 epochs\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=25 -prune=False -learning_rate=1e-3 -model_src=\"nas_20220119_rn50_01\" -model_dest=\"nas_20220119_rn50_ts5_02\"\n",
    "```\n",
    "- Learning rate was too low.  Prune and train again\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=25 -prune=True -learning_rate=1e-2 -model_src=\"nas_20220119_rn50_ts5_02\" -model_dest=\"nas_20220119_rn50_ts5_03\"\n",
    "...\n",
    "Train steps: 100%|| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|| 25/25 [43:57<00:00, 105.52s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  3.96it/s]\n",
    "test_accuracy=0.8608\n",
    "```\n",
    "- Final fine tuning at a lower learning rate\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=True -learning_rate=1e-3 -model_src=nas_20220119_rn50_ts5_03 -model_dest=nas_20220119_rn50_ts5_03\n",
    "...\n",
    "Total Trainable Params: 21300619\n",
    "Reduced parameters 21300619/22183066 = 0.960219791078474\n",
    "...\n",
    "Train steps: 100%|| 125/125.0 [01:31<00:00,  1.37it/s]\n",
    "Train epochs: 100%|| 25/25 [38:25<00:00, 92.23s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:05<00:00,  4.35it/s]\n",
    "test_accuracy=0.869\n",
    "```\n",
    "- The fainal pruned network is:\n",
    "![Network chanel gradient norms](../img/nas_20220119_rn50_ts5_03_gn.png)\n",
    "- Retrain from the pretraining with a reducted target structure:\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.25 -epochs=25 -prune=False -learning_rate=1e-2 -model_src=nas_20220119_rn50_01 -model_dest=nas_20220119_rn50_ts25_02\n",
    "...\n",
    "Train steps: 100%|| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|| 25/25 [44:53<00:00, 107.75s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  4.16it/s]\n",
    "test_accuracy=0.8641\n",
    "``` \n",
    "- Prune and final training:\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=1.0 -epochs=25 -prune=True -learning_rate=1e-2 -model_src=nas_20220119_rn50_ts25_02 -model_dest=nas_20220119_rn50_ts25_03\n",
    "...\n",
    "Total Trainable Params: 21557456\n",
    "Reduced parameters 21557456/22183066 = 0.9717978569779309\n",
    "Train epochs:   0%|                                                                                                                                 | 0/25 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.747500 test accuracy=0.742500 training loss=1.06741e+02, test loss=6.18376e+01 arcitecture_reduction: 7.52974e-01                   \n",
    "Test [1, 10.000000] training accuracy=0.755000 test accuracy=0.777500 training loss=4.46887e+01, test loss=2.69561e+01 arcitecture_reduction: 8.37857e-01                  \n",
    "Test [1, 15.000000] training accuracy=0.845000 test accuracy=0.792500 training loss=2.14885e+01, test loss=1.61794e+01 arcitecture_reduction: 8.75440e-01                  \n",
    "Test [1, 20.000000] training accuracy=0.847500 test accuracy=0.807500 training loss=1.42387e+01, test loss=1.23288e+01 arcitecture_reduction: 8.91240e-01                  \n",
    "\n",
    "...\n",
    "Train steps: 100%|| 125/125.0 [01:32<00:00,  1.35it/s]\n",
    "Train epochs: 100%|| 25/25 [39:47<00:00, 95.48s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:05<00:00,  4.36it/s]\n",
    "test_accuracy=0.8716\n",
    "```\n",
    "- Train from scratch to a target structure\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.25 -epochs=50 -prune=False -learning_rate=1e-2 -model_dest=nas_20220119_none_rn50_ts25_00\n",
    "```\n",
    "- Pruning too aggresively & reporting an incorrect size.  Switch conv_weights back to product of norm and weight scale rather than average.  Both should be values from 0 to 1.  \n",
    "``` python\n",
    "def ArchitectureWeights(self):\n",
    "        #conv_weights = (torch.tanh(self.weight_gain*norm)+weight_scale)/2.0\n",
    "        conv_weights = torch.tanh(self.weight_gain*norm)*weight_scale\n",
    "```\n",
    "- Optimizing to 0 size network at a small architecture weight\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.0 -epochs=50 -prune=False -model_dest=nas_20220119_rn50_ts0_0- -search_structure=True -k_structure=1.0e1\n",
    "...\n",
    "Test [50, 115.000000] training accuracy=0.877500 test accuracy=0.847500 training loss=4.16946e-01, test loss=4.57426e-01 arcitecture_reduction: 3.66704e-02                                                                  \n",
    "Test [50, 120.000000] training accuracy=0.837500 test accuracy=0.830000 training loss=4.59586e-01, test loss=4.99954e-01 arcitecture_reduction: 3.67033e-02                                                                  \n",
    "Test [50, 125.000000] training accuracy=0.867500 test accuracy=0.797500 training loss=4.28933e-01, test loss=5.90774e-01 arcitecture_reduction: 3.66914e-02                                                                  \n",
    "Train steps: 100%|| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|| 50/50 [1:28:45<00:00, 106.51s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  4.08it/s]\n",
    "test_accuracy=0.8242\n",
    "```\n",
    "- The tensorboard shows that the training and test are still well aligned.  Because the network minimization will be so drastic, I want to continue training at a smaller learning rate before pruning \\\n",
    "![Training tensorboard](../img/nas_20220119_rn50_ts0_0-tb.png) \\\n",
    "![Training final class weights](../img/nas_20220119_rn50_ts0_0-cw.png)\n",
    "\n",
    "- Continued training\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.0 -epochs=25 -model_src=nas_20220119_rn50_ts0_0- -model_dest=nas_20220119_rn50_ts0_1- -search_structure=True -learning_rate=1e-3\n",
    "```\n",
    "- Stop after 10 epochs because test magnitude increase has stalled. \n",
    "``` cmd\n",
    "Train epochs:  40%|                                                                                                     | 10/25 [18:02<26:42, 106.86s/it^Train steps:  15%|                                                                                                                                              | 19/125.0 [00:16<01:30,  1.17it/s]\n",
    "Train epochs:  40%|                                                                                                     | 10/25 [18:06<27:09, 108.65s/it]\n",
    "```\n",
    "- There has been a slight increase in \"architecture_loss\" which is why I performed the finer training - Add the weights that are needed for higher accuracy.  \\\n",
    "![Class Weights](../img/nas_20220119_rn50_ts0_1-cw.png)\n",
    "-  Try it a second time with learning reat 1e-4:\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.0 -epochs=10 -model_src=nas_20220119_rn50_ts0_1- -model_dest=nas_20220119_rn50_ts0_2- -search_structure=True -learning_rate=1e-4\n",
    "...\n",
    "Total Trainable Params: 22183066\n",
    "...\n",
    "Test [10, 125.000000] training accuracy=0.890000 test accuracy=0.865000 training loss=2.65042e-01, test loss=4.38974e-01 arcitecture_reduction: 3.75011e-02                                                                  \n",
    "Train steps: 100%|| 125/125.0 [01:46<00:00,  1.18it/s]\n",
    "Train epochs: 100%|| 10/10 [17:46<00:00, 106.66s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  3.89it/s]\n",
    "test_accuracy=0.8573\n",
    "\n",
    "```\n",
    "- Lots of time, not much improvement.  Lets prune and see what happens\n",
    "``` cmd\n",
    "py networks/cell2d.py -prune=True -epochs=25 -model_src=nas_20220119_rn50_ts0_2- -model_dest=nas_20220119_rn50_ts0_3- -search_structure=False -learning_rate=1e-2\n",
    "...\n",
    "Total Trainable Params: 1712585\n",
    "Reduced parameters 1712585/22183066 = 0.0772023578706388\n",
    "...\n",
    "Test [1, 5.000000] training accuracy=0.402500 test accuracy=0.465000 training loss=1.18123e+01, test loss=1.10280e+01 arcitecture_reduction: 2.43444e-02                                                                     \n",
    "Test [1, 10.000000] training accuracy=0.475000 test accuracy=0.515000 training loss=1.09074e+01, test loss=1.08152e+01 arcitecture_reduction: 2.43531e-02                                                                    \n",
    "Test [1, 15.000000] training accuracy=0.577500 test accuracy=0.585000 training loss=1.07588e+01, test loss=1.06765e+01 arcitecture_reduction: 2.43858e-02                                                                    \n",
    "Test [1, 20.000000] training accuracy=0.555000 test accuracy=0.607500 training loss=1.06855e+01, test loss=1.05820e+01 arcitecture_reduction: 2.44622e-02                                                                    \n",
    "Test [1, 25.000000] training accuracy=0.637500 test accuracy=0.667500 training loss=1.05531e+01, test loss=1.04909e+01 arcitecture_reduction: 2.45635e-02                                                                    \n",
    "Test [1, 30.000000] training accuracy=0.672500 test accuracy=0.670000 training loss=1.05048e+01, test loss=1.05151e+01 arcitecture_reduction: 2.46570e-02                                                                    \n",
    "Test [1, 35.000000] training accuracy=0.712500 test accuracy=0.605000 training loss=1.04531e+01, test loss=1.06228e+01 arcitecture_reduction: 2.47624e-02                                                                    \n",
    "Test [1, 40.000000] training accuracy=0.690000 test accuracy=0.720000 training loss=1.04491e+01, test loss=1.03660e+01 arcitecture_reduction: 2.48824e-02                                                                    \n",
    "Test [1, 45.000000] training accuracy=0.715000 test accuracy=0.705000 training loss=1.03513e+01, test loss=1.03664e+01 arcitecture_reduction: 2.50210e-02                                                                    \n",
    "Test [1, 50.000000] training accuracy=0.700000 test accuracy=0.677500 training loss=1.03175e+01, test loss=1.03993e+01 arcitecture_reduction: 2.51657e-02                                                                    \n",
    "Test [1, 55.000000] training accuracy=0.727500 test accuracy=0.740000 training loss=1.02908e+01, test loss=1.03112e+01 arcitecture_reduction: 2.53102e-02                                                                    \n",
    "Test [1, 60.000000] training accuracy=0.770000 test accuracy=0.750000 training loss=1.02227e+01, test loss=1.02677e+01 arcitecture_reduction: 2.54572e-02\n",
    "...\n",
    "Test [25, 120.000000] training accuracy=0.890000 test accuracy=0.837500 training loss=8.92025e+00, test loss=9.08880e+00 arcitecture_reduction: 7.18184e-02                                                                  \n",
    "Test [25, 125.000000] training accuracy=0.862500 test accuracy=0.835000 training loss=8.97310e+00, test loss=9.11956e+00 arcitecture_reduction: 7.18398e-02                                                                  \n",
    "Train steps: 100%|| 125/125.0 [01:11<00:00,  1.75it/s]\n",
    "Train epochs: 100%|| 25/25 [29:44<00:00, 71.37s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:04<00:00,  5.47it/s]\n",
    "test_accuracy=0.8394\n",
    "```\n",
    "\n",
    "- Looks like we maintained reasonable accuracy are may climb to a similar accuracy\n",
    "- Even in the test, we got an inference speedup.  \n",
    "- Train a little at 1e-3 learning rate to see if that gives us the final little boost to the same as before\n",
    "- Increae the batch size to 800 since we now have a much smaller network \\\n",
    "![Tensorboard](../img/nas_20220119_rn50_ts0_3-tb.png) \\\n",
    "![Gradient norm](../img/nas_20220119_rn50_ts0_3-gn.png)\n",
    "\n",
    "``` cmd\n",
    "py networks/cell2d.py -prune=False -epochs=10 -model_src=nas_20220119_rn50_ts0_3- -model_dest=nas_20220119_rn50_ts0_4- -search_structure=False -learning_rate=1e-3 -batch_size=800\n",
    "...\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|        cells.0.cnn.0.channel_scale        |     64     |\n",
    "|         cells.0.cnn.0.conv.weight         |    1728    |\n",
    "|          cells.0.cnn.0.conv.bias          |     64     |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.0.cnn.1.channel_scale        |     64     |\n",
    "|         cells.0.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.0.cnn.1.conv.bias          |     64     |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.0.conv_residual.channel_scale    |     64     |\n",
    "|     cells.0.conv_residual.conv.weight     |    192     |\n",
    "|      cells.0.conv_residual.conv.bias      |     64     |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|        cells.1.cnn.0.channel_scale        |     64     |\n",
    "|         cells.1.cnn.0.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.0.conv.bias          |     64     |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.1.cnn.1.channel_scale        |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.1.conv.bias          |     64     |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.1.conv_residual.channel_scale    |     64     |\n",
    "|     cells.1.conv_residual.conv.weight     |    4096    |\n",
    "|      cells.1.conv_residual.conv.bias      |     64     |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|        cells.2.cnn.0.channel_scale        |     64     |\n",
    "|         cells.2.cnn.0.conv.weight         |   36864    |\n",
    "|          cells.2.cnn.0.conv.bias          |     64     |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.2.cnn.1.channel_scale        |     64     |\n",
    "|         cells.2.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.2.cnn.1.conv.bias          |     64     |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.2.conv_residual.channel_scale    |     64     |\n",
    "|     cells.2.conv_residual.conv.weight     |    4096    |\n",
    "|      cells.2.conv_residual.conv.bias      |     64     |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|        cells.3.cnn.0.channel_scale        |    128     |\n",
    "|         cells.3.cnn.0.conv.weight         |   73728    |\n",
    "|          cells.3.cnn.0.conv.bias          |    128     |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.3.cnn.1.channel_scale        |    128     |\n",
    "|         cells.3.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.3.cnn.1.conv.bias          |    128     |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.3.conv_residual.channel_scale    |    128     |\n",
    "|     cells.3.conv_residual.conv.weight     |    8192    |\n",
    "|      cells.3.conv_residual.conv.bias      |    128     |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|        cells.4.cnn.0.channel_scale        |    128     |\n",
    "|         cells.4.cnn.0.conv.weight         |   147456   |\n",
    "|          cells.4.cnn.0.conv.bias          |    128     |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.4.cnn.1.channel_scale        |    128     |\n",
    "|         cells.4.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.4.cnn.1.conv.bias          |    128     |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.4.conv_residual.channel_scale    |    128     |\n",
    "|     cells.4.conv_residual.conv.weight     |   16384    |\n",
    "|      cells.4.conv_residual.conv.bias      |    128     |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|        cells.5.cnn.0.channel_scale        |    128     |\n",
    "|         cells.5.cnn.0.conv.weight         |   147456   |\n",
    "|          cells.5.cnn.0.conv.bias          |    128     |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.5.cnn.1.channel_scale        |    128     |\n",
    "|         cells.5.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.5.cnn.1.conv.bias          |    128     |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.5.conv_residual.channel_scale    |    128     |\n",
    "|     cells.5.conv_residual.conv.weight     |   16384    |\n",
    "|      cells.5.conv_residual.conv.bias      |    128     |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|        cells.6.cnn.0.channel_scale        |    128     |\n",
    "|         cells.6.cnn.0.conv.weight         |   147456   |\n",
    "|          cells.6.cnn.0.conv.bias          |    128     |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |    128     |\n",
    "|        cells.6.cnn.1.channel_scale        |    128     |\n",
    "|         cells.6.cnn.1.conv.weight         |   147456   |\n",
    "|          cells.6.cnn.1.conv.bias          |    128     |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |    128     |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |    128     |\n",
    "|    cells.6.conv_residual.channel_scale    |    128     |\n",
    "|     cells.6.conv_residual.conv.weight     |   16384    |\n",
    "|      cells.6.conv_residual.conv.bias      |    128     |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |    128     |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |    128     |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|        cells.7.cnn.0.channel_scale        |    256     |\n",
    "|         cells.7.cnn.0.conv.weight         |   294912   |\n",
    "|          cells.7.cnn.0.conv.bias          |    256     |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.7.cnn.1.channel_scale        |    256     |\n",
    "|         cells.7.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.7.cnn.1.conv.bias          |    256     |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.7.conv_residual.channel_scale    |    256     |\n",
    "|     cells.7.conv_residual.conv.weight     |   32768    |\n",
    "|      cells.7.conv_residual.conv.bias      |    256     |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|        cells.8.cnn.0.channel_scale        |    256     |\n",
    "|         cells.8.cnn.0.conv.weight         |   589824   |\n",
    "|          cells.8.cnn.0.conv.bias          |    256     |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.8.cnn.1.channel_scale        |    256     |\n",
    "|         cells.8.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.8.cnn.1.conv.bias          |    256     |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.8.conv_residual.channel_scale    |    256     |\n",
    "|     cells.8.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.8.conv_residual.conv.bias      |    256     |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|        cells.9.cnn.0.channel_scale        |    256     |\n",
    "|         cells.9.cnn.0.conv.weight         |   589824   |\n",
    "|          cells.9.cnn.0.conv.bias          |    256     |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.9.cnn.1.channel_scale        |    256     |\n",
    "|         cells.9.cnn.1.conv.weight         |   589824   |\n",
    "|          cells.9.cnn.1.conv.bias          |    256     |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.9.conv_residual.channel_scale    |    256     |\n",
    "|     cells.9.conv_residual.conv.weight     |   65536    |\n",
    "|      cells.9.conv_residual.conv.bias      |    256     |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |    256     |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|        cells.10.cnn.0.channel_scale       |    256     |\n",
    "|         cells.10.cnn.0.conv.weight        |   589824   |\n",
    "|          cells.10.cnn.0.conv.bias         |    256     |\n",
    "|     cells.10.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.10.cnn.1.channel_scale       |    256     |\n",
    "|         cells.10.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.10.cnn.1.conv.bias         |    256     |\n",
    "|     cells.10.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.10.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.10.conv_residual.channel_scale   |    256     |\n",
    "|     cells.10.conv_residual.conv.weight    |   65536    |\n",
    "|      cells.10.conv_residual.conv.bias     |    256     |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|        cells.11.cnn.0.channel_scale       |    256     |\n",
    "|         cells.11.cnn.0.conv.weight        |   589824   |\n",
    "|          cells.11.cnn.0.conv.bias         |    256     |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.11.cnn.1.channel_scale       |    256     |\n",
    "|         cells.11.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.11.cnn.1.conv.bias         |    256     |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.11.conv_residual.channel_scale   |    256     |\n",
    "|     cells.11.conv_residual.conv.weight    |   65536    |\n",
    "|      cells.11.conv_residual.conv.bias     |    256     |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|        cells.12.cnn.0.channel_scale       |    256     |\n",
    "|         cells.12.cnn.0.conv.weight        |   589824   |\n",
    "|          cells.12.cnn.0.conv.bias         |    256     |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |    256     |\n",
    "|        cells.12.cnn.1.channel_scale       |    256     |\n",
    "|         cells.12.cnn.1.conv.weight        |   589824   |\n",
    "|          cells.12.cnn.1.conv.bias         |    256     |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |    256     |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |    256     |\n",
    "|    cells.12.conv_residual.channel_scale   |    256     |\n",
    "|     cells.12.conv_residual.conv.weight    |   65536    |\n",
    "|      cells.12.conv_residual.conv.bias     |    256     |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|        cells.13.cnn.0.channel_scale       |    512     |\n",
    "|         cells.13.cnn.0.conv.weight        |  1179648   |\n",
    "|          cells.13.cnn.0.conv.bias         |    512     |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|        cells.13.cnn.1.channel_scale       |    512     |\n",
    "|         cells.13.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.13.cnn.1.conv.bias         |    512     |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|    cells.13.conv_residual.channel_scale   |    512     |\n",
    "|     cells.13.conv_residual.conv.weight    |   131072   |\n",
    "|      cells.13.conv_residual.conv.bias     |    512     |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |    512     |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|        cells.14.cnn.0.channel_scale       |    512     |\n",
    "|         cells.14.cnn.0.conv.weight        |  2359296   |\n",
    "|          cells.14.cnn.0.conv.bias         |    512     |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|        cells.14.cnn.1.channel_scale       |    512     |\n",
    "|         cells.14.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.14.cnn.1.conv.bias         |    512     |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|    cells.14.conv_residual.channel_scale   |    512     |\n",
    "|     cells.14.conv_residual.conv.weight    |   262144   |\n",
    "|      cells.14.conv_residual.conv.bias     |    512     |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |    512     |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|        cells.15.cnn.0.channel_scale       |    512     |\n",
    "|         cells.15.cnn.0.conv.weight        |  2359296   |\n",
    "|          cells.15.cnn.0.conv.bias         |    512     |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |    512     |\n",
    "|        cells.15.cnn.1.channel_scale       |    512     |\n",
    "|         cells.15.cnn.1.conv.weight        |  2359296   |\n",
    "|          cells.15.cnn.1.conv.bias         |    512     |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |    512     |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |    512     |\n",
    "|    cells.15.conv_residual.channel_scale   |    512     |\n",
    "|     cells.15.conv_residual.conv.weight    |   262144   |\n",
    "|      cells.15.conv_residual.conv.bias     |    512     |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |    512     |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |    512     |\n",
    "|                fc.fc.weight               |    5120    |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 22183066\n",
    "\n",
    "Pruned to:\n",
    "+-------------------------------------------+------------+\n",
    "|                  Modules                  | Parameters |\n",
    "+-------------------------------------------+------------+\n",
    "|          cells.0.cell_convolution         |     1      |\n",
    "|        cells.0.cnn.0.channel_scale        |     62     |\n",
    "|         cells.0.cnn.0.conv.weight         |    1674    |\n",
    "|          cells.0.cnn.0.conv.bias          |     62     |\n",
    "|      cells.0.cnn.0.batchnorm2d.weight     |     62     |\n",
    "|       cells.0.cnn.0.batchnorm2d.bias      |     62     |\n",
    "|        cells.0.cnn.1.channel_scale        |     63     |\n",
    "|         cells.0.cnn.1.conv.weight         |   35154    |\n",
    "|          cells.0.cnn.1.conv.bias          |     63     |\n",
    "|      cells.0.cnn.1.batchnorm2d.weight     |     63     |\n",
    "|       cells.0.cnn.1.batchnorm2d.bias      |     63     |\n",
    "|    cells.0.conv_residual.channel_scale    |     63     |\n",
    "|     cells.0.conv_residual.conv.weight     |    189     |\n",
    "|      cells.0.conv_residual.conv.bias      |     63     |\n",
    "|  cells.0.conv_residual.batchnorm2d.weight |     63     |\n",
    "|   cells.0.conv_residual.batchnorm2d.bias  |     63     |\n",
    "|          cells.1.cell_convolution         |     1      |\n",
    "|        cells.1.cnn.0.channel_scale        |     64     |\n",
    "|         cells.1.cnn.0.conv.weight         |   36288    |\n",
    "|          cells.1.cnn.0.conv.bias          |     64     |\n",
    "|      cells.1.cnn.0.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.0.batchnorm2d.bias      |     64     |\n",
    "|        cells.1.cnn.1.channel_scale        |     64     |\n",
    "|         cells.1.cnn.1.conv.weight         |   36864    |\n",
    "|          cells.1.cnn.1.conv.bias          |     64     |\n",
    "|      cells.1.cnn.1.batchnorm2d.weight     |     64     |\n",
    "|       cells.1.cnn.1.batchnorm2d.bias      |     64     |\n",
    "|    cells.1.conv_residual.channel_scale    |     64     |\n",
    "|     cells.1.conv_residual.conv.weight     |    4032    |\n",
    "|      cells.1.conv_residual.conv.bias      |     64     |\n",
    "|  cells.1.conv_residual.batchnorm2d.weight |     64     |\n",
    "|   cells.1.conv_residual.batchnorm2d.bias  |     64     |\n",
    "|          cells.2.cell_convolution         |     1      |\n",
    "|        cells.2.cnn.0.channel_scale        |     62     |\n",
    "|         cells.2.cnn.0.conv.weight         |   35712    |\n",
    "|          cells.2.cnn.0.conv.bias          |     62     |\n",
    "|      cells.2.cnn.0.batchnorm2d.weight     |     62     |\n",
    "|       cells.2.cnn.0.batchnorm2d.bias      |     62     |\n",
    "|        cells.2.cnn.1.channel_scale        |     63     |\n",
    "|         cells.2.cnn.1.conv.weight         |   35154    |\n",
    "|          cells.2.cnn.1.conv.bias          |     63     |\n",
    "|      cells.2.cnn.1.batchnorm2d.weight     |     63     |\n",
    "|       cells.2.cnn.1.batchnorm2d.bias      |     63     |\n",
    "|    cells.2.conv_residual.channel_scale    |     63     |\n",
    "|     cells.2.conv_residual.conv.weight     |    4032    |\n",
    "|      cells.2.conv_residual.conv.bias      |     63     |\n",
    "|  cells.2.conv_residual.batchnorm2d.weight |     63     |\n",
    "|   cells.2.conv_residual.batchnorm2d.bias  |     63     |\n",
    "|          cells.3.cell_convolution         |     1      |\n",
    "|        cells.3.cnn.0.channel_scale        |    105     |\n",
    "|         cells.3.cnn.0.conv.weight         |   59535    |\n",
    "|          cells.3.cnn.0.conv.bias          |    105     |\n",
    "|      cells.3.cnn.0.batchnorm2d.weight     |    105     |\n",
    "|       cells.3.cnn.0.batchnorm2d.bias      |    105     |\n",
    "|        cells.3.cnn.1.channel_scale        |    100     |\n",
    "|         cells.3.cnn.1.conv.weight         |   94500    |\n",
    "|          cells.3.cnn.1.conv.bias          |    100     |\n",
    "|      cells.3.cnn.1.batchnorm2d.weight     |    100     |\n",
    "|       cells.3.cnn.1.batchnorm2d.bias      |    100     |\n",
    "|    cells.3.conv_residual.channel_scale    |    100     |\n",
    "|     cells.3.conv_residual.conv.weight     |    6300    |\n",
    "|      cells.3.conv_residual.conv.bias      |    100     |\n",
    "|  cells.3.conv_residual.batchnorm2d.weight |    100     |\n",
    "|   cells.3.conv_residual.batchnorm2d.bias  |    100     |\n",
    "|          cells.4.cell_convolution         |     1      |\n",
    "|        cells.4.cnn.0.channel_scale        |     70     |\n",
    "|         cells.4.cnn.0.conv.weight         |   63000    |\n",
    "|          cells.4.cnn.0.conv.bias          |     70     |\n",
    "|      cells.4.cnn.0.batchnorm2d.weight     |     70     |\n",
    "|       cells.4.cnn.0.batchnorm2d.bias      |     70     |\n",
    "|        cells.4.cnn.1.channel_scale        |     83     |\n",
    "|         cells.4.cnn.1.conv.weight         |   52290    |\n",
    "|          cells.4.cnn.1.conv.bias          |     83     |\n",
    "|      cells.4.cnn.1.batchnorm2d.weight     |     83     |\n",
    "|       cells.4.cnn.1.batchnorm2d.bias      |     83     |\n",
    "|    cells.4.conv_residual.channel_scale    |     83     |\n",
    "|     cells.4.conv_residual.conv.weight     |    8300    |\n",
    "|      cells.4.conv_residual.conv.bias      |     83     |\n",
    "|  cells.4.conv_residual.batchnorm2d.weight |     83     |\n",
    "|   cells.4.conv_residual.batchnorm2d.bias  |     83     |\n",
    "|          cells.5.cell_convolution         |     1      |\n",
    "|        cells.5.cnn.0.channel_scale        |     52     |\n",
    "|         cells.5.cnn.0.conv.weight         |   38844    |\n",
    "|          cells.5.cnn.0.conv.bias          |     52     |\n",
    "|      cells.5.cnn.0.batchnorm2d.weight     |     52     |\n",
    "|       cells.5.cnn.0.batchnorm2d.bias      |     52     |\n",
    "|        cells.5.cnn.1.channel_scale        |     54     |\n",
    "|         cells.5.cnn.1.conv.weight         |   25272    |\n",
    "|          cells.5.cnn.1.conv.bias          |     54     |\n",
    "|      cells.5.cnn.1.batchnorm2d.weight     |     54     |\n",
    "|       cells.5.cnn.1.batchnorm2d.bias      |     54     |\n",
    "|    cells.5.conv_residual.channel_scale    |     54     |\n",
    "|     cells.5.conv_residual.conv.weight     |    4482    |\n",
    "|      cells.5.conv_residual.conv.bias      |     54     |\n",
    "|  cells.5.conv_residual.batchnorm2d.weight |     54     |\n",
    "|   cells.5.conv_residual.batchnorm2d.bias  |     54     |\n",
    "|          cells.6.cell_convolution         |     1      |\n",
    "|        cells.6.cnn.0.channel_scale        |     12     |\n",
    "|         cells.6.cnn.0.conv.weight         |    5832    |\n",
    "|          cells.6.cnn.0.conv.bias          |     12     |\n",
    "|      cells.6.cnn.0.batchnorm2d.weight     |     12     |\n",
    "|       cells.6.cnn.0.batchnorm2d.bias      |     12     |\n",
    "|        cells.6.cnn.1.channel_scale        |     18     |\n",
    "|         cells.6.cnn.1.conv.weight         |    1944    |\n",
    "|          cells.6.cnn.1.conv.bias          |     18     |\n",
    "|      cells.6.cnn.1.batchnorm2d.weight     |     18     |\n",
    "|       cells.6.cnn.1.batchnorm2d.bias      |     18     |\n",
    "|    cells.6.conv_residual.channel_scale    |     18     |\n",
    "|     cells.6.conv_residual.conv.weight     |    972     |\n",
    "|      cells.6.conv_residual.conv.bias      |     18     |\n",
    "|  cells.6.conv_residual.batchnorm2d.weight |     18     |\n",
    "|   cells.6.conv_residual.batchnorm2d.bias  |     18     |\n",
    "|          cells.7.cell_convolution         |     1      |\n",
    "|        cells.7.cnn.0.channel_scale        |     16     |\n",
    "|         cells.7.cnn.0.conv.weight         |    2592    |\n",
    "|          cells.7.cnn.0.conv.bias          |     16     |\n",
    "|      cells.7.cnn.0.batchnorm2d.weight     |     16     |\n",
    "|       cells.7.cnn.0.batchnorm2d.bias      |     16     |\n",
    "|        cells.7.cnn.1.channel_scale        |     26     |\n",
    "|         cells.7.cnn.1.conv.weight         |    3744    |\n",
    "|          cells.7.cnn.1.conv.bias          |     26     |\n",
    "|      cells.7.cnn.1.batchnorm2d.weight     |     26     |\n",
    "|       cells.7.cnn.1.batchnorm2d.bias      |     26     |\n",
    "|    cells.7.conv_residual.channel_scale    |     26     |\n",
    "|     cells.7.conv_residual.conv.weight     |    468     |\n",
    "|      cells.7.conv_residual.conv.bias      |     26     |\n",
    "|  cells.7.conv_residual.batchnorm2d.weight |     26     |\n",
    "|   cells.7.conv_residual.batchnorm2d.bias  |     26     |\n",
    "|          cells.8.cell_convolution         |     1      |\n",
    "|        cells.8.cnn.0.channel_scale        |     97     |\n",
    "|         cells.8.cnn.0.conv.weight         |   22698    |\n",
    "|          cells.8.cnn.0.conv.bias          |     97     |\n",
    "|      cells.8.cnn.0.batchnorm2d.weight     |     97     |\n",
    "|       cells.8.cnn.0.batchnorm2d.bias      |     97     |\n",
    "|        cells.8.cnn.1.channel_scale        |     47     |\n",
    "|         cells.8.cnn.1.conv.weight         |   41031    |\n",
    "|          cells.8.cnn.1.conv.bias          |     47     |\n",
    "|      cells.8.cnn.1.batchnorm2d.weight     |     47     |\n",
    "|       cells.8.cnn.1.batchnorm2d.bias      |     47     |\n",
    "|    cells.8.conv_residual.channel_scale    |     47     |\n",
    "|     cells.8.conv_residual.conv.weight     |    1222    |\n",
    "|      cells.8.conv_residual.conv.bias      |     47     |\n",
    "|  cells.8.conv_residual.batchnorm2d.weight |     47     |\n",
    "|   cells.8.conv_residual.batchnorm2d.bias  |     47     |\n",
    "|          cells.9.cell_convolution         |     1      |\n",
    "|        cells.9.cnn.0.channel_scale        |     10     |\n",
    "|         cells.9.cnn.0.conv.weight         |    4230    |\n",
    "|          cells.9.cnn.0.conv.bias          |     10     |\n",
    "|      cells.9.cnn.0.batchnorm2d.weight     |     10     |\n",
    "|       cells.9.cnn.0.batchnorm2d.bias      |     10     |\n",
    "|        cells.9.cnn.1.channel_scale        |     17     |\n",
    "|         cells.9.cnn.1.conv.weight         |    1530    |\n",
    "|          cells.9.cnn.1.conv.bias          |     17     |\n",
    "|      cells.9.cnn.1.batchnorm2d.weight     |     17     |\n",
    "|       cells.9.cnn.1.batchnorm2d.bias      |     17     |\n",
    "|    cells.9.conv_residual.channel_scale    |     17     |\n",
    "|     cells.9.conv_residual.conv.weight     |    799     |\n",
    "|      cells.9.conv_residual.conv.bias      |     17     |\n",
    "|  cells.9.conv_residual.batchnorm2d.weight |     17     |\n",
    "|   cells.9.conv_residual.batchnorm2d.bias  |     17     |\n",
    "|         cells.10.cell_convolution         |     1      |\n",
    "|    cells.10.conv_residual.channel_scale   |    256     |\n",
    "|     cells.10.conv_residual.conv.weight    |    4352    |\n",
    "|      cells.10.conv_residual.conv.bias     |    256     |\n",
    "| cells.10.conv_residual.batchnorm2d.weight |    256     |\n",
    "|  cells.10.conv_residual.batchnorm2d.bias  |    256     |\n",
    "|         cells.11.cell_convolution         |     1      |\n",
    "|        cells.11.cnn.0.channel_scale       |     8      |\n",
    "|         cells.11.cnn.0.conv.weight        |   18432    |\n",
    "|          cells.11.cnn.0.conv.bias         |     8      |\n",
    "|     cells.11.cnn.0.batchnorm2d.weight     |     8      |\n",
    "|      cells.11.cnn.0.batchnorm2d.bias      |     8      |\n",
    "|        cells.11.cnn.1.channel_scale       |     6      |\n",
    "|         cells.11.cnn.1.conv.weight        |    432     |\n",
    "|          cells.11.cnn.1.conv.bias         |     6      |\n",
    "|     cells.11.cnn.1.batchnorm2d.weight     |     6      |\n",
    "|      cells.11.cnn.1.batchnorm2d.bias      |     6      |\n",
    "|    cells.11.conv_residual.channel_scale   |     6      |\n",
    "|     cells.11.conv_residual.conv.weight    |    1536    |\n",
    "|      cells.11.conv_residual.conv.bias     |     6      |\n",
    "| cells.11.conv_residual.batchnorm2d.weight |     6      |\n",
    "|  cells.11.conv_residual.batchnorm2d.bias  |     6      |\n",
    "|         cells.12.cell_convolution         |     1      |\n",
    "|        cells.12.cnn.0.channel_scale       |     42     |\n",
    "|         cells.12.cnn.0.conv.weight        |    2268    |\n",
    "|          cells.12.cnn.0.conv.bias         |     42     |\n",
    "|     cells.12.cnn.0.batchnorm2d.weight     |     42     |\n",
    "|      cells.12.cnn.0.batchnorm2d.bias      |     42     |\n",
    "|        cells.12.cnn.1.channel_scale       |     23     |\n",
    "|         cells.12.cnn.1.conv.weight        |    8694    |\n",
    "|          cells.12.cnn.1.conv.bias         |     23     |\n",
    "|     cells.12.cnn.1.batchnorm2d.weight     |     23     |\n",
    "|      cells.12.cnn.1.batchnorm2d.bias      |     23     |\n",
    "|    cells.12.conv_residual.channel_scale   |     23     |\n",
    "|     cells.12.conv_residual.conv.weight    |    138     |\n",
    "|      cells.12.conv_residual.conv.bias     |     23     |\n",
    "| cells.12.conv_residual.batchnorm2d.weight |     23     |\n",
    "|  cells.12.conv_residual.batchnorm2d.bias  |     23     |\n",
    "|         cells.13.cell_convolution         |     1      |\n",
    "|        cells.13.cnn.0.channel_scale       |    299     |\n",
    "|         cells.13.cnn.0.conv.weight        |   61893    |\n",
    "|          cells.13.cnn.0.conv.bias         |    299     |\n",
    "|     cells.13.cnn.0.batchnorm2d.weight     |    299     |\n",
    "|      cells.13.cnn.0.batchnorm2d.bias      |    299     |\n",
    "|        cells.13.cnn.1.channel_scale       |     78     |\n",
    "|         cells.13.cnn.1.conv.weight        |   209898   |\n",
    "|          cells.13.cnn.1.conv.bias         |     78     |\n",
    "|     cells.13.cnn.1.batchnorm2d.weight     |     78     |\n",
    "|      cells.13.cnn.1.batchnorm2d.bias      |     78     |\n",
    "|    cells.13.conv_residual.channel_scale   |     78     |\n",
    "|     cells.13.conv_residual.conv.weight    |    1794    |\n",
    "|      cells.13.conv_residual.conv.bias     |     78     |\n",
    "| cells.13.conv_residual.batchnorm2d.weight |     78     |\n",
    "|  cells.13.conv_residual.batchnorm2d.bias  |     78     |\n",
    "|         cells.14.cell_convolution         |     1      |\n",
    "|        cells.14.cnn.0.channel_scale       |    320     |\n",
    "|         cells.14.cnn.0.conv.weight        |   224640   |\n",
    "|          cells.14.cnn.0.conv.bias         |    320     |\n",
    "|     cells.14.cnn.0.batchnorm2d.weight     |    320     |\n",
    "|      cells.14.cnn.0.batchnorm2d.bias      |    320     |\n",
    "|        cells.14.cnn.1.channel_scale       |     34     |\n",
    "|         cells.14.cnn.1.conv.weight        |   97920    |\n",
    "|          cells.14.cnn.1.conv.bias         |     34     |\n",
    "|     cells.14.cnn.1.batchnorm2d.weight     |     34     |\n",
    "|      cells.14.cnn.1.batchnorm2d.bias      |     34     |\n",
    "|    cells.14.conv_residual.channel_scale   |     34     |\n",
    "|     cells.14.conv_residual.conv.weight    |    2652    |\n",
    "|      cells.14.conv_residual.conv.bias     |     34     |\n",
    "| cells.14.conv_residual.batchnorm2d.weight |     34     |\n",
    "|  cells.14.conv_residual.batchnorm2d.bias  |     34     |\n",
    "|         cells.15.cell_convolution         |     1      |\n",
    "|        cells.15.cnn.0.channel_scale       |    286     |\n",
    "|         cells.15.cnn.0.conv.weight        |   87516    |\n",
    "|          cells.15.cnn.0.conv.bias         |    286     |\n",
    "|     cells.15.cnn.0.batchnorm2d.weight     |    286     |\n",
    "|      cells.15.cnn.0.batchnorm2d.bias      |    286     |\n",
    "|        cells.15.cnn.1.channel_scale       |    133     |\n",
    "|         cells.15.cnn.1.conv.weight        |   342342   |\n",
    "|          cells.15.cnn.1.conv.bias         |    133     |\n",
    "|     cells.15.cnn.1.batchnorm2d.weight     |    133     |\n",
    "|      cells.15.cnn.1.batchnorm2d.bias      |    133     |\n",
    "|    cells.15.conv_residual.channel_scale   |    133     |\n",
    "|     cells.15.conv_residual.conv.weight    |    4522    |\n",
    "|      cells.15.conv_residual.conv.bias     |    133     |\n",
    "| cells.15.conv_residual.batchnorm2d.weight |    133     |\n",
    "|  cells.15.conv_residual.batchnorm2d.bias  |    133     |\n",
    "|                fc.fc.weight               |    1330    |\n",
    "|                 fc.fc.bias                |     10     |\n",
    "+-------------------------------------------+------------+\n",
    "Total Trainable Params: 1712585\n",
    "Reduced parameters 1712585/22183066 = 0.0772023578706388\n",
    "Train epochs:   0%|                                                                                                                                                                                   | 0/10 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 6.000000] training accuracy=0.896250 test accuracy=0.852500 training loss=8.91533e+00, test loss=9.03551e+00 arcitecture_reduction: 7.18547e-02                                                                     \n",
    "Test [1, 12.000000] training accuracy=0.916250 test accuracy=0.848750 training loss=8.88420e+00, test loss=9.04996e+00 arcitecture_reduction: 7.18693e-02 \n",
    "...\n",
    "Test [10, 54.000000] training accuracy=0.930000 test accuracy=0.862500 training loss=8.80388e+00, test loss=9.03733e+00 arcitecture_reduction: 7.27267e-02                                                                   \n",
    "Test [10, 60.000000] training accuracy=0.926250 test accuracy=0.851250 training loss=8.80574e+00, test loss=9.05619e+00 arcitecture_reduction: 7.27321e-02                                                                   \n",
    "Train steps: 63it [00:54,  1.15it/s]                 | 9/10 [09:12<00:55, 55.28s/it]\n",
    "Train epochs: 100%|| 10/10 [09:14<00:00, 55.48s/it]\n",
    "Test steps: 13it [00:04,  2.82it/s]                                                                                                                                                                                          \n",
    "test_accuracy=0.9022916666666667\n",
    "```\n",
    "- Test accuracy of 90% is unexpected!  I thought I needed to switch to multi-stage training with sgd and pretraining before I would pass into the 90% test accuracy on CIFAR-10, not prune a network to nearly nothing.\n",
    "- Training gradients are distributed throughout the pruned network with a fairly uniform magnitude\n",
    "- Test accuracy and cross entropy loss began to degrade after iteration 400\n",
    "![Final pruned network gradient norm](../img/nas_20220119_rn50_ts0_4-gn.png)\n",
    "![Final training Tensorboard](../img/nas_20220119_rn50_ts0_4-tb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b30790-ccb5-4eb5-9449-9ce3e1023517",
   "metadata": {},
   "source": [
    "21 January 2022\n",
    "- \n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=75 -prune=False -search_structure=True -learning_rate=1e-2 -model_dest=crisp20220121_t50_00\n",
    "...\n",
    "Test [75, 120.000000] training accuracy=0.900000 test accuracy=0.845000 training loss=2.32706e-01, test loss=4.64866e-01 arcitecture_reduction: 5.00009e-01                                            \n",
    "Test [75, 125.000000] training accuracy=0.925000 test accuracy=0.832500 training loss=2.29566e-01, test loss=5.13253e-01 arcitecture_reduction: 4.99894e-01                                            \n",
    "Train steps: 100%|| 125/125.0 [01:45<00:00,  1.19it/s]\n",
    "Train epochs: 100%|| 75/75 [2:12:41<00:00, 106.15s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  4.02it/s]\n",
    "test_accuracy=0.8537\n",
    "\n",
    "```\n",
    "- Training to 0.5 class weights\n",
    "\n",
    "![](../img/crisp20220121_t50_00_cw.png)\n",
    "![](../img/crisp20220121_t50_00_tb.png)\n",
    "\n",
    "- Prune \n",
    "\n",
    "```\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=25 -prune=True -search_structure=False -learning_rate=1e-2 -model_src=crisp20220121_t50_00 -model_dest=crisp20220121_t50_01\n",
    "...\n",
    "Total Trainable Params: 13376476\n",
    "Reduced parameters 13376476/22183066 = 0.6030039310165691\n",
    "Train epochs:   0%|                                                                                                                                                             | 0/25 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.887500 test accuracy=0.837500 training loss=4.37770e-01, test loss=5.90980e-01 arcitecture_reduction: 4.01812e-01                                               \n",
    "Test [1, 10.000000] training accuracy=0.917500 test accuracy=0.862500 training loss=3.57989e-01, test loss=5.13867e-01 arcitecture_reduction: 4.08487e-01                                              \n",
    "Test [1, 15.000000] training accuracy=0.902500 test accuracy=0.822500 training loss=3.51265e-01, test loss=6.14315e-01 arcitecture_reduction: 4.15208e-01\n",
    "...\n",
    "Test [25, 115.000000] training accuracy=0.947500 test accuracy=0.872500 training loss=1.28096e-01, test loss=4.63626e-01 arcitecture_reduction: 4.99841e-01                                            \n",
    "Test [25, 120.000000] training accuracy=0.957500 test accuracy=0.815000 training loss=1.44876e-01, test loss=5.65249e-01 arcitecture_reduction: 4.99830e-01                                            \n",
    "Test [25, 125.000000] training accuracy=0.917500 test accuracy=0.862500 training loss=1.69795e-01, test loss=4.57527e-01 arcitecture_reduction: 4.99909e-01                                            \n",
    "Train steps: 100%|| 125/125.0 [01:33<00:00,  1.34it/s]\n",
    "Train epochs: 100%|| 25/25 [39:54<00:00, 95.79s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:05<00:00,  4.40it/s]\n",
    "test_accuracy=0.8586\n",
    "Finished cell2d Test\n",
    "\n",
    "```\n",
    "\n",
    "- Training and test accuracy staied practiclly unchanged\n",
    "- Cut short training when test accuracy and lost begins to rise and then final training with 1e-3 learning rate\n",
    "\n",
    "```\n",
    "py networks/cell2d.py -target_structure=0.5 -epochs=10 -prune=True -search_structure=False -learning_rate=1e-3 -model_src=crisp20220121_t50_01 -model_dest=crisp20220121_t50_02 \n",
    "...\n",
    "Test [10, 120.000000] training accuracy=0.987500 test accuracy=0.862500 training loss=6.64550e-02, test loss=5.17078e-01 arcitecture_reduction: 4.99998e-01                                            \n",
    "Test [10, 125.000000] training accuracy=0.972500 test accuracy=0.875000 training loss=9.10177e-02, test loss=4.40394e-01 arcitecture_reduction: 5.00002e-01                                            \n",
    "Train steps: 100%|| 125/125.0 [01:33<00:00,  1.33it/s]\n",
    "Train epochs: 100%|| 10/10 [15:58<00:00, 95.88s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:05<00:00,  4.41it/s]\n",
    "test_accuracy=0.8773\n",
    "```\n",
    "\n",
    "![60% pruned class weights](../img/crisp20220121_t50_02_gn.png)\n",
    "\n",
    "- Pruned middle channels so much that instead of an increase in channels, there is a decrease.\n",
    "- Kept the full width on the final (far right section especially near the jump in channels.\n",
    "- Repeat same process for pruning 75%\n",
    "``` cmd\n",
    "py networks/cell2d.py -target_structure=0.25 -epochs=75 -prune=False -search_structure=True -learning_rate=1e-2 -model_dest=crisp20220121_t25_00\n",
    "...\n",
    "Test [75, 120.000000] training accuracy=0.902500 test accuracy=0.865000 training loss=2.17066e-01, test loss=4.69373e-01 arcitecture_reduction: 2.51623e-01                                            \n",
    "Test [75, 125.000000] training accuracy=0.907500 test accuracy=0.877500 training loss=2.20994e-01, test loss=4.40662e-01 arcitecture_reduction: 2.51564e-01                                            \n",
    "Train steps: 100%|| 125/125.0 [01:44<00:00,  1.19it/s]\n",
    "Train epochs: 100%|| 75/75 [2:17:58<00:00, 110.38s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  4.06it/s]\n",
    "test_accuracy=0.8637\n",
    "```\n",
    "- Clean convergence\n",
    "- Will prune out the middle of the last two resnet levels\n",
    "![Target .25 Gradient Norm](../img/crisp20220121_t25_00_gn.png)\n",
    "![Target .25 Class Weights](../img/crisp20220121_t25_00_cw.png)\n",
    "![Target .25 Tensorboard](../img/crisp20220121_t25_00_tb.png)\n",
    "- Prune and train for 25 epochs at 1e-2 learning rate\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=25 -prune=True -search_structure=False -learning_rate=1e-2 -model_src=crisp20220121_t25_00 -model_dest=crisp20220121_t25_01\n",
    "...\n",
    "Total Trainable Params: 9047900\n",
    "Reduced parameters 9047900/22183066 = 0.40787418655293184\n",
    "Train epochs:   0%|                                                                                                                                                             | 0/25 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.920000 test accuracy=0.845000 training loss=6.92640e+00, test loss=7.11933e+00 arcitecture_reduction: 1.84899e-01                                               \n",
    "Train epochs:   0%|                                                                                                                                                             | 0/25 [00:06<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 10.000000] training accuracy=0.922500 test accuracy=0.847500 training loss=6.87471e+00, test loss=7.03949e+00 arcitecture_reduction: 1.89374e-01                                              \n",
    "Test [1, 15.000000] training accuracy=0.927500 test accuracy=0.847500 training loss=6.77528e+00, test loss=6.99641e+00 arcitecture_reduction: 1.94217e-01                                              \n",
    "Test [1, 20.000000] training accuracy=0.932500 test accuracy=0.870000 training loss=6.69325e+00, test loss=6.78030e+00 arcitecture_reduction: 1.99556e-01                                              \n",
    "T\n",
    "```\n",
    "- 60% compression, preserved accuracy follwing pruning\n",
    "``` cmd\n",
    "est [25, 115.000000] training accuracy=0.952500 test accuracy=0.895000 training loss=3.89919e+00, test loss=4.12011e+00 arcitecture_reduction: 3.86886e-01                                            \n",
    "Test [25, 120.000000] training accuracy=0.937500 test accuracy=0.845000 training loss=3.90302e+00, test loss=4.37877e+00 arcitecture_reduction: 3.86887e-01                                            \n",
    "Test [25, 125.000000] training accuracy=0.930000 test accuracy=0.880000 training loss=3.90966e+00, test loss=4.23690e+00 arcitecture_reduction: 3.86887e-01                                            \n",
    "Train steps: 100%|| 125/125.0 [02:05<00:00,  1.00s/it]\n",
    "Train epochs: 100%|| 25/25 [52:55<00:00, 127.03s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:32<00:00,  1.30s/it]\n",
    "test_accuracy=0.8694\n",
    "```\n",
    "- 10 Epchs training with 1e-3 learning rate\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=10 -prune=True -search_structure=False -learning_rate=1e-3 -model_src=crisp20220121_t25_01 -model_dest=crisp20220121_t25_02 -debug\n",
    "...\n",
    "Test [10, 115.000000] training accuracy=0.982500 test accuracy=0.900000 training loss=3.82421e+00, test loss=4.13265e+00 arcitecture_reduction: 3.86929e-01                                            \n",
    "Test [10, 120.000000] training accuracy=0.977500 test accuracy=0.875000 training loss=3.81356e+00, test loss=4.28123e+00 arcitecture_reduction: 3.86929e-01                                            \n",
    "Test [10, 125.000000] training accuracy=0.987500 test accuracy=0.895000 training loss=3.82077e+00, test loss=4.24202e+00 arcitecture_reduction: 3.86929e-01                                            \n",
    "Train steps: 100%|| 125/125.0 [01:59<00:00,  1.05it/s]\n",
    "Train epochs: 100%|| 10/10 [20:18<00:00, 121.84s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:06<00:00,  3.94it/s]\n",
    "test_accuracy=0.8831\n",
    "```\n",
    "- Nice to get a couple of percentage boost from this\n",
    "- Pruning created a second bottleneck in the same position\n",
    "![](../img/crisp20220121_t25_02_gn.png)\n",
    "- Next prune even more to find the point the performance degrades\n",
    "- Set -target_structure to 0.01\n",
    "- Boost -k_structure from 1e1 to 1e3 so it forces the network size over model accuracy\n",
    "```cmd\n",
    "py networks/cell2d.py -epochs=75 -prune=False -search_structure=True -target_structure=0.01 -k_structure=1e3  -learning_rate=1e-2 -model_dest=crisp20220121_t01_00\n",
    "...\n",
    "Test [75, 120.000000] training accuracy=0.902500 test accuracy=0.845000 training loss=2.76101e-01, test loss=4.96990e-01 arcitecture_reduction: 1.10572e-02                                                                                 \n",
    "Test [75, 125.000000] training accuracy=0.887500 test accuracy=0.845000 training loss=2.96646e-01, test loss=4.88102e-01 arcitecture_reduction: 1.10523e-02                                                                                 \n",
    "Train steps: 100%|| 125/125.0 [02:21<00:00,  1.13s/it]\n",
    "Train epochs: 100%|| 75/75 [2:54:07<00:00, 139.30s/it]| 125/125.0 [02:21<00:00,  1.48s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:07<00:00,  3.38it/s]\n",
    "test_accuracy=0.8552\n",
    "```\n",
    "- The traing was suprisngly normal.\n",
    "![1% strcture unpruned class weigthts](../img/crisp20220121_t01_00_cw.png)\n",
    "![1% strcture unpruned gradient norm](../img/crisp20220121_t01_00_gn.png)\n",
    "![1% strcture unpruned tensorboard](../img/crisp20220121_t01_00_tb.png)\n",
    "- Reduce learning rate from 1e-2 to 1e-3 and to a final training before pruning\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=10 -prune False -search_structure True -target_structure=0.01 -k_structure=1e3  -learning_rate=1e-3 -model_src=crisp20220121_t01_00 -model_dest=crisp20220121_t01_01\n",
    "...\n",
    "Test [10, 120.000000] training accuracy=0.957500 test accuracy=0.887500 training loss=1.41753e-01, test loss=4.24293e-01 arcitecture_reduction: 1.13174e-02                                                                                 \n",
    "Test [10, 125.000000] training accuracy=0.932500 test accuracy=0.867500 training loss=1.61560e-01, test loss=4.10597e-01 arcitecture_reduction: 1.13180e-02                                                                                 \n",
    "Train steps: 100%|| 125/125.0 [02:20<00:00,  1.12s/it]\n",
    "Train epochs: 100%|| 10/10 [23:40<00:00, 142.10s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:07<00:00,  3.45it/s]\n",
    "test_accuracy=0.8734\n",
    "```\n",
    "- Prune and train after pruning:\n",
    "``` cmd\n",
    "py networks/cell2d.py -epochs=10 -prune True -search_structure False -target_structure=0.01 -k_structure=1e3  -learning_rate=1e-2 -model_src=crisp20220121_t01_01 -model_dest=crisp20220121_t01_02\n",
    "...\n",
    "Total Trainable Params: 3071136\n",
    "Reduced parameters 3071136/22183066 = 0.13844506435674853\n",
    "...\n",
    "Total Trainable Params: 3071136\n",
    "Reduced parameters 3071136/22183066 = 0.13844506435674853\n",
    "Train epochs:   0%|                                                                                                                                                                                                  | 0/10 [00:00<?, ?it/s/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
    "  return F.mse_loss(input, target, reduction=self.reduction)\n",
    "Test [1, 5.000000] training accuracy=0.330000 test accuracy=0.347500 training loss=2.50454e+00, test loss=1.86002e+00 arcitecture_reduction: 9.01897e-03                                                                                    \n",
    "Test [1, 10.000000] training accuracy=0.367500 test accuracy=0.405000 training loss=1.79165e+00, test loss=1.65403e+00 arcitecture_reduction: 8.95732e-03                                                                                   \n",
    "Test [1, 15.000000] training accuracy=0.462500 test accuracy=0.520000 training loss=1.55283e+00, test loss=1.42762e+00 arcitecture_reduction: 8.91929e-03                                                                                   \n",
    "Test [1, 20.000000] training accuracy=0.552500 test accuracy=0.515000 training loss=1.40417e+00, test loss=1.30682e+00 arcitecture_reduction: 8.90896e-03                                                                                   \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae4c22-202a-465e-876d-874ea3fdafb3",
   "metadata": {},
   "source": [
    "25 Jan, 2022\n",
    "- Gave algorim name: Continuous Relaxation in Structured Pruning - CRISP\n",
    "- The plot below shows structured Resenet 50 on CIFAR-10 data set\n",
    "- Pruning accurcy improves to 93% compression then a rapid drop.\n",
    "- Poor convergence in my resnet 152 structure,  I believe this is because I have 1x1 convolutions in the residual path.  Restructure the residual path\n",
    "1. Get training to work on hiocnn\n",
    "1. Try algorithms claiming improved results vs adam\n",
    "1. Try pretraining (coco/imagenet) before CIFAR-10\n",
    "1. Move to segment/denoise network\n",
    "1. Low-light image enhancement\n",
    "1. Will mlflow help to understand and track experaments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09df6399-b1c6-4cdd-82ca-6efad054b99b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Baseline\n",
    "x = [.50, 1.0] \n",
    "y = [.77,.77] \n",
    "plt.plot(x, y, color='purple')\n",
    "\n",
    "#Variational Dropout\n",
    "x = [.50, 0.8, .90, .979, .98] \n",
    "y = [.769,.76, .74, .61, .60] \n",
    "plt.plot(x, y, color='blue', label='variational dropout')\n",
    "\n",
    "#Magnitude Pruning\n",
    "x = [.50, 0.8, .90, .975] \n",
    "y = [.769,.76, .74, .59] \n",
    "plt.plot(x, y, color='Green', label='magnitude pruning')\n",
    "\n",
    "# Random Pruning\n",
    "x = [.50,.70, .80,.90,.95,.97] \n",
    "y = [.74,.725,.70,.65,.55,.45] \n",
    "plt.plot(x, y, color='yellow', label='random pruning')\n",
    "\n",
    "# CRISP\n",
    "y = [0.8773, 0.8831, 0.887, 0.8943, 0.8798, 0.8829, 0.8767, 0.8582, 0.8367, 0.7214,0.5562] \n",
    "x = [1-13376476/22183066, 1-9047900/22183066, 1-6686420/22183066 ,1-3484738/22183066, 1-2207584/22183066,1-1863924/22183066 , 1-1712585/22183066, 1-1392685/22183066, 1-1214003/22183066, 1-1139234/22183066, 1-1104412/22183066] \n",
    "x1, y1 = [1-13376476/22183066, 1.0], [0.8773, 0.8773]\n",
    "plt.plot(x, y, color='orange', label='CRISP (our method)')\n",
    "plt.title('Sparcity/Accuracy trade-off for Resnet-50')\n",
    "plt.ylabel('Top-1 Accuracy')\n",
    "plt.xlabel('Sparsity')\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832233e-b661-4870-be02-139839a41731",
   "metadata": {},
   "source": [
    "27 January 2022\n",
    "- created classify/crisp.py based on cell2d Test\n",
    "- Executing pytorch resnet classifier so I can have a reference on a common resnet implementaion\n",
    "- Testing performance\n",
    "- Model my prunable resnet on the pytorch resnet implementaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9ff16",
   "metadata": {},
   "source": [
    "29 January 2022\n",
    "- Restructured network to have a straight residual branch execpt for the 1x1 resize convolutions\n",
    "- Pruning weights are not properly accounting for collapsing a branch.  When branch collapses, not just the searchable networks collpase but the whole convolutional branch collapses.\n",
    "- Need to provide a weighting function where the weight of the convolutions become 0 when the channel becomes 0 and grows quickly to the maximum when the channel diverges from 0\n",
    "- Need a differential weight to wight the full collaps of a set of convolutions.\n",
    "- The layer weight provides a norm of each layer\n",
    "- conv_weights provide a maximumn value of each layer weight\n",
    "- The ration is a weighting of how much is left of each cnn\n",
    "- tanh provides a continuous scaling with a small gradient except where the value is near 0\n",
    "- Multiply the produce of these scalings by the architecture weight\n",
    "- \n",
    "\n",
    "\n",
    "``` python\n",
    "architecture_weights = architecture_weights.sum_to_size((1))\n",
    "cnn_weights = torch.tanh(cnn_weights*self.cell_convolution)\n",
    "architecture_weights *= torch.prod(cnn_weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f599447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm=0.08099694781629463 (0.08099694781629463,0.9449026564916556), \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_gain = 22.0\n",
    "x = np.arange(0.0, 1.0, 0.01) \n",
    "y = np.tanh(weight_gain*x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "conv_weights=[0.0806, 0.0232, 0.110, 0.0939, 0.0350, 0.0740, 0.0283, 0.0982, 0.1181]\n",
    "conv_weights = np.linalg.norm(conv_weights)/np.sqrt(len(conv_weights))\n",
    "conv_loss = np.tanh(weight_gain*conv_weights)\n",
    "print('norm={} ({},{}), '.format(conv_weights, conv_weights, conv_loss))\n",
    "conv_loc = plt.Circle((conv_weights, conv_loss), 0.01, color='r')\n",
    "ax = plt.gca()\n",
    "ax.add_patch(conv_loc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588f4f0",
   "metadata": {},
   "source": [
    "31 January 2022\n",
    "- Add visualization to convolution weighting function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf45a43",
   "metadata": {},
   "source": [
    "1 February 2020\n",
    "- Restrucured resnet so the residual path only encounters 1x1 convolutions on size changes to match the standard implementation\n",
    "- SVG jointly searched poorly - never pruned out a whole convolution path\n",
    "- Adam jointly searched model and pruning weights\n",
    "- Minimize booth error and size to zero\n",
    "- Propegates gradient norm more uniformaly across network\n",
    "- Need to autoscale gradient norm\n",
    "- Optimizer rate schedule working well: optim.lr_scheduler.MultiStepLR\n",
    "- Not achieving the same accuracy as before but this may be because the 8x downsizing to the beginning of resnet.\n",
    "- See what happens if I pull of the 8x downsizing convolution at the begining\n",
    "    ![](../img/crisp20220201_t00_01_tb.png)\n",
    "    ![](../img/crisp20220201_t00_01_cw.png)\n",
    "    ![](../img/crisp20220201_t00_01_gn.png)\n",
    "    ```cmd\n",
    "    [113/300,      6/62.5]  accuracy: 0.927500|0.826250 loss: 2.23489e-01|5.70975e-01 remaining: 5.51220e-02 (train|test)                                                                         \n",
    "    [113/300,     12/62.5]  accuracy: 0.916250|0.823750 loss: 2.22670e-01|6.13765e-01 remaining: 5.51220e-02 (train|test)                                                                         \n",
    "    [113/300,     18/62.5]  accuracy: 0.913750|0.805000 loss: 2.17565e-01|7.09836e-01 remaining: 5.51221e-02 (train|test)                                                                         \n",
    "    [113/300,     24/62.5]  accuracy: 0.928750|0.832500 loss: 2.18517e-01|6.02128e-01 remaining: 5.51221e-02 (train|test)                                                                         \n",
    "    Train epochs:  37%|                                                                                   | 112/300 [2:15:23<3:46:13, 72.20s/it^Train steps:  37%|                                                                                       | 23/62.5 [00:25<00:44,  1.13s/it]\n",
    "    Train epochs:  37%|                                                                                   | 112/300 [2:15:24<3:47:18, 72.54s/it]\n",
    "    ```\n",
    "- Prune and see how the stats work out\n",
    "    ```cmd\n",
    "    Total Trainable Params: 4668400\n",
    "    Reduced parameters 4668400/23581658 = 0.19796742027214542\n",
    "    [  1/100,      6/62.5]  accuracy: 0.861250|0.768750 loss: 3.77656e-01|8.04519e-01 remaining: 7.85033e-02 (train|test)                                                                         \n",
    "    [  1/100,     12/62.5]  accuracy: 0.858750|0.792500 loss: 3.90868e-01|7.15877e-01 remaining: 7.83786e-02 (train|test)                                                                         \n",
    "    [  1/100,     18/62.5]  accuracy: 0.867500|0.761250 loss: 3.77579e-01|7.25845e-01 remaining: 7.83118e-02 (train|test)                                                                         \n",
    "    [  1/100,     24/62.5]  accuracy: 0.892500|0.791250 loss: 3.56796e-01|6.41781e-01 remaining: 7.82733e-02 (train|test)                                                                         \n",
    "\n",
    "    ```\n",
    "- 5% remaining pruned 80% of network\n",
    "- Accuracy remained good\n",
    "- Full convolution paths not pruned because all channels need to be removed first.  See what happens if there is a lower threshold\n",
    "- If it doesn't prune a full convolution path successfully, I could add and additional set of paramets to prune these out.\n",
    "- Peaked at 82%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa04ea87",
   "metadata": {},
   "source": [
    "1 February 2022\n",
    "- Setting up mlflow\n",
    "```cmd\n",
    "$ helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "$ helm install mlf-db bitnami/postgresql --set postgresqlDatabase=mlflow_db --set postgresqlPassword=mlflow --set service.type=NodePort\n",
    "PostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:\n",
    "\n",
    "    mlf-db-postgresql.default.svc.cluster.local - Read/Write connection\n",
    "\n",
    "To get the password for \"postgres\" run:\n",
    "\n",
    "    export POSTGRES_PASSWORD=$(kubectl get secret --namespace default mlf-db-postgresql -o jsonpath=\"{.data.postgresql-password}\" | base64 --decode)\n",
    "\n",
    "To connect to your database run the following command:\n",
    "\n",
    "    kubectl run mlf-db-postgresql-client --rm --tty -i --restart='Never' --namespace default --image docker.io/bitnami/postgresql:11.14.0-debian-10-r28 --env=\"PGPASSWORD=$POSTGRES_PASSWORD\" --command -- psql --host mlf-db-postgresql -U postgres -d mlflow_db -p 5432\n",
    "\n",
    "\n",
    "\n",
    "To connect to your database from outside the cluster execute the following commands:\n",
    "\n",
    "    export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\"{.items[0].status.addresses[0].address}\")\n",
    "    export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].nodePort}\" services mlf-db-postgresql)\n",
    "    PGPASSWORD=\"$POSTGRES_PASSWORD\" psql --host $NODE_IP --port $NODE_PORT -U postgres -d mlflow_db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5623999-b91d-4b73-9aef-9028672596eb",
   "metadata": {},
   "source": [
    "2 February 2022\n",
    "- Resent 152 training is progressing well.  Test accuracy 70% and rising\n",
    "- Removed resnet initial 7x7 convolution\n",
    "- Updated gradient norm plot with autoscale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918f711",
   "metadata": {},
   "source": [
    "3 February 2022\n",
    "- With a erodible resnet with a cleaned up residual path\n",
    "- Resnet 152 trains without difficulties\n",
    "- The final convolution of a residual block must output the number of channels required to sum with the residual\n",
    "- We can still prune the entire residual function ([Deep Residual Learning for Image Recognition figure 5](https://arxiv.org/pdf/1512.03385.pdf)) convolution path\n",
    "- To reward the optimizer for removing an entire residual function in order to reduce model size, there must be a reduction of loss that increases as channel pruning approaches 0\n",
    "- Method: \n",
    "    1. store original size of a the ConvBR structure\n",
    "    1. If ConvBR.search_structure is True, weight decay is proportional to the remaining channels\n",
    "    1. If ConvBR.search_structure is False, its weight is apportioned among those that are true\n",
    "    1. If any of the convolutions in the residual function are 0, the whole convolution branch disappears\n",
    "    1. The product of the ration of current channels to total channels achieves this: \n",
    "        $$\n",
    "        \\newcommand{\\rem}{r=\\textrm{remaining channels}}\n",
    "        \\newcommand{\\all}{f=\\textrm{total channels}}\n",
    "        \\newcommand{\\aw}{a=\\textrm{architecture weights}}\n",
    "\n",
    "        aw = \\sum_{i = 1}^{m} x + K_i * tanh(\\prod_{j = 1}^{n} \\frac{ r_{j}}{ t_{j}})\n",
    "        \\\\\n",
    "        \\\\\n",
    "        \\rem{} \\\\\n",
    "        \\all{} \\\\\n",
    "        \\aw{}\n",
    "        $$\n",
    "    1. One different between the target size and the pruned size in this formation is due to the size reduction of the residual function not being accounted for\n",
    "    1. A second difference is the residual is pruned at a fixed point where the function is continuous.  The hyperbolic tangent reduces this differences and enables us to select a specific value to prune at. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ae605",
   "metadata": {},
   "source": [
    "7 February, 2022\n",
    "- SGD optimizer: optim.SGD\n",
    "- CIFAR-10 Resnet 56 default data augmentation max test accuracy 75%\n",
    "- CIFAR-10 Resnet 56 node data augmentation max test accuracy 87%\n",
    "- CIFAR-10 Resnet 56 0 mean, 1 std, translate 0.125, Nearest interpolation, test accuracy 85%\n",
    "    ```cmd\n",
    "    Train steps: 100%|| 125/125.0 [01:37<00:00,  1.28it/s]\n",
    "    Train epochs: 100%|| 150/150 [4:06:52<00:00, 98.75s/it]\n",
    "    Test steps: 100%|| 25/25.0 [00:03<00:00,  7.00it/s]\n",
    "    test_accuracy=0.8495\n",
    "    ```\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b88f72",
   "metadata": {},
   "source": [
    "8 February 2022\n",
    "- CIFAR-10 Resnet 0 mean, 1 std, translate 0.125, BILINEAR interpolation, 54 target architecture 10% max test accuracy 86%\n",
    "    ``` cmd\n",
    "    [134/150,     45/125.0]  accuracy: 0.985000|0.857500 loss: 8.80765e-02|6.11403e-01 remaining: 1.57671e-01 (train|test)                                                                                                                                      \n",
    "    [134/150,     50/125.0]  accuracy: 0.980000|0.825000 loss: 9.38828e-02|6.25705e-01 remaining: 1.57671e-01 (train|test)                                                                                                                                      \n",
    "    Train epochs:  89%|                    | 133/150 [3:42:44<28:26, 100.36s/it]                  \n",
    "    ^Cain steps:  41%|                                                                                                                     | 51/125.0 [00:41<01:01,  1.21it/s]\n",
    "\n",
    "    ```\n",
    "- Pruning\n",
    "```cmd\n",
    "Total Trainable Params: 433606\n",
    "Reduced parameters 433606/467186 = 0.9281228461469307\n",
    "```\n",
    "- Getting about the same results with the minimal CIFAR-10 Resenet as the big resnet described in the [Resnet paper](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- I am getting about 2x the error as the Table 6 in the Resnet paper after applying the same parameters and preprocessing.\n",
    "- Sources of error: unoptimized parameters, error in network, pretraining, corrections in CIFAR, loss function\n",
    "- Pytorch Resnet had similar results \n",
    "- Moved to padding and random cropping.  test_accuracy=0.8231 did not improve:\n",
    "```cmd\n",
    "[ 80/80,    125/125.0]  accuracy: 0.957500|0.837500 loss: 1.65346e-01|6.19811e-01 remaining: 9.74377e-01 (train|test) | 124/125.0 [01:21<00:00,  1.60it/s]\n",
    "Train epochs:  99%| | 79/80 [1:50:07<01:22, 82.5Train steps: 100%|| 125/125.0 [01:22<00:00,  1.52it/s]\n",
    "Train epochs: 100%|| 80/80 [1:50:08<00:00, 82.5Train epochs: 100%|| 80/80 [1:50:08<00:00, 82.61s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.91it/s]\n",
    "test_accuracy=0.8231\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcdc0d",
   "metadata": {},
   "source": [
    "9 February 2022\n",
    "- 100% training \n",
    "    ```cmd\n",
    "        parser.add_argument('-augment_rotation', type=float, default=0.0, help='Input augmentation rotation degrees')\n",
    "        parser.add_argument('-augment_scale_min', type=float, default=1.00, help='Input augmentation scale')\n",
    "        parser.add_argument('-augment_scale_max', type=float, default=1.00, help='Input augmentation scale')\n",
    "        parser.add_argument('-augment_translate_x', type=float, default=0.25, help='Input augmentation translation')\n",
    "        parser.add_argument('-augment_translate_y', type=float, default=0.25, help='Input augmentation translation')\n",
    "        parser.add_argument('-augment_noise', type=float, default=0.0, help='Input augmentation rotation degrees')\n",
    "\n",
    "    [120/120,    125/125.0]  accuracy: 0.977500|0.817500 loss: 1.02125e-01|7.67410e-01 remaining: 9.73807e-01 (train|test) | 124/125.0 [01:26<00:00,  1.54it/s]\n",
    "    Train epochs:  99%|Train steps: 100%|| 125/125.0 [01:27<00:00,  1.43it/s]\n",
    "    Train epochs: 100%|Train epochs: 100%|| 120/120 [5:24:31<00:00, 162.26s/it]\n",
    "    Test steps: 100%|| 25/25.0 [00:03<00:00,  7.81it/s]\n",
    "    test_accuracy=0.766\n",
    "    ```\n",
    "- The initial convolution had a residual branch.  I eliminated it and retrained 50 cycles: crisp20220209_t100_00 \\\n",
    "    ![Tensorboard](../img/crisp20220209_t100_00_tb.png)\n",
    "    ```cmd\n",
    "    [ 50/50,    120/125.0]  accuracy: 0.907500|0.810000 loss: 2.89950e-01|6.16024e-01 remaining: 9.74487e-01 (train|test)       | 119/125.0 [02:04<00:05,  1.04it/s]\n",
    "    Train epochs:  98%|   | 49/50 [1:51:15<02:12, 133.00                                                                                                                                                                                                               [ 50/50,    125/125.0]  accuracy: 0.905000|0.837500 loss: 3.08497e-01|4.81070e-01 remaining: 9.74485e-01 (train|test) | 124/125.0 [02:09<00:00,  1.03it/s]\n",
    "    Train epochs:  98%|   | 49/50 [1:51:20<02:12, 133.00Train steps: 100%|| 125/125.0 [02:11<00:00,  1.05s/it]\n",
    "    Train epochs: 100%|| 50/50 [1:51:21<00:00, 132.77Train epochs: 100%|| 50/50 [1:51:21<00:00, 133.63s/it]\n",
    "    Test steps: 100%|| 25/25.0 [00:03<00:00,  6.58it/s]\n",
    "    test_accuracy=0.8259\n",
    "    ```\n",
    "- Continue training: crisp20220209_t100_01\n",
    "    ```cmd\n",
    "    [ 40/40,    120/125.0]  accuracy: 0.985000|0.815000 loss: 6.32229e-02|7.06654e-01 remaining: 9.74219e-01 (train|test)          \n",
    "    [ 40/40,    125/125.0]  accuracy: 0.987500|0.857500 loss: 5.66682e-02|5.80368e-01 remaining: 9.74219e-01 (train|test)          \n",
    "    Train steps: 100%|| 125/125.0 [01:27<00:00,  1.43it/s]\n",
    "    Train epochs: 100%|| 40/40 [1:02:56<00:00, 94.40s/it]\n",
    "    Test steps: 100%|| 25/25.0 [00:03<00:00,  7.50it/s]\n",
    "    test_accuracy=0.8676\n",
    "    ```\n",
    "- crisp20220209_t50_02 still has a target size of 100% but had a nice bump in accuracy.  Start with this one.\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.975000|0.837500 loss: 7.52500e-02|5.15717e-01 remaining: 9.74088e-01 (train|test)                                                                                                   \n",
    "[ 20/20,    125/125.0]  accuracy: 0.985000|0.887500 loss: 7.50653e-02|4.58454e-01 remaining: 9.74088e-01 (train|test)                                                                                                   \n",
    "Train steps: 100%|| 125/125.0 [01:29<00:00,  1.40it/s]\n",
    "Train epochs: 100%|| 20/20 [30:06<00:00, 90.33s/it]]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.41it/s]\n",
    "test_accuracy=0.8781\n",
    "Finished cell2d Test\n",
    "```\n",
    "- Train to 50%\n",
    "- Same accuracy\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.965000|0.852500 loss: 7.38883e-02|5.39016e-01 remaining: 5.01593e-01 (train|test)                                                                                                   \n",
    "[ 20/20,    125/125.0]  accuracy: 0.982500|0.887500 loss: 7.11580e-02|4.42782e-01 remaining: 5.01593e-01 (train|test)                                                                                                   \n",
    "Train steps: 100%|| 125/125.0 [01:26<00:00,  1.45it/s]\n",
    "Train epochs: 100%|| 20/20 [30:06<00:00, 90.34s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.55it/s]\n",
    "test_accuracy=0.8775\n",
    "Finished cell2d Test\n",
    "root@0b8e21e8af84:/app# \n",
    "```\n",
    "- Failed to prune and train.  Debug\n",
    "```cmd\n",
    "Total Trainable Params: 466643\n",
    "Reduced parameters 466643/466962 = 0.999316860900887\n",
    "Train steps:   0%|                                                                                                                                                                            | 0/125.0 [00:02<?, ?it/s]\n",
    "Train epochs:   0%|                                                                                                                                                                              | 0/20 [00:02<?, ?it/s]\n",
    "Traceback (most recent call last):\n",
    "  File \"networks/cell2d.py\", line 1309, in <module>\n",
    "    result = Test(args)\n",
    "  File \"networks/cell2d.py\", line 1145, in Test\n",
    "    outputs = model(inputs, isTraining=True)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 722, in forward\n",
    "    x = cell(x, isTraining=isTraining)\n",
    "  File \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
    "    return forward_call(*input, **kwargs)\n",
    "  File \"networks/cell2d.py\", line 426, in forward\n",
    "    y = x + residual\n",
    "RuntimeError: The size of tensor a (32) must match the size of tensor b (31) at non-singleton dimension 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf46c4f",
   "metadata": {},
   "source": [
    "10 February 2022\n",
    "- Corrected errors from \n",
    "- 50% training preserved test accuracy:\n",
    "``` cmd\n",
    "105/105,    120/125.0]  accuracy: 0.970000|0.857500 loss: 9.73530e-02|5.43195e-01 remaining: 5.04247e-01 (train|test)                                                                                                  \n",
    "[105/105,    125/125.0]  accuracy: 0.972500|0.870000 loss: 9.88603e-02|5.21643e-01 remaining: 5.04247e-01 (train|test)                                                                                                  \n",
    "Train steps: 100%|| 125/125.0 [01:27<00:00,  1.43it/s]\n",
    "Train epochs: 100%|| 105/105 [2:34:01<00:00, 88.02s/it] 125/125.0 [01:27<00:00,  1.31it/s]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.45it/s]\n",
    "test_accuracy=0.8582\n",
    "```\n",
    "- Prune and test\n",
    "- Pruning was targeting 50% but pruned 88% \\\n",
    "![Before pruning](../img/crisp20220209_t50_01_cw.png) \\\n",
    "![50% pruned to 88%](../img/crisp20220209_t50_03_cw.png) \\\n",
    "- One error in counting the parameters is when the size output of one convolution is reduced, the input size of the next one is reduced.\n",
    "-   This occurs along the residual cells of resnet but only the change in output cells is counted\n",
    "- Continuous relaxation considers weightings and floating point values.  When they are pruned it becomes 0 for what was pruned but remains floating point for what remains for the target weight.\n",
    "- Reduced parameters is the actual change in parameters, not the weighted values\n",
    "- Are the scale, bias, and batch norm weights counted correctly in the target structure?  The are counted as a ratio of the ConvBN parameters.\n",
    "![Parameters Compare](../img/crisp20220209_t50_01_03.png) \\\n",
    "- Trained a base model crisp20220210_t100_00 to quickly train the smaller models:\n",
    "    ``` cmd\n",
    "    [ 90/90,    120/125.0]  accuracy: 0.872500|0.832500 loss: 3.47883e-01|5.04425e-01 remaining: 9.74499e-01 (train|test)                                                                                                                                                                                                               \n",
    "    [ 90/90,    125/125.0]  accuracy: 0.870000|0.837500 loss: 3.81746e-01|4.89213e-01 remaining: 9.74499e-01 (train|test)                                                                                                                                                                                                               \n",
    "    Train steps: 100%|| 125/125.0 [01:53<00:00,  1.10it/s]\n",
    "    Train epochs: 100%|| 90/90 [2:51:02<00:00, 114.03s/it]\n",
    "    Test steps: 100%|| 25/25.0 [00:03<00:00,  7.91it/s]\n",
    "    test_accuracy=0.8429\n",
    "    Finished cell2d Test\n",
    "    ```\n",
    "- Train 90% \n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.920000|0.830000 loss: 3.38574e-01|4.57155e-01 remaining: 9.08950e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.900000|0.852500 loss: 3.36099e-01|4.53501e-01 remaining: 9.08950e-01 (train|test)                                \n",
    "Train steps: 100%|| 125/125.0 [01:55<00:00,  1.08it/s]\n",
    "Train epochs: 100%|| 20/20 [40:54<00:00, 122.71s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.46it/s]\n",
    "test_accuracy=0.8403\n",
    "Finished cell2d Test\n",
    "```\n",
    "- Train for 90%\n",
    "```python\n",
    "    parser.add_argument('-model_src', type=str,  default=\"crisp20220210_t100_00\")\n",
    "    parser.add_argument('-model_dest', type=str, default=\"crisp20220210_t90_00\")\n",
    "```\n",
    "- Nothing pruned\n",
    "```cmd\n",
    " 20/20,    120/125.0]  accuracy: 0.882500|0.855000 loss: 3.55629e-01|4.65631e-01 remaining: 9.08965e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.877500|0.845000 loss: 3.38583e-01|4.68672e-01 remaining: 9.08965e-01 (train|test)                                \n",
    "Train steps: 100%|| 125/125.0 [01:53<00:00,  1.10it/s]\n",
    "Train epochs: 100%|| 20/20 [37:59<00:00, 113.95s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.48it/s]\n",
    "test_accuracy=0.8449\n",
    "```\n",
    "- crisp20220210_t100_00 to crisp20220210_t90_00\n",
    "- Prune and final training\n",
    "- Nothing pruned:\n",
    "```cmd\n",
    "Total Trainable Params: 466962\n",
    "Reduced parameters 466962/466962 = 1.0\n",
    "```\n",
    "- Train crisp20220210_t100_00 to 80%: crisp20220210_t80_00\n",
    "```\n",
    "[ 20/20,    115/125.0]  accuracy: 0.862500|0.857500 loss: 3.62796e-01|4.36736e-01 remaining: 8.01001e-01 (train|test)                                \n",
    "[ 20/20,    120/125.0]  accuracy: 0.907500|0.840000 loss: 3.59787e-01|4.78969e-01 remaining: 8.01001e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.855000|0.862500 loss: 3.51241e-01|4.47174e-01 remaining: 8.01001e-01 (train|test)                                \n",
    "Train steps: 100%|| 125/125.0 [01:56<00:00,  1.07it/s]\n",
    "Train epochs: 100%|| 20/20 [39:41<00:00, 119.06s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.61it/s]\n",
    "test_accuracy=0.8445\n",
    "```\n",
    "![crisp20220210_t80_00](../img/crisp20220210_t80_00_cw.png)\n",
    "- It hit near 80:\n",
    "```cmd\n",
    "Total Trainable Params: 381702\n",
    "Reduced parameters 381702/466962 = 0.8174155498734372\n",
    "```\n",
    "- 80% Pruning reduced the initial convolutions and left the subsequent ones unchanged\n",
    "```\n",
    "[ 20/20,    120/125.0]  accuracy: 0.860000|0.845000 loss: 3.74923e-01|4.21833e-01 remaining: 7.78066e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.882500|0.862500 loss: 3.42709e-01|4.36758e-01 remaining: 7.78066e-01 (train|test)                                \n",
    "Train steps: 100%|| 125/125.0 [01:48<00:00,  1.15it/s]\n",
    "Train epochs: 100%|| 20/20 [36:34<00:00, 109.72s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  8.27it/s]\n",
    "test_accuracy=0.8483\n",
    "```\n",
    "![crisp20220210_t80_01](../img/crisp20220210_t80_01_cw.png)\n",
    "- Train crisp20220210_t100_00 for 70% target structure crisp20220210_t70_00\n",
    "- Pruned the initial 32 channel layers\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.855000|0.817500 loss: 3.19838e-01|4.90642e-01 remaining: 7.02769e-01 (train|test)                                \n",
    "[ 20/20,    125/125.0]  accuracy: 0.865000|0.845000 loss: 3.64346e-01|4.58762e-01 remaining: 7.02769e-01 (train|test)                                \n",
    "Train steps: 100%|| 125/125.0 [02:03<00:00,  1.01it/s]\n",
    "Train epochs: 100%|| 20/20 [40:37<00:00, 121.87s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:03<00:00,  7.11it/s]\n",
    "test_accuracy=0.844\n",
    "```\n",
    "![crisp20220210_t70_00](../img/crisp20220210_t70_00_cw.png) \\\n",
    "- prune and retrain\n",
    "70% remaining created 27% remaining\n",
    "```cmd\n",
    "Total Trainable Params: 124534\n",
    "Reduced parameters 124534/466962 = 0.2666897948869501\n",
    "```\n",
    "- Drop in accuracy\n",
    "![crisp20220210_t70_01](../img/crisp20220210_t70_01_cw.png)\n",
    "```cmd\n",
    "[ 20/20,    120/125.0]  accuracy: 0.810000|0.797500 loss: 2.43650e+00|2.45131e+00 remaining: 2.62802e-01 (train|test)                                                                                    \n",
    "[ 20/20,    125/125.0]  accuracy: 0.815000|0.790000 loss: 2.41060e+00|2.49772e+00 remaining: 2.62802e-01 (train|test)                                                                                    \n",
    "Train steps: 100%|| 125/125.0 [01:41<00:00,  1.23it/s]\n",
    "Train epochs: 100%|| 20/20 [32:31<00:00, 97.57s/it]\n",
    "Test steps: 100%|| 25/25.0 [00:02<00:00,  8.65it/s]\n",
    "test_accuracy=0.8134\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02727f48",
   "metadata": {},
   "source": [
    "14 February 2022\n",
    "- [U-Net: Convolutional Networks for Biomedical\n",
    "Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2796b",
   "metadata": {},
   "source": [
    "15 February 2022\n",
    "- Use set_extra_state and get_extra_state to extract pruned state along with segment.load_state_dict and torch.save(model.state_dict(), out_buffer)\n",
    "- State dictionary loading and saving rather that full object pickling will allow me to load data into a new object\n",
    "- In order to successfully train UNET with coco, I needed to drop the Adam learning rate to 1e-4\n",
    "- In order to successfully train UNET with coco, I needed to adjust class weights to [0.05,0.5, 1.0, 1.0]\n",
    "- With these settings both crisp and standard UNET are converging slowly\n",
    "- After the cross entropy loss dropped to ~ 0.3, I increased class weights [1.0,1.0, 1.0, 1.0] and learning rate to 1.0e-3\n",
    "- Failed to continue to converge.  Restart with class_weight [0.5,1.0, 1.0, 1.0] and learning_rate=2.5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fab92",
   "metadata": {},
   "source": [
    "16 February, 2022\n",
    "- Overnight trained 3 epochs, cross entropy loss 0.3 - Training 0.3 test\n",
    "- Set class weights [1.0,1.0, 1.0, 1.0] and learning rate to 1.0e-4\n",
    "- ConvBR channel pruning using : self.channel_scale = nn.Parameter(torch.zeros(self.out_channels, dtype=torch.float))\n",
    "- ConvBR channel output:\n",
    "    ```python\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "\n",
    "        if self.search_structure: #scale channels based on self.channel_scale\n",
    "                weight_scale = self.sigmoid(self.sigmoid_scale*self.channel_scale)[None,:,None,None]\n",
    "                x *= weight_scale\n",
    "        ...\n",
    "    ```\n",
    "- ConvBR architecture weights is based just on channel_scale\n",
    "    ``` python\n",
    "    def ArchitectureWeights(self):\n",
    "        conv_weights = self.sigmoid(self.sigmoid_scale*self.channel_scale)\n",
    "        cell_weights = model_weights(self)\n",
    "        architecture_weights = (cell_weights/ self.out_channels) * conv_weights.sum_to_size((1))\n",
    "\n",
    "        return architecture_weights, cell_weights, conv_weights\n",
    "    ```\n",
    "- Test after 1 training epoch:\n",
    "    ```cmd\n",
    "    [  1/1,  29496/29571.75]  loss: 1.70899e-01|2.09416e-01 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    [  1/1,  29520/29571.75]  loss: 1.90740e-01|3.37091e-02 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    [  1/1,  29544/29571.75]  loss: 1.78525e-01|6.42415e-02 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    [  1/1,  29568/29571.75]  loss: 2.20651e-01|1.39349e-01 remaining: 9.78487e-01 (train|test)                                                                                    \n",
    "    Train epochs:   0%|                                                                                                                                    | 0/1 [3:49:30<?, ?it/s]\n",
    "    /opt/conda/lib/python3.8/site-packages/tqdm/std.py:533: TqdmWarning: clamping frac to range [0, 1]| 29571/29571.75 [3:49:32<00:00,  1.86it/s]\n",
    "    Train steps: 100%|| 29572/29571.75 [3:49:32<00:00,  2.15it/s]\n",
    "    Train epochs: 100%|| 1/1 [3:49:33<00:00, 13773.56s/it]\n",
    "    Inference steps:   0%|                                                                                                                                                  | 2/1250 [00:01<16:26,  1.27it/s][W CPUAllocator.cpp:305] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
    "    Inference steps: 100%|| 1250/1250 [16:35<00:00,  1.26it/s]\n",
    "    Test results http://198.211.145.1:30990/mllib/test/segmentation/test_results.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220216T183557Z&X-Amz-Expires=7200&X-Amz-SignedHeaders=host&X-Amz-Signature=f68c8b7c5c199f2a65c3ac4dcaa99157ec4b4463ef8916d8ce44723699522f49\n",
    "    Finished network2d Test\n",
    "    ```\n",
    "- Generally the highest gradient norm was near the UNET input and ouptut.  The center was high at the very beginning and now at the very end \\\n",
    "![segment_nas_512x442_20220215_04 gradient norm](../img/segment_nas_512x442_20220215_04_gn.png)\n",
    "- UNET seems to train slowly compared to FCN.\n",
    "- UNET seems more sensitive to training imbalance compared to FCN\n",
    "-  There continues to be improvement in cross entropy loss \n",
    "- Training and test remain close.  \n",
    "- Coco is a has ~ 118K/5K images and it has been trained about 4 epochs with data augmentation so I would expect good correspondence\\\n",
    "![segment_nas_512x442_20220215_04 cross entropy loss](../img/segment_nas_512x442_20220215_04_tb.png) \\\n",
    "- Try to minimize segment_nas_512x442_20220215_04 while preserving accuracy\n",
    "```cmd\n",
    "    model_src segment_nas_512x442_20220215_04\n",
    "    model_dest segment_nas_512x442_20220215_04_T00\n",
    "    learning_rate=1.0e-4\n",
    "    k_structure=1.0e-1\n",
    "    target_structure=0.00\n",
    "```\n",
    "- \n",
    "```cmd\n",
    "[  1/1,  29544/29571.75]  loss: 2.52414e-01|1.33732e-01 remaining: 5.29465e-01 (train|test)                                                                                                                                         \n",
    "[  1/1,  29568/29571.75]  loss: 2.30963e-01|1.83640e-01 remaining: 5.29322e-01 (train|test)                                                                                                                                         \n",
    "Train epochs:   0%|                                                                                                                                                                                         | 0/1 [4:39:32<?, ?it/s]\n",
    "/opt/conda/lib/python3.8/site-packages/tqdm/std.py:533: TqdmWarning: clamping frac to range [0, 1]| 29571/29571.75 [4:39:34<00:00,  1.52it/s]\n",
    "Train steps: 100%|| 29572/29571.75 [4:39:34<00:00,  1.76it/s]\n",
    "Train epochs: 100%|| 1/1 [4:39:35<00:00, 16775.57s/it]\n",
    "Inference steps:   0%|                                                                                                                                                                            | 2/1250 [00:01<17:05,  1.22it/s][W CPUAllocator.cpp:305] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
    "Inference steps: 100%|| 1250/1250 [17:09<00:00,  1.21it/s]\n",
    "Test results http://198.211.145.1:30990/mllib/test/segmentation/test_results.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=admin%2F20220217%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220217T050024Z&X-Amz-Expires=7200&X-Amz-SignedHeaders=host&X-Amz-Signature=817b832fccb95da462850090c8811c03a81fcb85a53e99407b8e343eeca3b02e\n",
    "Finished network2d Test\n",
    "```\n",
    "```json\n",
    "\"results\": {\n",
    "    \"confusion\": [\n",
    "        [\n",
    "            1004999160,\n",
    "            11424560,\n",
    "            15806900,\n",
    "            4502694\n",
    "        ],\n",
    "        [\n",
    "            23747040,\n",
    "            75022555,\n",
    "            1754604,\n",
    "            1071338\n",
    "        ],\n",
    "        [\n",
    "            13971183,\n",
    "            336314,\n",
    "            31813327,\n",
    "            61794\n",
    "        ],\n",
    "        [\n",
    "            10597968,\n",
    "            1193715,\n",
    "            464631,\n",
    "            32032217\n",
    "        ]\n",
    "    ],\n",
    "    \"similarity\": {\n",
    "        \"0\": {\n",
    "            \"intersection\": 1004999160,\n",
    "            \"union\": 1085049505,\n",
    "            \"similarity\": 0.9262242463305856\n",
    "        },\n",
    "        \"1\": {\n",
    "            \"intersection\": 75022555,\n",
    "            \"union\": 114550126,\n",
    "            \"similarity\": 0.6549321036975551\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"intersection\": 31813327,\n",
    "            \"union\": 64208753,\n",
    "            \"similarity\": 0.49546713670019416\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"intersection\": 32032217,\n",
    "            \"union\": 49924357,\n",
    "            \"similarity\": 0.6416150136896105\n",
    "        }\n",
    "    },\n",
    "    \"average time\": 0.006536458800000008,\n",
    "    \"mean intersection over union\": 0.6795596251044863,\n",
    "    \"num images\": 5000\n",
    "},\n",
    "\"config\": {\n",
    "    \"debug\": false,\n",
    "    \"debug_port\": 3000,\n",
    "    \"debug_address\": \"0.0.0.0\",\n",
    "    \"fast\": false,\n",
    "    \"credentails\": \"creds.json\",\n",
    "    \"trainingset\": \"data/coco/annotations/instances_train2017.json\",\n",
    "    \"validationset\": \"data/coco/annotations/instances_val2017.json\",\n",
    "    \"train_image_path\": \"data/coco/train2017\",\n",
    "    \"imStatistics\": false,\n",
    "    \"val_image_path\": \"data/coco/val2017\",\n",
    "    \"class_dict\": \"model/segmin/coco.json\",\n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 1,\n",
    "    \"model_type\": \"segmentation\",\n",
    "    \"model_class\": \"segmin\",\n",
    "    \"model_src\": \"segment_nas_512x442_20220215_04_T00\",\n",
    "    \"model_dest\": \"segment_nas_512x442_20220215_04_T01\",\n",
    "    \"test_results\": \"test_results.json\",\n",
    "    \"cuda\": true,\n",
    "    \"height\": 480,\n",
    "    \"width\": 512,\n",
    "    \"imflags\": 1,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"max_search_depth\": 5,\n",
    "    \"min_search_depth\": 2,\n",
    "    \"max_cell_steps\": 3,\n",
    "    \"channel_multiple\": 2,\n",
    "    \"k_structure\": 0.25,\n",
    "    \"target_structure\": 0.0,\n",
    "    \"batch_norm\": false,\n",
    "    \"dropout_rate\": 0.0,\n",
    "    \"weight_gain\": 11.0,\n",
    "    \"sigmoid_scale\": 5.0,\n",
    "    \"feature_threshold\": 0.5,\n",
    "    \"convMaskThreshold\": 0.5,\n",
    "    \"residual\": false,\n",
    "    \"prune\": false,\n",
    "    \"train\": true,\n",
    "    \"infer\": true,\n",
    "    \"search_structure\": true,\n",
    "    \"onnx\": false,\n",
    "    \"job\": false,\n",
    "    \"test_dir\": \"/store/data/network2d\",\n",
    "    \"tensorboard_dir\": \"./tb\",\n",
    "    \"class_weight\": [\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0,\n",
    "        1.0\n",
    "    ],\n",
    "    \"description\": {\n",
    "        \"description\": \"NAS segmentation\"\n",
    "    }\n",
    "},\n",
    "\"system\": {\n",
    "    \"platform\": \"Linux-5.10.0-1057-oem-x86_64-with-glibc2.10\",\n",
    "    \"python\": \"3.8.12\",\n",
    "    \"numpy version\": \"1.21.4\"\n",
    "}\n",
    "```\n",
    "- Tensorboard shows training and test cross entropy loss remained rougly constant at about 0.2\n",
    "- Architecture loss proceeded smoothly with a magnitude at the end of about half of the cross-entropy loss\n",
    "- Architecture reduction is about 0.55 \\\n",
    "![Tensorboard](../img/segment_nas_512x442_20220215_04_T01_tb.png) \\\n",
    "- Channel weights show an erosion of channels in the decoder section of UNET \\\n",
    "![Class weights](../img/segment_nas_512x442_20220215_04_T01_cw.png) \\\n",
    "- Gradient norm is largest at the network input followed by the output\n",
    "- Residual connections trough the convolution regions should flatten the gradients through the network and may speed training \\\n",
    "![Gradient](../img/segment_nas_512x442_20220215_04_T01_gn.png) \\\n",
    "\n",
    "- Next, prune  segment_nas_512x442_20220215_04_T01 to segment_nas_512x442_20220215_04_T02, train for 1 epoch and see how pruned network performs\n",
    "```cmd\n",
    "Total Trainable Params: 31646930\n",
    "ApplyStructure\n",
    "network depth 5/5 = 1.0\n",
    "ConvBR::ApplyStructure convolution 0/2 search_structure=True 0.0=0/64 in_channels=3 out_channels=64\n",
    "ConvBR::ApplyStructure convolution 1/2 search_structure=True 0.0=0/64 in_channels=64 out_channels=64\n",
    "cell summary: weights=38849 in1_channels=3 in2_channels=0 out_channels=512 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/2 search_structure=True 0.0=0/128 in_channels=64 out_channels=128\n",
    "ConvBR::ApplyStructure convolution 1/2 search_structure=True 0.0=0/128 in_channels=128 out_channels=128\n",
    "cell summary: weights=221697 in1_channels=64 in2_channels=0 out_channels=512 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/2 search_structure=True 0.0=0/256 in_channels=128 out_channels=256\n",
    "ConvBR::ApplyStructure convolution 1/2 search_structure=True 0.00390625=1/256 in_channels=256 out_channels=255\n",
    "cell summary: weights=885761 in1_channels=128 in2_channels=0 out_channels=512 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/2 search_structure=True 0.00390625=2/512 in_channels=255 out_channels=510\n",
    "ConvBR::ApplyStructure convolution 1/2 search_structure=True 0.06640625=34/512 in_channels=510 out_channels=478\n",
    "cell summary: weights=3540993 in1_channels=255 in2_channels=0 out_channels=512 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/3 search_structure=True 0.357421875=366/1024 in_channels=478 out_channels=658\n",
    "ConvBR::ApplyStructure convolution 1/3 search_structure=True 0.595703125=610/1024 in_channels=658 out_channels=414\n",
    "ConvBR::ApplyStructure convolution 2/3 search_structure=True 0.390625=200/512 in_channels=414 out_channels=312\n",
    "ConvBR::ApplyStructure cell residual search_structure=False 0.390625=200/512 in_channels=478 out_channels=512\n",
    "cell summary: weights=16521217 in1_channels=478 in2_channels=0 out_channels=512 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/3 search_structure=True 0.73046875=374/512 in_channels=790 out_channels=138\n",
    "ConvBR::ApplyStructure convolution 1/3 search_structure=True 0.662109375=339/512 in_channels=138 out_channels=173\n",
    "ConvBR::ApplyStructure convolution 2/3 search_structure=True 0.26953125=69/256 in_channels=173 out_channels=187\n",
    "ConvBR::ApplyStructure cell residual search_structure=False 0.26953125=69/256 in_channels=790 out_channels=256\n",
    "cell summary: weights=7867393 in1_channels=312 in2_channels=478 out_channels=256 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/3 search_structure=True 0.46875=120/256 in_channels=442 out_channels=136\n",
    "ConvBR::ApplyStructure convolution 1/3 search_structure=True 0.37890625=97/256 in_channels=136 out_channels=159\n",
    "ConvBR::ApplyStructure convolution 2/3 search_structure=True 0.015625=2/128 in_channels=159 out_channels=126\n",
    "ConvBR::ApplyStructure cell residual search_structure=False 0.015625=2/128 in_channels=442 out_channels=128\n",
    "cell summary: weights=1967617 in1_channels=187 in2_channels=255 out_channels=128 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/3 search_structure=True 0.3125=40/128 in_channels=254 out_channels=88\n",
    "ConvBR::ApplyStructure convolution 1/3 search_structure=True 0.21875=28/128 in_channels=88 out_channels=100\n",
    "ConvBR::ApplyStructure convolution 2/3 search_structure=True 0.0=0/64 in_channels=100 out_channels=64\n",
    "ConvBR::ApplyStructure cell residual search_structure=False 0.0=0/64 in_channels=254 out_channels=64\n",
    "cell summary: weights=492289 in1_channels=126 in2_channels=128 out_channels=64 residual=False search_structure=True\n",
    "ConvBR::ApplyStructure convolution 0/3 search_structure=True 0.09375=6/64 in_channels=128 out_channels=58\n",
    "ConvBR::ApplyStructure convolution 1/3 search_structure=True 0.140625=9/64 in_channels=58 out_channels=55\n",
    "ConvBR::ApplyStructure convolution 2/3 search_structure=True 0.0=0/4 in_channels=55 out_channels=4\n",
    "cell summary: weights=111113 in1_channels=64 in2_channels=64 out_channels=4 residual=False search_structure=True\n",
    "...\n",
    "Total Trainable Params: 13379546\n",
    "Reduced parameters 13379546/31646930 = 0.42277547932769466\n",
    "...\n",
    "Total Trainable Params: 13379546\n",
    "Reduced parameters 13379546/31646930 = 0.42277547932769466\n",
    "[  1/1,     24/29571.75]  loss: 2.45062e-01|1.28190e-01 remaining: 8.81824e-01 (train|test)                                                                                                                                    \n",
    "[  1/1,     48/29571.75]  loss: 2.31084e-01|1.64998e-01 remaining: 8.81900e-01 (train|test)                                                                                                                                    \n",
    "[  1/1,     72/29571.75]  loss: 1.89927e-01|1.24898e-01 remaining: 8.81941e-01 (train|test)                                                                                                                                    \n",
    "[  1/1,     96/29571.75]  loss: 2.48598e-01|1.31237e-01 remaining: 8.81995e-01 (train|test)                                                                                                                                    \n",
    "```\n",
    "- Preserved (or improved)accuracy\n",
    "- Reduced size by 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38185832",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee99ae77",
   "metadata": {},
   "source": [
    "17 February, 2022\n",
    "- Improvement in class and total similarity after pruning\n",
    "- No improvement on inference time  \n",
    "- Before pruning: \"average time\": 0.006536458800000008\n",
    "- After pruning: \"average time\": 0.006610272000000009\n",
    "- Why - if the model can be fully loaded in the GPU, the change in execution may be minimal compared with the overhead of executing the model\n",
    "- Some improvement in training time\n",
    "```\n",
    "        \"results\": {\n",
    "            \"confusion\": [\n",
    "                [\n",
    "                    1015635123,\n",
    "                    15078195,\n",
    "                    3199779,\n",
    "                    2764627\n",
    "                ],\n",
    "                [\n",
    "                    19226725,\n",
    "                    81785236,\n",
    "                    407059,\n",
    "                    431455\n",
    "                ],\n",
    "                [\n",
    "                    20504247,\n",
    "                    654909,\n",
    "                    25018637,\n",
    "                    46536\n",
    "                ],\n",
    "                [\n",
    "                    12489446,\n",
    "                    1915986,\n",
    "                    29174,\n",
    "                    29612866\n",
    "                ]\n",
    "            ],\n",
    "            \"similarity\": {\n",
    "                \"0\": {\n",
    "                    \"intersection\": 1015635123,\n",
    "                    \"union\": 1088898142,\n",
    "                    \"similarity\": 0.9327182073564416\n",
    "                },\n",
    "                \"1\": {\n",
    "                    \"intersection\": 81785236,\n",
    "                    \"union\": 119499565,\n",
    "                    \"similarity\": 0.6843977716571604\n",
    "                },\n",
    "                \"2\": {\n",
    "                    \"intersection\": 25018637,\n",
    "                    \"union\": 49860341,\n",
    "                    \"similarity\": 0.5017742858998898\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"intersection\": 29612866,\n",
    "                    \"union\": 47290090,\n",
    "                    \"similarity\": 0.6261960169667683\n",
    "                }\n",
    "            },\n",
    "            \"average time\": 0.006610272000000009,\n",
    "            \"mean intersection over union\": 0.686271570470065,\n",
    "            \"num images\": 5000\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"debug\": false,\n",
    "            \"debug_port\": 3000,\n",
    "            \"debug_address\": \"0.0.0.0\",\n",
    "            \"fast\": false,\n",
    "            \"credentails\": \"creds.json\",\n",
    "            \"trainingset\": \"data/coco/annotations/instances_train2017.json\",\n",
    "            \"validationset\": \"data/coco/annotations/instances_val2017.json\",\n",
    "            \"train_image_path\": \"data/coco/train2017\",\n",
    "            \"imStatistics\": false,\n",
    "            \"val_image_path\": \"data/coco/val2017\",\n",
    "            \"class_dict\": \"model/segmin/coco.json\",\n",
    "            \"batch_size\": 4,\n",
    "            \"epochs\": 2,\n",
    "            \"model_type\": \"segmentation\",\n",
    "            \"model_class\": \"segmin\",\n",
    "            \"model_src\": \"segment_nas_512x442_20220215_04_T01\",\n",
    "            \"model_dest\": \"segment_nas_512x442_20220215_05_T00\",\n",
    "            \"test_results\": \"test_results.json\",\n",
    "            \"cuda\": true,\n",
    "            \"height\": 480,\n",
    "            \"width\": 512,\n",
    "            \"imflags\": 1,\n",
    "            \"learning_rate\": 0.0001,\n",
    "            \"max_search_depth\": 5,\n",
    "            \"min_search_depth\": 2,\n",
    "            \"max_cell_steps\": 3,\n",
    "            \"channel_multiple\": 2,\n",
    "            \"k_structure\": 0.25,\n",
    "            \"target_structure\": 1.0,\n",
    "            \"batch_norm\": false,\n",
    "            \"dropout_rate\": 0.0,\n",
    "            \"weight_gain\": 11.0,\n",
    "            \"sigmoid_scale\": 5.0,\n",
    "            \"feature_threshold\": 0.5,\n",
    "            \"convMaskThreshold\": 0.5,\n",
    "            \"residual\": false,\n",
    "            \"prune\": true,\n",
    "            \"train\": true,\n",
    "            \"infer\": true,\n",
    "            \"search_structure\": true,\n",
    "            \"onnx\": true,\n",
    "            \"job\": false,\n",
    "            \"test_dir\": \"/store/data/network2d\",\n",
    "            \"tensorboard_dir\": \"./tb\",\n",
    "            \"class_weight\": [\n",
    "                1.0,\n",
    "                1.0,\n",
    "                1.0,\n",
    "                1.0\n",
    "            ],\n",
    "            \"description\": {\n",
    "                \"description\": \"NAS segmentation\"\n",
    "            }\n",
    "        },\n",
    "        \"system\": {\n",
    "            \"platform\": \"Linux-5.10.0-1057-oem-x86_64-with-glibc2.10\",\n",
    "            \"python\": \"3.8.12\",\n",
    "            \"numpy version\": \"1.21.4\"\n",
    "        }\n",
    "```\n",
    "- Maintained accuracy while curing structure search\n",
    "\n",
    "![Tensorboard](../img/segment_nas_512x442_20220217s_05_T00_tb.png)\n",
    "![Class Weights](../img/segment_nas_512x442_20220217s_05_T00_cw.png)\n",
    "- Prune and train for 2 epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670844ef",
   "metadata": {},
   "source": [
    "18 February 2022\n",
    "- Good enough training to start pruning\n",
    "|Model|date | model class | test images | mean IoU | inference time|\n",
    "|-----|-----|-------------|-------------|----------|---------------|\n",
    "|5: segment_deeplabv3_512x442_20211113_00|11/14/2021, 06:50:10|deeplabv3|5000|0.809978|0.005556|\n",
    "|36: segment_nas_512x442_20220217s_04_T100|02/18/2022, 09:19:22|segmin|5000|0.728643|0.000656|\n",
    "- Prune over 3 epochs with target of 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f0f07",
   "metadata": {
    "tags": []
   },
   "source": [
    "19 February, 2022\n",
    "- Begin with segment_nas_512x442_20220217s_04_T100 and train segment_nas_512x442_20220217s_07_T50\n",
    "- k_structure=1.0e1, target_structure=0.50\n",
    "- Will the higher k_structure reduce test accuracy?\n",
    "- The higher k_structure did not appear to reduce inference accuracy based on to tensorboard cross-entropy loss\n",
    "- With k_structurea at 10, the network did arrive at the target structure in about 10,000 training setps (batch size 12, learning rate 1e04)\n",
    "- The search did not drive the class weight as close to 0 as is typical.  \n",
    "- Will this lead to a larger network change after pruning?\n",
    "- Will will lead to a larger difference between target size and pruned size \n",
    "- This has the same pattern as is typical in UNET pruning\n",
    "\n",
    "![Tensorboard](../img/segment_nas_512x442_20220217s_07_T50_tb.png)\n",
    "![Class Weights](../img/segment_nas_512x442_20220217s_07_T50_cw.png)]\n",
    "- 50% target resulted in 28% fetures remaining.\n",
    "- Network performance of pruned network is approaching performance before pruning\n",
    "- No full layers removed.  \n",
    "- Able to increase batch size from 12 to 24 after pruning and disabling structured search\n",
    "- This pruned network training time is ~1:15:00\n",
    "- Larger batch size should reduce the effective learning rate\n",
    "```cmd\n",
    "Reduced parameters 9078383/31646930 = 0.28686457106581903\n",
    "[  1/2,     24/9857.25]  loss: 3.90205e-01|2.52585e-01 remaining: 9.43220e-01 (train|test)                                                                                                  \n",
    "[  1/2,     48/9857.25]  loss: 2.29037e-01|2.84054e-01 remaining: 9.43220e-01 (train|test)                                                                                                  \n",
    "[  1/2,     72/9857.25]  loss: 2.16376e-01|1.03609e-01 remaining: 9.43220e-01 (train|test)                                                                                                  \n",
    "[  1/2,     96/9857.25]  loss: 1.74281e-01|2.28877e-01 remaining: 9.43220e-01 (train|test)                                                                                                  \n",
    "[  1/2,    120/9857.25]  loss: 2.20767e-01|2.05684e-01 remaining: 9.43220e-01 (train|test)  \n",
    "```\n",
    " - segment_nas_512x442_20220217i_08_T60 60% pruning target structure search results\n",
    " - Converged to target weight in ~ 8k steps\n",
    " - There didn't appear to be a degradation in cross-entropy loss in Tensorboard\n",
    " - cross-entropy loss ~ .166 in 80% of an epoch\n",
    " - The same center-right erosion of class weights pattern is applied\n",
    " - The first convolution after a downsize remains the highest\n",
    " - This creates bottlenecks in the number of channels\n",
    "![Tensorboard](../img/segment_nas_512x442_20220217i_08_T60_tb.png)\n",
    "![Class weights](../img/segment_nas_512x442_20220217i_08_T60_cw.png)\n",
    "- This pruning was everything I could have asked for.\n",
    "- when set to 0.6, we got 53% remaining parameters\n",
    "- Cross entropy loss was nearly identical\n",
    "```cmd\n",
    "segment_nas_512x442_20220217i_09_T60 remaining parameters 17047202/31646930 = 0.5386684269216635\n",
    "Train epochs:   0%|                                                                                                                         | 0/2 [00:00<?, ?it/sCorrupt JPEG data: premature end of data segment                                                                                       | 0/11828.7 [00:00<?, ?it/s]\n",
    "[  1/2,     24/11828.7]  loss: 1.73079e-01|1.40719e-01 remaining: 9.66222e-01 (train|test)                                                                        \n",
    "[  1/2,     48/11828.7]  loss: 1.45533e-01|1.31966e-01 remaining: 9.66222e-01 (train|test)                                                                        \n",
    "[  1/2,     72/11828.7]  loss: 1.68929e-01|2.07124e-01 remaining: 9.66222e-01 (train|test)                                                                        \n",
    "[  1/2,     96/11828.7]  loss: 1.51357e-01|9.75652e-02 remaining: 9.66222e-01 (train|test)                                                                        \n",
    "[  1/2,    120/11828.7]  loss: 1.68092e-01|1.90690e-01 remaining: 9.66222e-01 (train|test)\n",
    "```\n",
    "- Stop segment_nas_512x442_20220217s_08_T50 after first of 2 epochs to work on collapsing an entire cell.\n",
    "- Train segment_nas_512x442_20220217s_05_T10 from segment_nas_512x442_20220217s_04_T100 with search_structure=str2bool, default=True, target_structure=0.10, k_structure1.0e1, \n",
    "- Add in cell pruning: \n",
    "    - If any of the convolutions erode to near 0, architecture weight for cell should be near 0\n",
    "    - A cell is inactive if there is no path that a non-zero signal arrives\n",
    "    - If a cell nears 0, all other cells that become inactive should increase in weight\n",
    "    - If a cell on is pruned, all other cells that become inactive should be pruned\n",
    "    - In the encoder path, if a lower cell becomes zero, all cells at the same level and above are inactive\n",
    "    - In the decoder path, if a cell becomes zero, all cells at the same level and above are inactive\n",
    "- To compute network weight:\n",
    "    - At the UNET network level, accumulate all class weights into a list\n",
    "    - $ w_{l} = \\prod_{i=0}^{l-1} w_{l} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281b4e0-bf5e-4723-bd1f-9aa6879aaecf",
   "metadata": {},
   "source": [
    "22 February, 2022\n",
    "- Equations and plots for Microscopy and Microanalysis abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17031d2b-bfda-4c41-8e81-dc0e09be270d",
   "metadata": {},
   "source": [
    "$$ w_{l} = \\prod_{i=0}^{l-1} w_{l} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4678f2e-e1b5-48c9-8616-b437f1b01e82",
   "metadata": {},
   "source": [
    "$$ \\textrm{Conv2D}(N_i,C_{out_j}) = \\textrm{bias}(C_{out_j}) + \\sum_{k=0}^{C_{in}-1} \\textrm{weight}(C_{out_j}) \\star u(N_i,k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fdf7bf-5546-4a3e-bb42-1740758ae5bd",
   "metadata": {},
   "source": [
    "$$ \\textrm{BatchNorm}(\\textrm{Conv2D}(N_i,C_{out_j}))\\cdot\\textrm{sigmoid}(k_r \\cdot C_j) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c03ebf3-e9aa-4c04-a83a-a0b3cc6fd9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x, k=5.0):\n",
    "    return 1.0/(1.0+np.exp(-k*x))\n",
    "def gaussian(x):\n",
    "    stdev = .25\n",
    "    mean = 0\n",
    "    prod = -1/(2*stdev*stdev)\n",
    "    scale = 1.0\n",
    "    return np.exp(-1*np.square((x-mean)/(2*stdev)))\n",
    "\n",
    "x = np.arange(-1.0, 1.0, 0.01) \n",
    "ys = sigmoid(x) \n",
    "yg = gaussian(x) \n",
    "#y = np.linalg.norm((gaussian(x),sigmoid(x)))\n",
    "plt.plot(x, ys)\n",
    "plt.plot(x, yg)\n",
    "#plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e253e976-1ae0-4261-8455-83a20fb0e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "960546e1-1877-48c4-902e-e320f9641f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# UNET CRISP\n",
    "y = [0.70595, 0.7362168972983805, 0.73, 0.720725, 0.44696] \n",
    "x = [0, 1-10971707/31038801, 1-10971707/31038801, 1-9034509/31038801, 1-1397176/31038801] \n",
    "plt.plot(x, y, color='orange', label='CRISP (our method)')\n",
    "plt.title('Sparcity/Accuracy trade-off for Resnet-50')\n",
    "plt.ylabel('Top-1 Accuracy')\n",
    "plt.xlabel('Sparsity')\n",
    "plt.legend()\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9934ec8c-9a10-46bd-99a3-fee61c6ed1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7089285439859614"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-9034509/31038801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a6c5a4a-e28f-41ee-aee3-fe2c75eaadbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7089285439859614"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-9034509/31038801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a01d8619-443b-46c3-91fa-70bb7d70d2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6465164037747464"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-10971707/31038801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4b23a88-1378-47dc-ad9a-549327e4f828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9549861478218826"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-1397176/31038801"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd7b45-f4ea-49a7-a09b-1487a1086279",
   "metadata": {},
   "source": [
    "24 February, 2022\n",
    "- Initial training of LIT image set crisplit_20220223i_t100_02\n",
    "- {'cross_entropy_loss': 0.0069080255925655365, 'architecture_loss': 7.224996089935303, 'architecture_reduction': 0.9999997615814209}, 'test': {'confusion': [[389121016, 417767], [503600, 4402417]], 'similarity': {0: {'intersection': 389121016, 'union': 390042383, 'similarity': 0.9976377772258662}, 1: {'intersection': 4402417, 'union': 5323784, 'similarity': 0.8269338124912656}}, 'average time': 0.00234917549325026, 'mean intersection over union': 0.9122857948585659"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e7f27-02d7-4fb0-96f6-8daef1414432",
   "metadata": {},
   "source": [
    "25 February, 2022\n",
    "- What I wanted is shown to the left (CIFAR-10 Resnet): a pruning map that is predominately red (keep) or blue (prune).  In this case, the optimized network and pruned network have about the same behavior, the target pruning and the actual pruning are very close, and a little bit of final training of the pruned network achieves the same behavior of the pruned network.  Training UNET on the LIT dataset, I got instead was baby-blue pruning map to the right.  The network adapted its weights to the continuous relaxation, 99.5% of the network gets pruned, and its pruned behavior is totally different after training.  I had in my notes several places above that this could be a problem but since I didnt see it, I didnt compensate for it. \\\n",
    "![Resnet-56 cifar on CIFAR-10 ](../img/crisp20220121_t50_00_cw.png)\n",
    "![CRISP lit 25% full of fence sitters](../img/crisplit_20220224i_t025_04_cw.png)\n",
    "\n",
    "Next steps:\n",
    "1.\tGet the project cleaned up so others use it easily.\n",
    "1. \tIncorporate a fix for the fence sitters.  Options:\n",
    "    -\tInclude convolution norm in the channel pruning weights.  This blocks the optimizer from adjusting the convolution weights to compensate for the pruning weights.  This makes the backpropagation network tree much bigger and the pruning gradient more complicated.  However, there are not extra parameters or tuning.  \n",
    "    -\tIncrease the cutoff slope as [Growing Efficient Deep Networks by Structured Continuous Sparsification](https://arxiv.org/pdf/2007.15353.pdf) implements.  Eliminates the problems of (a) but also its benefits.  A capable optimizer may just learn to balance better so Im not convinced this the best approach. ![ProgressiveContinuousRelaxation](../img/ProgressiveContinuousRelaxation.png) \\\n",
    "    -\t[DAIS: Automatic Channel Pruning via Differentiable Annealing Indicator Search](https://arxiv.org/pdf/2011.02166.pdf) has a similar approach with a sigmoid function.  A capable optimizer may just learn to balance better in (b). ![Differential Annealing](../img/DifferentialAnnealing.png) \\\n",
    "\n",
    "    -\tInclude a basis function for each pruning weight as an optimizer objective.   To minimize the basis function, the optimizer will drive the pruning function towards off=-1 or on=11.  The basis function may need some tuning like b and c to not prevent potential optimal pruning.  A capable optimizer will generally move the pruning weights to on or off.  It adds another objective function.  It adds a basis function evaluation.  It eliminates gradient and backpropagation graph growth of (a)\n",
    " \n",
    "\n",
    "5)\tI think Ill experiment with several a few of these since I now have an easy way to test the solution with LIT segmentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e789c45-98fe-47f2-951d-b7c5a909ba62",
   "metadata": {},
   "source": [
    "28 February 2022\n",
    "- Including gradient norms in pruning weights.  This added more variation to the class weights map.  Keep this and add basis pruning weight basis funciton to optimizer objective.  Incrase basis funciton weight as architecture loss approaches 0\n",
    "- Add basis function to try to push pruning weights to 0 or 1\n",
    "- Add an exponential function to enable pruning optimization based on architecture loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a173a1db-cda8-4eae-bd37-9045ec43e70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def exp(x, k=3):\n",
    "    return np.exp(-k*x)\n",
    "\n",
    "x = np.arange(0.0, 5.0, 0.01) \n",
    "plt.plot(x, exp(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd244c0-b02b-408c-b11d-3776bac41411",
   "metadata": {},
   "source": [
    "1 March, 2022\n",
    "- Cell weighting with convolution norm and convolution weights adds some randomness to the pruning weights\n",
    "- Random initialization of cell weights also produces more randomness\n",
    "- Because we have convolution->batch norm->pruning weight function, I don't think pruning weights as a product of the pruning weight and convolution norm helps other than that\n",
    "- Measure backpropagation time with and without convolution norm to see the time impact of this \\\n",
    "![30% pruning class weights](../img/crisplit_20220228i_t30_00_cw.png)\n",
    "![Tensorboard](../img/crisplit_20220301i_t250_01_tb.png)\n",
    "![Pruning weights](../img/crisplit_20220301i_t250_01_cw.png)\n",
    "![Gradient Norm](../img/crisplit_20220301i_t250_01_gn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2647ed91-4d92-4400-a648-7801d5c1427f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "2 March 2022\n",
    "- Prune loss successfully pushed weights to 1 or -1\n",
    "- Still able to achieve pruning target with prune loss\n",
    "- Maintained cross entropy loss\n",
    "- Test results crisplit_20220302i_t025_00 'test': {'confusion': [[389538349, 16205], [3370370, 1519876]], 'similarity': {0: {'intersection': 389538349, 'union': 392924924, 'similarity': 0.9913811143219817}, 1: {'intersection': 1519876, 'union': 4906451, 'similarity': 0.30977095256836357}}, 'average time': 0.01020493042575285, 'mean intersection over union': 0.6505760334451727, 'num images': 963}}\n",
    "- Test resuls crisplit_20220302i_t030_00: 'test': {'confusion': [[389609704, 57700], [2137538, 2639858]], 'similarity': {0: {'intersection': 389609704, 'union': 391804942, 'similarity': 0.9943971150828415}, 1: {'intersection': 2639858, 'union': 4835096, 'similarity': 0.5459784045652868}}, 'average time': 0.009466161993769477, 'mean intersection over union': 0.7701877598240641, 'num images': 963}\n",
    "- Train architecture weights \\\n",
    "![Prune Weights 25%](../img/crisplit_20220302i_t025_00_cw.png)\n",
    "![Prune Weights 30%](../img/crisplit_20220302i_t030_00_cw.png)\n",
    "![Target Weights Tensroborad](../img/crisplit_20220302i_t030_00_tb.png) \\\n",
    "- Increase k_prune_basis from 1 to 10 to push values to 1\n",
    "\n",
    "- Add convolution norm to architecture loss\n",
    "- Compare UNET-LIT prune-loss computed wtih L1 and L2 norm.\n",
    "- L1 training results after 30 epochs:'training': {'cross_entropy_loss': 0.000783453113399446, 'architecture_loss': 2.7374879209673963e-05, 'prune_loss': 0.0009341231780126691, 'architecture_reduction': 0.30165454745292664} \n",
    "![Prune loss from L1 norm](../img/crisplit_20220302i_t030_00_tb_l1.png)\n",
    "![Prune loss from L1 norm](../img/crisplit_20220302i_t030_00_cw_l1.png)\n",
    "- L2 prune loss training after 39 epochs: 'training': {'cross_entropy_loss': 3.696866997415782e-06, 'architecture_loss': 0.03833708167076111, 'prune_loss': 0.007094455882906914, 'architecture_reduction': 0.3619169592857361}\n",
    "![Prune loss from L2 norm](../img/crisplit_20220302i_t030_00_norm.png)\n",
    "![Prune loss from L2 norm](../img/crisplit_20220302i_t030_00_norm_cw.png)\n",
    "- L2 norm produced a higher and longer prune_loss peak \n",
    "- L2 norm of pruning basis funciton resulted in greater sepration between on and off\n",
    "- L2 norm of pruning basis converged more slowly\n",
    "- Image dataset without augmenting test\n",
    "- Support switching datasets - coco, image, cityscapes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49401ce-1bf8-43ea-8108-ba4372414f1e",
   "metadata": {},
   "source": [
    "3 March 2022\n",
    "- crisplit_20220302i_t020_02_pnorm cell 4 has 1 output channel remaining and cell 5 has 0.  \n",
    "- I have disabled the cell pruning and this prevents this from being compensated\n",
    "- Somehow the empty convolution is making it correctly into the purne_weights plot which I didn't expect\n",
    "- The prune weight plot as the first convolution of cell 6 as blue.  \n",
    "- Correct weight display\n",
    "- Correctly prune convolutions and UNET when a convolution is pruned\n",
    "- Generalize this for different network structures\n",
    "- Realized that batch norm off for UNEt.  \n",
    "- In cell2d ConvBR ArchitectureWeights and ApplyStructure, conv_mask = sigmoid(sigmoid_scale*channel_scale) * tanh(weight_gain*l2_norm(conv.weight_output_channels))\n",
    "- This means that the weight scale -> if channel weights->0 or the channel->0\n",
    "- This prevetns the optimizer from compensate for a smaller channel scale with larger convolution values\n",
    "- Why product? Both sigmoid and tanh will be from 0-1.  ConvBR output is this same product  so it matches system behavior\n",
    "- in cell2d.py cell:ArchitectureWeights, I changed the prune weight to the norm of the the layer_weight/cnn_weight\n",
    "    ```python\n",
    "    norm_conv_weight.append(layer_weight/cnn_weight)\n",
    "    prune_weight = torch.norm(torch.cat(norm_conv_weight))/np.sqrt(num_conv_weights)\n",
    "    prune_weight = torch.tanh(self.weight_gain*prune_weight)\n",
    "    ```\n",
    "- Previously, this was a product and there was a tendencay to prune aggressively as the network becomes small.  \n",
    "- This should prune more linearly as the target size decreases\n",
    "- Move prune_basis compututaiton from cell2d ConvBR::ArchitectureWeights to Cell::ArchitectureWeights so the prune weight will be scaled by prune_weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268c06c-48b6-4484-941b-89c5809f077f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
