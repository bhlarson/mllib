{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLLib Development Notes\n",
    "\n",
    "Reverse time order - Newest on top"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 January 2023\n",
    "- Now it switches between train and test without allocating more memory.  \n",
    "- the change is moving outputs = model(inputs) into with dorch.no_grad() scope:\n",
    "    ```python\n",
    "    with torch.no_grad():\n",
    "        data = next(iTest)\n",
    "        inputs, labels = data\n",
    "\n",
    "        if args.cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        outputs = model(inputs)\n",
    "        loss, cross_entropy_loss, architecture_loss, architecture_reduction, cell_weights, prune_loss, sigmoid_scale = loss_fcn(outputs, labels, model)\n",
    "        classifications = torch.argmax(outputs, 1)\n",
    "        top1_correct = (classifications == labels).float()\n",
    "        test_accuracy = torch.sum(top1_correct)/len(top1_correct)\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 January 2023\n",
    "- Merging pytorch imagenet training algorithm into cell2d.py\n",
    "- Model re-allocates model between test and training batch in Train()\n",
    "- This results in out of memory errors for large batches\n",
    "- What triggers model data to be allocated a second time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 November 2022\n",
    "- Build plot from multiple runs.\n",
    "- First step of cityscapescrisp-l9fl7 looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Nobember\n",
    "- Can now initialize Resnet 18, 35, 50, 101, and 152 from pyorch\n",
    "- Automatically initializes based on the choice\n",
    "- Only initialize matching layers\n",
    "- Can handel with and without conv1\n",
    "Imagenet 152\n",
    "- crispimagnent_20221104_232354_ai2 \n",
    "- job crispimagenet-s7mxf\n",
    "- batch_size 75 because something was causing out of memory at startup.\n",
    "- I think it is now running on AI3, GPU 0 wtih 51267 MB GPU memory\n",
    "- I set learning rate to 1e-4, k_accuracy = 1, k_structure 0.2, k_prune_basis = 5.0.  I think that is high for k_prune_bais.  Epochs=20, ejector_start=9, ejector_full=1-.\n",
    "- After 14k samples, architecture_redution is 95%.  I think this is limited by the learning rate.  \n",
    "I expect a larger learning rate would reduce the accuracy beyond you can restore without cutmix and other training sequences that I haven't implemented.\n",
    "- I started a similar run on hiocnn but it failed to initialize from pretrained resnet.  Why?\n",
    "CIFAR10\n",
    "- Start with imagenet pretrained\n",
    "- Shortened the initial training too much.\n",
    "- Try a few short runs to see how to get the best CIFAR-10 accuracy from pretrained resnet.\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 November\n",
    "CIFAR 10\n",
    "- 2 runs with the following parameters\n",
    "    ```yaml\n",
    "    - {name: ejector_start, value: 24}\n",
    "    - {name: ejector_full, value: 25}\n",
    "    - {name: ejector_max, value: 1.0}\n",
    "    - {name: k_accuracy, value: 1.0}\n",
    "    - {name: k_structure, value: 0.3}\n",
    "    - {name: k_prune_basis, value: 1.0}\n",
    "    - {name: learning_rate, value: 1e-3}\n",
    "    - {name: learning_rate_decay, value: 0.25}\n",
    "    - {name: rate_schedule, value: \"[20, 30, 40, 45, 48]\"}\n",
    "    ```\n",
    "- Final results crispcifar10-j6598\n",
    "    ```yaml\n",
    "    \"prune\": {\n",
    "        \"final parameters\": 1472196,\n",
    "        \"initial parameters\": 11144978,\n",
    "        \"remaining ratio\": 0.13209501176224844,\n",
    "        \"final flops\": 519159872.0,\n",
    "        \"initial flops\": 2081330186.0,\n",
    "        \"remaining flops\": 0.24943657450034215\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"accuracy\": 0.9104,\n",
    "        \"minimum time\": 0.00018616,\n",
    "        \"average time\": 0.00022547190000000002,\n",
    "        \"num images\": 10000\n",
    "    }\n",
    "    ```\n",
    "- Final results crispcifar10-7hzbv\n",
    "    ```yaml\n",
    "    \"prune\": {\n",
    "    \"final parameters\": 1158295,\n",
    "    \"initial parameters\": 11144978,\n",
    "    \"remaining ratio\": 0.10392976998249795,\n",
    "    \"final flops\": 392450690.0,\n",
    "    \"initial flops\": 2081330186.0,\n",
    "    \"remaining flops\": 0.18855763138391343\n",
    "  },\n",
    "  \"test\": {\n",
    "    \"accuracy\": 0.9081,\n",
    "    \"minimum time\": 0.00017398,\n",
    "    \"average time\": 0.00017570680000000003,\n",
    "    \"num images\": 10000\n",
    "  }\n",
    "- Test k_structure 0.5 to see if these maintain accuracy: crispcifar10-hk4p9, 20221103_083925_ipc001\n",
    "- Test k_structure 0.7 to see if these maintain accuracy: \n",
    "\n",
    "Cityscapes\n",
    "- 2 completed runs\n",
    "- cityscapescrisp-7z2hd, crispcityscapes_20221102_173821_hiocnn\n",
    "    - name: k_structure value: 0.3\n",
    "    - Final miou: 0.3409290518484971\n",
    "    - Final flops:\n",
    "      ```yaml\n",
    "      \"prune\": {\n",
    "      \"final parameters\": 9483951,\n",
    "      \"initial parameters\": 31039057,\n",
    "      \"remaining ratio\": 0.305548941129236,\n",
    "      \"final flops\": 50761052160.0,\n",
    "      \"initial flops\": 218911211520.0,\n",
    "      \"remaining flops\": 0.23187963653182928\n",
    "      },\n",
    "      ```\n",
    "    - Unique accuracy divergence in searc_structure_04\n",
    "    - No significant change in prune_weights\n",
    "    - Not enough prune weights in play for effective pruning.  Most already solid red or blue\n",
    "    - Nothing else really happing at point that cross_entropy_loss diverged - step 33,000\n",
    "- cityscapescrisp-fdbm6, crispcityscapes_20221102_173841_hiocnn\n",
    "  - name: k_structure value: 0.1\n",
    "  - Final miou: 0.6625339287796528\n",
    "  - Final flops:\n",
    "    ```yaml\n",
    "    \"prune\": {\n",
    "    \"final parameters\": 1591184,\n",
    "    \"initial parameters\": 31039057,\n",
    "    \"remaining ratio\": 0.05126392854009708,\n",
    "    \"final flops\": 11552055296.0,\n",
    "    \"initial flops\": 218911211520.0,\n",
    "    \"remaining flops\": 0.05277050552042918\n",
    "  },\n",
    "  ```\n",
    "- This run was almost everything I could have wanted\n",
    "- I would have wanted MIOU over 7 but it looks like it could have achieved this with more time\n",
    "Next test:\n",
    "- k_structure value: 0.1\n",
    "- k_prune_basis value: 0.3\n",
    "- Increase epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 November 2022\n",
    "CIFAR10\n",
    "- [cifar10crisp_20221101_184447_ipc001](../test/cifar10crisp_20221101_184447_ipc001.yaml) successfully converged to 90% sparsity while maintaining test accuracy\n",
    "- Smooth, convergence\n",
    "- Prune loss always on\n",
    "- Good separation between on/off weights\n",
    "    ![](../img/cifar10crisp_20221101_184447_ipc001.png)\n",
    "    ![](../img/cifar10crisp_20221101_184447_ipc001_cw.png)\n",
    "- Can more agressive pruning still maintain accuracy?\n",
    "- Try cyling prune loss on and off\n",
    "Cityscapes\n",
    "- [cityscapescrisp_20221101_191519_hiocnn](../test/cityscapescrisp_20221101_191519_hiocnn.yaml) Poor sepration betweeen on and off\n",
    "- Contuned to erode model while accuracy decreased\n",
    "    ![](../img/cityscapescrisp_20221101_191519_hiocnn_tb.png)\n",
    "    ![](../img/cityscapescrisp_20221101_191519_hiocnn_cw.png)\n",
    "- Are prune weights being initialzed to 1.0 or 0.5?\n",
    "- the following resulting no architecture reduction.  It started going down but then got pushed back up by the other losses.\n",
    "```yaml\n",
    "        - name: ejector_start\n",
    "          value: -1\n",
    "        - name: ejector_full\n",
    "          value: 0\n",
    "        - name: ejector_max\n",
    "          value: 1.0\n",
    "        - name: model_src\n",
    "          value: ''\n",
    "        - name: model_dest\n",
    "          value: crispcityscapes_{{workflow.parameters.output_name}}\n",
    "        - name: k_accuracy\n",
    "          value: 1.0\n",
    "        - name: k_structure\n",
    "          value: 0.1\n",
    "        - name: k_prune_basis\n",
    "          value: 1.0\n",
    "        - name: learning_rate\n",
    "          value: 2e-3\n",
    "\n",
    "```\n",
    "- Test k_structure: 0.3\n",
    "```yaml\n",
    "            - {name: ejector_start, value: 24}\n",
    "            - {name: ejector_full, value: 25}\n",
    "            - {name: ejector_max, value: 1.0}\n",
    "            - {name: model_src, value: \"\"}\n",
    "            - {name: model_dest, value: \"crispcifar10_{{workflow.parameters.output_name}}\"}\n",
    "            - {name: k_accuracy, value: 1.0}\n",
    "            - {name: k_structure, value: 0.3}\n",
    "            - {name: k_prune_basis, value: 1.0}\n",
    "            - {name: learning_rate, value: 1e-3}\n",
    "```\n",
    "- Test k_structure: 0.1\n",
    "```yaml\n",
    "            - {name: ejector_start, value: 24}\n",
    "            - {name: ejector_full, value: 25}\n",
    "            - {name: ejector_max, value: 1.0}\n",
    "            - {name: model_src, value: \"\"}\n",
    "            - {name: model_dest, value: \"crispcifar10_{{workflow.parameters.output_name}}\"}\n",
    "            - {name: k_accuracy, value: 1.0}\n",
    "            - {name: k_structure, value: 0.1}\n",
    "            - {name: k_prune_basis, value: 1.0}\n",
    "            - {name: learning_rate, value: 1e-3}\n",
    "```yaml\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 November 2022\n",
    "- crispcityscapes_20221101_135120_hiocnn_tb prune weights all blue.  MIOU 0.66.  \n",
    "    - {name: k_accuracy, value: 1.0}\n",
    "    - {name: k_structure, value: 0.5}\n",
    "    - {name: k_prune_basis, value: 0.25}\n",
    "- Next test: k_structure from 0.5 to 0.1 because prune_weights almost completely blue.  Raise k_prune_basis from 0.25 to 1.0 for same reason\n",
    "- Match state_dict from loaded torchvision to crisp model.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 October 2022\n",
    "\n",
    "| Server | Workflow | Image | Job | Tensorboard | Purpose | Result |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| ai3 | imagenetcrispbase.yaml | crisptrain:0.1.143 | [crispimagenet-266v9](https://10.45.1.87:30992/workflows/tr/crispimagenet-266v9?tab=workflow) | [crispimagenet_20221023_211808_ai3_tb] | Train resnet 50 on imagenet for intro plot |\n",
    "| hiocnn | cityscapes.yaml | crisptrain:0.1.143 | [cityscapescrisp-kc7sh](https://192.168.0.163:30992/workflows/ml/cityscapescrisp-kc7sh?tab=workflow&nodeId=cityscapescrisp-kc7sh-1817027528&nodePanelView=inputs-outputs) | [crispcityscapes_20221023_000504_hiocnn_tb] | Train resnet 50 on imagenet for intro plot | Everyting is nothing |\n",
    "| hiocnn | cityscapes.yaml | localhost:32000/crispdev:0.1.144 | [cityscapescrisp-cp759](https://192.168.0.163:30992/workflows/ml/cityscapescrisp-cp759) | [crispcityscapes_20221024_064430_hiocnn_tb](http://hiocnn:6006/) | Cityscapes training for paper.  Initially train without weights. |  |\n",
    "| hiocnn | cityscapescrispss.yaml | localhost:32000/crispdev:0.1.144 | [cityscapescrisp-tzj28](https://192.168.0.163:30992/workflows/ml/cityscapescrisp-q8flp) | [crispcityscapes_20221024_064437_hiocnn_tb](http://hiocnn:6007/) | Cityscapes single step training for paper.  Initially train without weights. |  |\n",
    "| ipc001 | cifar10crisp.yaml | localhost:32000/crispdev:0.1.143 | [crispcifar10-kh7wt](https://192.168.0.155:30992/workflows/ml/crispcifar10-kh7wt) | [crispcifar10_20221022_224910_ipc001_tb](http://hiocnn:6007/) | Train with initial training based on crispcifar10_20221022_224910_ipc001_tb. |  |\n",
    "| ipc001 | workflow/cifar10.yaml | localhost:32000/crispdev:0.1.143 | [\"crispcifar10-sqd4l](https://192.168.0.155:30992/workflows/ml/\"crispcifar10-sqd4l) | [crispcifar10_20221024_095159_ipc001_tb]() | Train Resnet 56 CIFAR model with extended initial training based on crispcifar10_20221022_224910_ipc001_tb. |  |\n",
    "| ai2 |cifar10crisp.yaml | sci-ai-docker-internal.ha-us.dso.thermofisher.net/crispdev:0.1.143 | [crispcifar10-hrgp9](https://192.168.0.155:30992/workflows/ml/crispcifar10-hrgp9) | [crispcifar20221024_100353_ai2_tb]() | Train Resnet 56 CIFAR model with extended initial training based on crispcifar10_20221022_224910_ipc001_tb. |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22 October 2022\n",
    "\n",
    "| Server | Workflow | Job | Tensorboard | Purpose | Result |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| HIOCNN | cityscapes.yaml | [cityscapescrisp-5tz88](https://192.168.0.163:30992/workflows/ml/cityscapescrisp-5tz88?tab=workflow) | [crispcityscapes_20221022_200725_hiocnn_tb](http://192.168.0.163:6006/) | Standard training for paper results | Stalled pulling image from dsofrog |\n",
    "| HIOCNN | cityscapescrisp.yaml | [cityscapescrisp-5qsjm](https://192.168.0.163:30992/workflows/ml/cityscapescrisp-5qsjm?tab=workflow&nodeId=cityscapescrisp-5qsjm-2684523045) | [crispcityscapes_20221022_202621_hiocnn_tb](http://192.168.0.163:6007/) | CRISP multi-step training for paper resiults | Stalled pulling image from dsofrog |\n",
    "| AI3 | cityscapescrispss.yaml | [cityscapescrisp-xx9c7](https://10.45.1.87:30992/workflows/tr/cityscapescrisp-xx9c7?tab=workflow) | [crispcityscapes_20221022_204948_ai3_tb](http://localhost:6006/) | CRISP single-step training for paper results |\n",
    "| IPC001 |  cifar10.yaml | [crispcifar10-82rjt](https://192.168.0.155:30992/workflows/ml/crispcifar10-82rjt?tab=workflow) | [crispcifar10_20221022_224314_ipc001_tb](http://ipc001:6012/) | Test Resnet-56 Cifar-10 results | accuracy: 0.917 |\n",
    "| IPC001 | cifar10.yaml | [crispcifar10-7ljc9](https://192.168.0.155:30992/workflows/ml/crispcifar10-7ljc9?tab=workflow) | crispcifar10_20221022_224910_ipc001_tb | Extend CIFAR-10 Renet 18 initial training ot 40 epochs | accuracy: 0.9158 | \n",
    "| IPC001 | cifar10.yaml | [crispcifar10-qwwbj](https://192.168.0.155:30992/workflows/ml/crispcifar10-qwwbj?tab=workflow) | | Extend CIFAR-10 Renet 34 initial training ot 40 epochs | accuracy\": 0.9154 |\n",
    "| IPC001 | cifar10.yaml | [crispcifar10-2vfl](https://192.168.0.155:30992/workflows/ml/crispcifar10-2vfll?tab=workflow) | | Extend CIFAR-10 Renet 56 sgd training | accuracy: 0.8219 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19 October 2022\n",
    "\n",
    "Collapse pentality function\n",
    "- Max of sigmoid scalied prune weights\n",
    "- If max prune weight drops below 0.5, max conv_weight becomes 1.0-max\n",
    "- Add to architecture weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 October 2022\n",
    "\n",
    "Copmpleted all items from 6 October list except graphs\n",
    "1. Write models to S3 every epoch (done)\n",
    "1. Incorperate Bishal's class weightings\n",
    "1. Enable penality function to keep initial and final unet convolutions from collapsing\n",
    "1. Prepare graphs of training stability for paper\n",
    "1. Single vs multi-step pruning graphic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 October 2022\n",
    "Complted the folloing:\n",
    "1. Noise augmentation not working for CIFAR tests because the parameters were not passed correctly.  In cifar10store and iamgenetstore.  Now randomly select noise level from 0 to augment_noise and then apply this value as the scale factor for a random tensor:\n",
    "```python\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        rand_std = random.uniform(0, self.std)\n",
    "        return tensor + torch.randn(tensor.size()) * rand_std + self.mean\n",
    "```\n",
    "1. Fully pruning operators - Why did full collapse not occur in Resnet? - Now seeting channel size to 0 and then collapsing ConvBR based on this.\n",
    "1. Accurate FLOPS counter: needed correct convolution size  from prevous step to comptue this correctly.  \n",
    "\n",
    "Next:\n",
    "1. Explore initializing relaxation variables to a small distribuiton about 0.  This should eliminate the need to delay k_prune_basis.  When I set k_structure and k_prune_basis to 0 initially, the optimzier has been pushing the relaxation variables to 0 for some reason.\n",
    "1. Correct convolution number of parameters with input size\n",
    "1. Segmentation statistics package\n",
    "1. Incrase frequency of infrequent classes in Cityscapes training\n",
    "1. Full paper edit\n",
    "1. Prepare graphs of training stability for paper\n",
    "1. Single vs multi-step pruning graphic\n",
    "1. Cifar training with dropout\n",
    "1. Cifar training with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 October 2022\n",
    "\n",
    "Why is cityscapes MIOU of full dataset plateauing at ~ 40%?\n",
    "Test similarity of crispcityscapes_20221003_183915_hiocnn0_search_structure_05:\n",
    "```yaml\n",
    "    \"similarity\": {\n",
    "      \"0\": {\n",
    "        \"intersection\": 7246949,\n",
    "        \"union\": 14915080,\n",
    "        \"similarity\": 0.4858806657423225\n",
    "      },\n",
    "      \"1\": {\n",
    "        \"intersection\": 3,\n",
    "        \"union\": 3748718,\n",
    "        \"similarity\": 8.002735868635625e-07\n",
    "      },\n",
    "      \"2\": {\n",
    "        \"intersection\": 78862687,\n",
    "        \"union\": 90035956,\n",
    "        \"similarity\": 0.8759021451385489\n",
    "      },\n",
    "      \"3\": {\n",
    "        \"intersection\": 6547099,\n",
    "        \"union\": 12705750,\n",
    "        \"similarity\": 0.5152863073805167\n",
    "      },\n",
    "      \"4\": {\n",
    "        \"intersection\": 2398,\n",
    "        \"union\": 807556,\n",
    "        \"similarity\": 0.0029694535115830974\n",
    "      },\n",
    "      \"5\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 243128,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"6\": {\n",
    "        \"intersection\": 25223002,\n",
    "        \"union\": 40450349,\n",
    "        \"similarity\": 0.6235546200108187\n",
    "      },\n",
    "      \"7\": {\n",
    "        \"intersection\": 1385,\n",
    "        \"union\": 1274649,\n",
    "        \"similarity\": 0.001086573637134615\n",
    "      },\n",
    "      \"8\": {\n",
    "        \"intersection\": 7045,\n",
    "        \"union\": 1594836,\n",
    "        \"similarity\": 0.004417382100730107\n",
    "      },\n",
    "      \"9\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 317,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"10\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 43935,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"11\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 0,\n",
    "        \"similarity\": 1.0\n",
    "      },\n",
    "      \"12\": {\n",
    "        \"intersection\": 657347,\n",
    "        \"union\": 3259181,\n",
    "        \"similarity\": 0.20169085423607955\n",
    "      },\n",
    "      \"13\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 45793,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"14\": {\n",
    "        \"intersection\": 4629,\n",
    "        \"union\": 315389,\n",
    "        \"similarity\": 0.01467711302550184\n",
    "      },\n",
    "      \"15\": {\n",
    "        \"intersection\": 106986,\n",
    "        \"union\": 940503,\n",
    "        \"similarity\": 0.11375402311316392\n",
    "      },\n",
    "      \"16\": {\n",
    "        \"intersection\": 22483545,\n",
    "        \"union\": 29852598,\n",
    "        \"similarity\": 0.7531520372196752\n",
    "      },\n",
    "      \"17\": {\n",
    "        \"intersection\": 2349104,\n",
    "        \"union\": 6071877,\n",
    "        \"similarity\": 0.3868826723597991\n",
    "      },\n",
    "      \"18\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 515808,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"19\": {\n",
    "        \"intersection\": 15743853,\n",
    "        \"union\": 21689985,\n",
    "        \"similarity\": 0.7258581783251579\n",
    "      },\n",
    "      \"20\": {\n",
    "        \"intersection\": 93,\n",
    "        \"union\": 557967,\n",
    "        \"similarity\": 0.00016667652388044455\n",
    "      },\n",
    "      \"21\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 1320045,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"22\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 27510,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"23\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 20646,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"24\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 113259,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"25\": {\n",
    "        \"intersection\": 0,\n",
    "        \"union\": 180297,\n",
    "        \"similarity\": 0.0\n",
    "      },\n",
    "      \"26\": {\n",
    "        \"intersection\": 944897,\n",
    "        \"union\": 2303846,\n",
    "        \"similarity\": 0.41013895894083197\n",
    "      }\n",
    "    },\n",
    "    \"average time\": 0.000906032,\n",
    "    \"miou\": 0.18945994301997524,\n",
    "```\n",
    "Train ID for table similarty index above\n",
    "```json\n",
    "    \"objects\":[\n",
    "        {\"id\":0,    \"name\":\"unlabeled\",     \"trainId\":0 , \"category\":\"void\", \"display\":false, \"color\": [ 0,  0,  0]},\n",
    "        {\"id\":1,    \"name\":\"ego vehicle\",   \"trainId\":0 , \"category\":\"void\", \"display\":false, \"color\": [ 0,  0,  0]},\n",
    "        {\"id\":2,    \"name\":\"rectification border\", \"trainId\":0 , \"category\":\"void\", \"display\":false, \"color\": [ 0,  0,  0]},\n",
    "        {\"id\":3,    \"name\":\"out of roi\",    \"trainId\":0 , \"category\":\"void\", \"display\":false, \"color\": [ 0,  0,  0]},\n",
    "        {\"id\":4,    \"name\":\"static\",        \"trainId\":0 , \"category\":\"void\", \"display\":false, \"color\": [ 0,  0,  0]},\n",
    "        {\"id\":5,    \"name\":\"dynamic\",       \"trainId\":0 , \"category\":\"void\", \"display\":false, \"color\": [ 0,  0,  0]},\n",
    "        {\"id\":6,    \"name\":\"ground\",        \"trainId\":1 , \"category\":\"ground\", \"display\":false, \"color\": [ 81,  0, 81]},\n",
    "        {\"id\":7,    \"name\":\"road\",          \"trainId\":2 , \"category\":\"flat\", \"display\":false, \"color\": [ 128, 64,128]},\n",
    "        {\"id\":8,    \"name\":\"sidewalk\",      \"trainId\":3 , \"category\":\"flat\", \"display\":false, \"color\": [ 244, 35,232]},\n",
    "        {\"id\":9,    \"name\":\"parking\",       \"trainId\":4 , \"category\":\"flat\", \"display\":false, \"color\": [ 250,170,160]},\n",
    "        {\"id\":10,   \"name\":\"rail track\",    \"trainId\":5 , \"category\":\"flat\", \"display\":false, \"color\": [ 230,150,140]},\n",
    "        {\"id\":11,   \"name\":\"building\",      \"trainId\":6 , \"category\":\"construction\", \"display\":false, \"color\": [ 70, 70, 70]},\n",
    "        {\"id\":12,   \"name\":\"wall\",          \"trainId\":7 , \"category\":\"construction\", \"display\":false, \"color\": [ 102,102,156]},\n",
    "        {\"id\":13,   \"name\":\"fence\",         \"trainId\":8 , \"category\":\"construction\", \"display\":false, \"color\": [ 190,153,153]},\n",
    "        {\"id\":14,   \"name\":\"guard rail\",    \"trainId\":9 , \"category\":\"construction\", \"display\":false, \"color\": [ 180,165,180]},\n",
    "        {\"id\":15,   \"name\":\"bridge\",        \"trainId\":10 , \"category\":\"construction\", \"display\":false, \"color\": [150,100,100]},\n",
    "        {\"id\":16,   \"name\":\"tunnel\",        \"trainId\":11 , \"category\":\"construction\", \"display\":false, \"color\": [150,120, 90]},\n",
    "        {\"id\":17,   \"name\":\"pole\", \"trainId\":12 , \"category\":\"object\", \"display\":false, \"color\": [153,153,153]},\n",
    "        {\"id\":18,   \"name\":\"polegroup\",  \"trainId\":13 , \"category\":\"object\", \"display\":false, \"color\": [ 153,153,153]},\n",
    "        {\"id\":19,   \"name\":\"traffic light\",    \"trainId\":14 , \"category\":\"object\", \"display\":false, \"color\": [ 250,170, 30]},\n",
    "        {\"id\":20,   \"name\":\"traffic sign\",       \"trainId\":15 , \"category\":\"object\", \"display\":false, \"color\": [ 220,220,  0]},\n",
    "        {\"id\":21,   \"name\":\"vegetation\",           \"trainId\":16 , \"category\":\"nature\", \"display\":false, \"color\": [ 107,142, 35]},\n",
    "        {\"id\":22,   \"name\":\"terrain\",     \"trainId\":0 , \"category\":\"nature\", \"display\":false, \"color\": [152,251,152]},\n",
    "        {\"id\":23,   \"name\":\"sky\",     \"trainId\":0 , \"category\":\"sky\", \"display\":false, \"color\": [ 70,130,180]},\n",
    "        {\"id\":24,   \"name\":\"person\",        \"trainId\":17 , \"category\":\"human\", \"display\": true, \"color\":[ 220, 20, 60]},\n",
    "        {\"id\":25,   \"name\":\"rider\",         \"trainId\":18 , \"category\":\"human\", \"display\": true, \"color\":[ 255,  0,  0]},\n",
    "        {\"id\":26,   \"name\":\"car\",           \"trainId\":19 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0,  0,142]},\n",
    "        {\"id\":27,   \"name\":\"truck\",         \"trainId\":20 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0,  0, 70]},\n",
    "        {\"id\":28,   \"name\":\"bus\",           \"trainId\":21 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0, 60,100]},\n",
    "        {\"id\":29,   \"name\":\"caravan\",       \"trainId\":22 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0,  0, 90]},\n",
    "        {\"id\":30,   \"name\":\"trailer\",       \"trainId\":23 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0,  0,110]},\n",
    "        {\"id\":31,   \"name\":\"train\",         \"trainId\":24 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0, 80,100]},\n",
    "        {\"id\":32,   \"name\":\"motorcycle\",    \"trainId\":25 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 0,  0,230]},\n",
    "        {\"id\":33,   \"name\":\"bicycle\",       \"trainId\":26 , \"category\":\"vehicle\", \"display\": true, \"color\":[ 119, 11, 32]}\n",
    "    ]\n",
    "```\n",
    "- MIOU is the average similarity of all classes.  \n",
    "- There is a clear correlation between the accuracy of a class and the frequency of the class shown in the raw similarity numbers.\n",
    "- To incrase  the accuracy of uncommin classes, I could increase their freqency in trainig.\n",
    "- References for method?\n",
    "\n",
    "To do:\n",
    "1. Why is noise augmentation not working for CIFAR tests? - Randomly compute sigma\n",
    "1. Fully pruning operators - Why did full collapse not occur in Resnet?\n",
    "1. Accurate FLOPS counter\n",
    "1. Correct convolution number of parameters with input size\n",
    "1. Segmentation statistics package\n",
    "1. Incrase frequency of infrequent classes in Cityscapes training\n",
    "1. Full paper edit\n",
    "1. Prepare graphs of training stability for paper\n",
    "1. Single vs multi-step pruning graphic\n",
    "1. Cifar training with dropout\n",
    "1. Cifar training with noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 September 2022\n",
    "\n",
    "| Model | Test Accuracy | Learning Rate | Learning Rate Decay | Rate Schedule | Batch Size | Epochs |\n",
    "|---|---|---|---|---|---|---|\n",
    "| crispcifar10_20220923_174111_ai20 | 88% | 2e-3  | 0.25 | [90, 95, 97] | 256 | 100 |\n",
    "| crispcifar10_20220923_224415_ai20 | 90% | 2e-3  | 0.25 | [50, 75, 90] | 256 | 82 |\n",
    "| crispcifar10_20220923_224717_ai20 | 74% | 1e-2  | 0.5 | [50, 75, 90] | 70 | 60 |\n",
    "| crispcifar10_20220923_224728_ai20 | 79% | 5e-3  | 0.5 | [50, 75, 90] | 70 | 61 |\n",
    "| crispcifar10_20220923_224741_ai20 | 84% | 4e-3  | 0.25 | [50, 75, 90] | 70 | 100 |\n",
    "| crispcifar10_20220923_224750_ai20 | 79% | 2e-3  | 0.25 | [50, 75, 90] |  70 | 15 |\n",
    "| crispcifar10_20220923_224759_ai20 | 81% | 1e-3 | 0.25 | [50, 75, 90] | 70 | 10 |\n",
    "| crispcifar10_20220923_224805_ai20 | 84% | 1e-3 | .1 | [50, 75, 90] | 70 | 24 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24 September 2022\n",
    "Navigate network to automatically identify subnetworks for relaxation\n",
    "- Assume network input is fixed\n",
    "- Assume network output is fixed\n",
    "- Segments bound by convolutions\n",
    "- Elements that do not change size: ReLu, Batch Norm, element-wise multiplication\n",
    "- Elements with fixed behavior to channel pruning: concatination, sum, matrix multiplication\n",
    "- Elements that change tensor size: convolution\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 September 2022\n",
    "How to automated augmentation, relaxation, and pruning for any network\n",
    "\n",
    "| Operator                           | Operands | Input Shape | Output Shape | Parameters | Relaxed Params | FLOPS | Relaxed FLOPS | Prune | Collapse |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|  Actuvation (e.g. ReLu)                   | 1 | $ T_i = (b,c)\\oplus d $  | $ T_o = T_i $ | $p = 0$ | $p = 0$ | $q = {\\displaystyle \\prod T_i }$ | $ q = \\displaystyle{ \\prod T_i }$ <br> where $\\displaystyle {T_i[c] = \\sum f(s) }$ | $ f(x)=f(x) $ | $ f(x)=0 $ |\n",
    "|  batch norm $\\lVert x \\rVert$             | 1 | $ T_i = (b,c)\\oplus d $ | $ T_o = T_i $ | $p = 2 \\cdot c$ | $p = 2 \\cdot \\sum f(s)$ | $q = {\\displaystyle \\prod T_i }$ | $ q = \\displaystyle{ \\prod T_i }$ <br> where $\\displaystyle {T_i[c] = \\sum f(s) }$ |  $w_{bias} = w_{bias}[k \\neq 0]$ <br> $w_{weight} = w_{weight}[k \\neq 0]$ | $ f(x)=0 $  |\n",
    "|  Sum $\\sum$                               | n | $ T = (b,c)\\oplus d $ $  | $ T_o = T_i $ | 0 | 0 | $q = {\\displaystyle \\prod T_i }$ | $ q = \\displaystyle{ \\prod T_i }$ <br> where $\\displaystyle {T_i[c] = \\sum f(s) }$ |$ f(x)=f(x) $ | $ f(x)=0 $ |\n",
    "|  Element-wise product $ v \\cdot w $       | 2 | $ T_i = (b,c)\\oplus d $  | $ T_o = T_i $ | 0 | 0 | $q = {\\displaystyle \\prod T_i }$ | $ q = \\displaystyle{ \\prod T_i }$ <br> where $\\displaystyle {T_i[c] = \\sum f(s) }$ |$ f(x)=f(x) $ | $ f(x)=0 $ |\n",
    "|  Concantination $\\oplus$                  | n | $ T_{i,n} = (b,c_n)\\oplus d $  | $ {\\displaystyle T_{o} = (b,\\sum^n c_n)\\oplus d }$ | 0 | 0 | 0 | 0 | | |\n",
    "|  Convolution nD $ \\circledast $           | 1 | $ T_i = (b,c_i)\\oplus d_i $ | $ T_o = (b,c_o)\\oplus d_o $ | p=${\\displaystyle c_{i} \\cdot c_{o} \\cdot \\prod m }$ | $ p={\\displaystyle c_{i} \\cdot f_{i}(s) \\cdot c_{o} \\cdot f_{o}(s) \\cdot \\prod m }$ | $ q = c_o \\cdot \\prod d_o \\cdot ( \\prod m \\cdot c_i + 1) $ | $ q = c_o \\cdot f_{o}(s) \\cdot \\prod d_o \\cdot ( \\prod m \\cdot c_i \\cdot f_{i}(s) + 1) $ | $w_{weight} = w_{weight}[k \\neq 0]$ | $ f(x)=0 $ |\n",
    "|  Convolution Transpose nD $ \\circledast $ | 1 | $ (b,c_{i},y,x) $  | $ (b,c_{o},y,x) $ | ${\\displaystyle c_{i} \\cdot c_{o} \\cdot \\prod m }$ | | | | |\n",
    "|  Fully Connected                          | 1 | $ T_i $  | $ (b,c_{o},y,x) $ | | | | | |\n",
    "|  Attention $ \\circledast $                | 1 | $ (b,c_{i},y,x) $  | $ (b,c_{o},y,x) $ | | | | | |\n",
    "\n",
    "\n",
    "$ b $: batch size <br>\n",
    "$ c $: channels <br>\n",
    "$ d $: array of image spatial dimenstions kernal dimensions <br>\n",
    "\n",
    "$ k $: channel mask vector <br>\n",
    "$ m $: array of convolution kernal dimensions <br>\n",
    "$ p $: parameters  <br>\n",
    "$ q $: FLOPS  <br>\n",
    "$ s $: pruning weight vector <br>\n",
    "$ T $: array of matrix/tensor dimensions  <br>\n",
    "$ w $: learnable weights  <br>\n",
    "\n",
    "            self.conv.bias.data = self.conv.bias[conv_mask!=0]\n",
    "            if self.conv_transpose:\n",
    "                self.conv.weight.data = self.conv.weight[:, conv_mask!=0]\n",
    "            else:\n",
    "                self.conv.weight.data = self.conv.weight[conv_mask!=0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 September 2022\n",
    "- Identifed error in computing prune_loss - channel_scale parameters at both the ConvBR and RelaxedChanel level\n",
    "- Optimizer able to push disconnected ConvBR.channel_scale to zero to get a good score without changing RelaxedChannel scale.\n",
    "- Removed ConvBR.channel_scale\n",
    "- Data:\n",
    "\n",
    "![](../img/crisplit_20220906_135553_hiocnn0_tb.png)\n",
    "\n",
    "![](../img/crisplit_20220906_135553_hiocnn0_search_structure_00_cw.png)\n",
    "![](../img/crisplit_20220906_135553_hiocnn0_search_structure_01_cw.png)\n",
    "![](../img/crisplit_20220906_135553_hiocnn0_search_structure_02_cw.png)\n",
    "\n",
    "- There is now a much greater sepration beween green and red - the purpose of prune loss\n",
    "- As prune loss decreases, the sepration improves\n",
    "- Run failed on search_structure_03 with \"RuntimeError: stack expects each tensor to be equal size, but got [0, 133] at entry 0 and [640, 640] at entry 1\"\n",
    "- After adding a lot of retries, I have been ignoring this\n",
    "- I hope to eliminate this as I restructure the data pipeline\n",
    "- Today: can measure the flops difference in inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 September, 2022\n",
    "- crisplit_20220905_092852_hiocnn0 Tensorboard\n",
    "- Good reduction in cross entropy loss, architecture loss, and prune loss\n",
    "- Architecture loss and prune loss continue to drop in accracy at train-fine because this run retains RelaxChannels.forward:\n",
    "    ```python\n",
    "    x *= self.weights()[None,:,None,None] \n",
    "    ```\n",
    "- Visible speedup but not on the scale  predected by flops.  Try increasing batch size and reporting time/image.\n",
    "\n",
    "    ![](../img/crisplit_20220905_092852_tb.png)\n",
    "    \n",
    "    ![](../img/crisplit_20220905_092852_time_tb.png)\n",
    "    \n",
    "- Is prune loss computed correcty?  The separation between red and blue doesn't look as good as the prune_loss indicates it should\n",
    "\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_train_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_search_structure_00_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_search_structure_01_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_search_structure_02_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_search_structure_03_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_search_structure_04_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_search_structure_05_cw.png)\n",
    "![](../img/crisplit_20220905_092852_hiocnn0_cw.png)\n",
    "\n",
    "- Multi-step pruning remaining param: 00) 0.7731378625396268 01) 0.48561083468771227 02) 0.42105322495725067 03) 0.22855120405909413 04) 0.15663937722675275 05) 0.08637464480268803\n",
    "- Multi-step pruning remaining flops: 00) 0.5653346670404749  01) 0.2930746287240015 02) 0.25276051148131595 03) 0.1277587762880712 04) 0.08201866718763626 05) 0.043475724497471\n",
    "- Multi-step pruning miou: 00) 0.9078078050483269 01) 0.9091851611239994 02) 0.9122214043542808 03) 0.9107696964245597 04) 0.9124966168431123 05) 0.8963407455978467 train-fine) 0.9118852332572867 \n",
    "- Corrected an error in train-fine source model.  Was \"crisplit_{{workflow.parameters.output_name}}{{workflow.parameters.target_structure}}_search_structure_02\"\n",
    "- Did not save the model from train-fine to minio\n",
    "- Set moduel size when pruning so ptflops computes correctly for downsized images\n",
    "- Investigate adding flops measurements for additional modules used by fvcore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 September 2022\n",
    "- Optimization based on flops rather than sparsity\n",
    "- Achieving 95% flops reduciton and hasn't arrived at the bottom\n",
    "- No loss in acciuracy at this point\n",
    "  ![](../img/crisplit_20220904_091208_hiocnn_tb.png)\n",
    "- Multi-step pruning remaining flops: 1) 52%, 2) 25%, 3) 20%, 4) 12% 5) 8%, 6) 6%\n",
    "- Multi-step pruning remaining param: 1) 69%, 2) 41%, 3) 36%, 4) 23% 5) 18%, 6) 13%\n",
    "\n",
    "![](../img/crisplit_20220904_091208_hiocnn0_search_structure_00_cw.png)\n",
    "![](../img/crisplit_20220904_091208_hiocnn0_search_structure_01_cw.png)\n",
    "![](../img/crisplit_20220904_091208_hiocnn0_search_structure_02_cw.png)\n",
    "![](../img/crisplit_20220904_091208_hiocnn0_search_structure_03_cw.png)\n",
    "![](../img/crisplit_20220904_091208_hiocnn0_search_structure_04_cw.png)\n",
    "![](../img/crisplit_20220904_091208_hiocnn0_search_structure_05_cw.png)\n",
    "\n",
    "- Only achieving 2x inprovement in inference time\n",
    "  - A6000 GPUs have a lot of resources and can load entier model compofrtably\n",
    "  - I am not adjusting batch size - aloways 8\n",
    "  - Overall, we may be limited by image loading and preprocessing\n",
    "- Improvements:\n",
    "  - Push relaxation scaling into the convolution gain fore removing the relaxation layers\n",
    "  - Adjust bactch size based on available resources\n",
    "  - Create a test for inference time & identify limitaitons of achieving full speedup predicted by flops\n",
    "  - Improve parameter separation by fixing/adjusting losses.  I wouldn't expect such a small prune loss with so many parmeters far from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 August 2022\n",
    "- A complete parameter weight or flops counter that can be back-propegated contains the relaxed input vector and relaxed output vector\n",
    "- Torch dynamically computes its network graph\n",
    "- Torch has nn.Module.children() but not .parnets() so you can walk forward in the network graph \n",
    "- Torch backpropegation must walk backwards through the graph\n",
    "- Can relxation data be passed through network during forward?\n",
    "- Can relaxation data be passed through the network using \"children\"\n",
    "- How to account for diverging input signals like the decoder portion of a U-NET - inputs are received from the pass-through connection and the previous connection?\n",
    "- Option 1 - include the relaxed vector in in Module.forward(x)\n",
    "    - This mean a single method of navigating the network\n",
    "    - Two types of data will be passed as x in the forward call\n",
    "    - Must explicitly run a forward call before computing loss - this is the standard pattern\n",
    "    - It looks like this isn't fesible with torch tensors and forward\n",
    "- Option 2 - Pass the relaxed vector through the graph using nn.Module.children()\n",
    "    - Need to know initial layer\n",
    "    - Can walk through layers and pass relaxation functions\n",
    "    - Requires explicit call and second handling of graph\n",
    "- Option 3- share weights between modules\n",
    "- Option 4 - Place the relxation into a seprate module and call it in muliple places\n",
    "- Torch.Tensor.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 August 2022\n",
    "- It looks like parameter sparcity is a poor proxy for model run time on UNET\n",
    "- FLOPS as objective rather than weights would take into account the image size at each layer\n",
    "- Parameter sparcity is not taking into account actual compute time\n",
    "- Results from crisplit_20220811_223202_hiocnn0 and crisplit_20220811_224009_hiocnn0 illustrate 97% parameter sparsity but only 2x speeed increase\n",
    "- This is explained by flops because the weight for the center & smallest image layers were pruned and weights for the larger layers were retained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 August 2022\n",
    "- Archicture reduction learning_rate 2e-4, k_structure 0.1, k_prune_sigma 1 \n",
    "- Maintained cross entropy loss and mean intersection/union until final pruning\n",
    "- Pruning slowed but did not stop as cross entropy loss and MIOU increased.\n",
    "- Cross entropy loss is the smallest loss: cross_entropy_loss: 0.007638514041900635 architecture_loss: 0.09289148449897766 prune_loss: 0.17779725790023804\n",
    "- Prune loss impacted and slowed archetecture loss curves\n",
    "- Failed train-fine due to out of memory error\n",
    "- Computed pruned size from size in previous step not from origonal size\n",
    "\n",
    "![crisplit_20220801h00_tb](../img/crisplit_20220801h00_tb_k_structure_01.png)\n",
    "\n",
    "- Execution order: Train->Prune->Test\n",
    "- Tests performed after pruning\n",
    "- Maintained accurcy for the first 2 pruning steps\n",
    "\n",
    "[normalized-train_cw]: ../img/crisplit_20220801h000_normalized-train_cw.png\n",
    "[train_cw]: ../img/crisplit_20220801h000_train_cw.png\n",
    "[ss_00_cw]: ../img/crisplit_20220801h000_search_structure_00_cw.png\n",
    "[ss_01_cw]: ../img/crisplit_20220801h000_search_structure_01_cw.png\n",
    "[ss_02_cw]: ../img/crisplit_20220801h000_search_structure_02_cw.png\n",
    "|  | normalized-train | train | Search 0 | Search 1 | Search 2 |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|Cross Entropy Loss|0.2146|0.0046 |0.0054 |0.002837|0.00763|\n",
    "|MIOU              |      |0.8762 |0.9055 |0.891740|0.85680|\n",
    "|Reamining Ratio   |1.0   |1.0    |0.0594 |0.033540|0.02967|\n",
    "| Prune Weights |![][normalized-train_cw]|![][train_cw] |![][ss_00_cw] |![][ss_01_cw]|![][ss_02_cw]|\n",
    "\n",
    "- Failed to prune effectively: k_structure: 0.05 k_prune_basis: 1.0 \n",
    "- Failed to prune effectively: k_structure: 0.01 k_prune_basis: 1.0\n",
    "- Failed to prune effectively: k_structure: 0.01 k_prune_basis: 0.5\n",
    "- Add  k_accuracy to increase cross entropy loss weight without decreasing k_structure and k_prune_sigma to the point they have a vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27 July 2022\n",
    "- Yesterday's training froze at the end of the step \"search-structure-01\".  There was no error message and the pod was in the executing state\n",
    "- There were several failed S3 image reads during the run.\n",
    "- I killed the pod and restarted the run to see if the failure repeats.\n",
    "- I added ptflops flops counting library to the project\n",
    "- get_model_complexity_info is called from a few points and printed to stdout\n",
    "- Investigate the output in the debugger and include the network flops in the model output\n",
    "- Investigate replacing current model size counter with flops counter for loss computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 July 2022\n",
    "- Implemented multi-step training\n",
    "- Much faster - 5 days to 10 hours\n",
    "- Each step is 2x faster because I enable 1 device reading thread\n",
    "- About the same number of total steps\n",
    "- Accuracy maintained until final train\n",
    "- Final step removed sigmoid scaling without compensation\n",
    "- Initializing sigmoid to 0 and sech not pushing output to 1\n",
    "- Final train accuracy would likely match other accuracy with additional training\n",
    "- Infer time did not drop\n",
    "- Torch passes data to and from GPU on each layer - this will limit infer time\n",
    "- The batch size remained fixed through training - this will limit infer time\n",
    "\n",
    "![crisplit_20220723i01_tb](../img/crisplit_20220723i01_tb.png)\n",
    "![crisplit_20220723i01_tb_time](../img/crisplit_20220723i01_tb_time.png)\n",
    "\n",
    "- The search strcture image are from the previous step's pruning\n",
    "- crisplit_20220723i010_search_structure_00 remaining parameters 10288177/31037517 = 0.33147551719423946\n",
    "- crisplit_20220723i010_search_structure_01 remaining parameters 1749302/10288177 = 0.1700303173244395\n",
    "- crisplit_20220723i010_search_structure_02 remaining parameters 394959/1749302 = 0.2257809114721186\n",
    "- Pruned quickly even with k_prune_basis=0.1, k_structure=0.1, convMaskThreshold=0.01\n",
    "- Rarely push retained class weights to red\n",
    "- Only cross entropy loss settled\n",
    "- Architecture loss still dropping\n",
    "- prune loss still dripping\n",
    "![train](../img/crisplit_20220723i010_train_cw.png)\n",
    "![search_structure_00](../img/crisplit_20220723i010_search_structure_00_cw.png)\n",
    "![search_structure_01](../img/crisplit_20220723i010_search_structure_01_cw.png)\n",
    "![search_structure_02](../img/crisplit_20220723i010_search_structure_02_cw.png)\n",
    "![final](../img/crisplit_20220723i010_cw.png)\n",
    "\n",
    "- Maintain size comparison to initial network size, not previous pruned size\n",
    "- Push sigmoid output to 1\n",
    "- If not, compensate for sigmoid scale when pruning\n",
    "- Dynamically adjust batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 July 2022\n",
    "- Initialized pruning weight to 0 so architecture loss has maximum gradient and prune loss gradient is small\n",
    "- Quick convergence of architecture loss\n",
    "- Slow convergence of prune loss\n",
    "- Maintained good cross entropy loss throughout optimization \n",
    "\n",
    "![Tensorboard](../img/crisplit_20220709i_tb.png)\n",
    "\n",
    "- There is is a significant blue-shift for the prune_weights plot\n",
    "- This isn't necessarily a problem because there is a clear separation between the dark blue and other weights\n",
    "\n",
    "![Class Weights](../img/crisplit_20220709i0_train-fine_cw.png)\n",
    "\n",
    "- prune_weight moved quickly to blue (off) but not to red(on)\n",
    "- Setting target_structure to 0 means there is always a reward to reduce architecture loss\n",
    "- Try early pruning at ~ -2 sigma to speed training\n",
    "- Need to autoscale batch size to get full GPU speedup:L \n",
    "- Scale learning rate based on batch size:  [Learning Rates as a Function of Batch Size: A Random\n",
    "- Experiment with replacing the sigmoid function with a scale function for retained weights to improve the similarity between relaxed and pruned network \n",
    "- Push scale into previous network element to speed execution (e.g. convolution weights, batch norm scale)\n",
    "Matrix Theory Approach to Neural Network Training](https://arxiv.org/pdf/2006.09092.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 July 2022\n",
    "- Can prune_basis be applied 100% throughout train_fine relaxation? No\n",
    "- When I applied prune loss at epoch 1, any progress in architecture loss was reversed\n",
    "- I expect the gradient of prune loss is greater then the gradient of architecture reduction when they are applied simultaniously\n",
    "- I dropped k_prune_basis from 1.0 to 0.1 and had the same results where the architecure_loss and arcitecture_reduciton reverses\n",
    "![crisplit_20220708i0_tb](../img/crisplit_20220708i0_tb.png)\n",
    "\n",
    "- From [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION](https://arxiv.org/pdf/1412.6980.pdf),\n",
    "\n",
    "![Adam Optimizer](../img/AdamAlgorithm.png)\n",
    "\n",
    "- Step is dependendend on gradient\n",
    "- The total gradient is a product of the gradients of each objective/regularization function\n",
    "- I could compute/display the gradient of each objective function independently\n",
    "- I expect the gradient of cross entropy loss to near 0 since this is a pre-trained network\n",
    "- The gradient of architecture loss is dependent on k_structure and the change in sigmoid(prune weights)\n",
    "- Since we start all prune weights at 1, this will be a small gradient\n",
    "- The gradient of prune_loss will be dependent on k_prune_basis and the gradient of gaussian(prune weights)\n",
    "- Initializing prune wights at 1 and k_prune_sigma=1, there will be a clear gradient pushing prune weights more positive\n",
    "- What happens if we initialize prune_weights to 0 rather then 1?\n",
    "- We start with architecture loss gradient at its maximum\n",
    "- We start with prune_loss with a small gradient but near an unstable equilibrium point\n",
    "- Lets see what happens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from pymlutil.functions import Sigmoid, GaussianBasis\n",
    "x = np.arange(-2.0, 2.0, 0.01) \n",
    "y1 = Sigmoid(x, scale = 1.0, offset=0.0, k_exp = 5.0)\n",
    "plt.plot(x, y1)\n",
    "y2 = GaussianBasis(torch.tensor(x), sigma=0.5)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 July 2022\n",
    "- Removed batch norm layers from UNET\n",
    "- Training and pruning results were at least as good to network with batch norm\n",
    "- No clear advantage provided by batch norm for weight magnitude from impacting relaxation magnitude\n",
    "- crisplit_20220630i0_train\n",
    "    ```yaml\n",
    "    cross_entropy_loss: 0.0030129256192594767 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9977446895166078 \n",
    "            1: 0.8221591634441512 \n",
    "        average time: 0.003201360332294909 \n",
    "        miou: 0.9099519264803795 \n",
    "        num images: 963\n",
    "    ```\n",
    "- crisplit_20220630i0_train-fine\n",
    "    ```yaml\n",
    "    cross_entropy_loss: 0.002983461134135723 \n",
    "    architecture_loss: 0.005337885580956936 \n",
    "    prune_loss: 0.1341208666563034 \n",
    "    architecture_reduction: 0.05337885394692421 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9975066577436897 \n",
    "            1: 0.8136417113713045 \n",
    "        average time: 0.007769013499480788 \n",
    "        miou: 0.9055741845574972 \n",
    "        num images: 963\n",
    "    ```\n",
    "- crisplit_20220630i0_prune \n",
    "    ```yaml\n",
    "    prune: \n",
    "        final parameters: 593296 \n",
    "        initial parameters: 31037517 \n",
    "        remaining ratio: 0.019115446638337724 \n",
    "    training: \n",
    "        cross_entropy_loss: 0.0037698952946811914 \n",
    "    test: \n",
    "        similarity: \n",
    "            0: 0.9975929543526758 \n",
    "            1: 0.8155541153342405 \n",
    "        average time: 0.0032667279335410193 \n",
    "        miou: 0.9065735348434582 \n",
    "        num images: 963\n",
    "    ```\n",
    "- No full-layer collapse of UNET which may have caused a deviation between full and relaxed network\n",
    "\n",
    "![crisplit_20220630i0_train-fine_cw.pn](../img/crisplit_20220630i0_train-fine_cw.png)\n",
    "![crisplit_20220630i0_prune_cw.png](../img/crisplit_20220630i0_prune_cw.png)\n",
    "\n",
    "- Architecture reduction never became 0\n",
    "- Possibility of additional pruning while maintaining accuracy\n",
    "- Prune loss continued to fall but with a falling rate\n",
    "- Possibility of improved relaxation with increased time in train_fine\n",
    "- increasing prune_basis did not appear to negatively impact architecture loss or cross_entropy loss\n",
    "- How sloppy can I be with the prune_basis?  \n",
    "- Can prune_basis be applied 100% throughout train_fine relaxation? \n",
    "- Would this slow architecture search?\n",
    "- This would eliminate several parameters\n",
    "- This would provide additional time to converge prune_loss\n",
    "- When relaxation is removed, the scale factor is not applied to the convolution weights.\n",
    "- If the convolution weight is 1 or 0, then there is a resulting shift that must be removed in post prune training\n",
    "- Can post prune training be shortened or removed?\n",
    "- Investigate changing prune epochs from 15 to 7 or 5\n",
    "- Investigate converting sigmoid function to a fixed scale multiple\n",
    "\n",
    "![crisplit_20220630i_tb.png](../img/crisplit_20220630i_tb.png)\n",
    "\n",
    "### From literature review\n",
    "- Can I demonstrate a linear reduction in time for structured pruning size reduction?\n",
    "- This time reduction will need to adjust batch size to benefit from very heavy pruning.\n",
    "- Can I demonstrate a different time reduction for unstructured pruning?\n",
    "- Is this independent from other neural network optimizations: NAS, parameter optimization, quantization, target hardware optimization\n",
    "- Structured pruning may be achieving similar compression ratios and accuracies as unstructured pruning.  \n",
    "- If so, how?\n",
    "- Quantify performance advantages of structured pruning\n",
    "- What differences are there to the Lottery Ticket Hypothesis?\n",
    "- Is there a more appropriate theoretical basis?\n",
    "- Is there a theoretical basis independent of an approximation function?\n",
    "- Are there any useful conclusions that can be made from this?\n",
    "- Is the final pruning dependent on the initial state as is commonly assumed in the lottery ticket hypothesis\n",
    "- What are the advantages/disadvantages of relaxation vs magnitude pruning?\n",
    "- Explore simultaneous training/pruning\n",
    "- Training/pruning from random variables\n",
    "- Training/pruning from pretrained network\n",
    "- Training/pruning from transfer learning\n",
    "- How to speed pytorch training?  \n",
    "- It seems much slower than Tensorflow training.  \n",
    "- This may be due to FCN vs UNET not PyTorch vs Tensorflow or Tensorflow optimized data pipeline vs my Pytorch S3 data pulling.\n",
    "- How robust is a pruned model in retraining?\n",
    "- How robust is a pruned model to training in a new domain?\n",
    "- Does the model converge to an equivalent solution?  \n",
    "- How to deter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sigma(s) = \\frac{1.0}{1.0+ e^{-k_1s}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from pymlutil.functions import Sigmoid, GaussianBasis\n",
    "x = np.arange(-2.0, 2.0, 0.01) \n",
    "y1 = Sigmoid(x, scale = 1.0, offset=0.0, k_exp = 5.0)\n",
    "plt.plot(x, y1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\delta(s) = e^{-\\sqrt{\\frac{s}{2*k_3}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from pymlutil.functions import Sigmoid, GaussianBasis\n",
    "x = np.arange(-2.0, 2.0, 0.01) \n",
    "y2 = GaussianBasis(torch.tensor(x), sigma=0.5)\n",
    "plt.plot(x, y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
