{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Pytorch to TensorRT Conversion Path\n",
    "In my [previos tests](https://github.com/bhlarson/mllib#readme), converting Tensorflow models to TensorRT resulted in a ~ 10x reduction in runtime without reducing inference accuracy.  Following the same process for PyTorch models resulted in an incorrect TensorRT inference.  \n",
    "\n",
    "[This file](https://github.com/bhlarson/mllib/blob/master/TestONNXTRT.ipynb) isolates the ONNX to TensorRT conversion to identify usage or conversion errors that may explain the TensorRT inference failure.  In troubleshooting this process, I have updated to [TensorRT 8.2](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html) and incorperated the conversion calls described in TensorRT 8.2 documentation.  \n",
    "\n",
    "Process:\n",
    "1. Clone [MLLIB](https://github.com/bhlarson/mllib) project:\n",
    "```console\n",
    "git clone https://github.com/bhlarson/mllib.git\n",
    "```\n",
    "2. Fom the mllib directory, create an empty creds.json file and, build docker containers:\n",
    "```console\n",
    "cd mllib\n",
    "echo '{}' > creds.json\n",
    "./build\n",
    "```\n",
    "3. Start the NVIDIA pytorch docker container in console mode:\n",
    "```console\n",
    "./dtr\n",
    "```\n",
    "4. From the docker command line, start the jupyter engine to run this notebook:\n",
    "```console\n",
    "./lab\n",
    "```\n",
    "5. Open a chrome browser to [http://localhost:9999](http://localhost:9999)\n",
    "1. Open a TestONNXTRT.ipynb in Jupyter\n",
    "1. From the Jupyter Lab toolbar, Select Run->Run All Cells to run ONNX inference, ONNX to TensorRT conversion, and TensorRT conversion.\n",
    "1. Note that ONNX inference segments, people, animals, and vehicles\n",
    "1. Note the ONNX to TensorRT conversion is successful witout errors\n",
    "1. Note the TensorRT inference is corrupted with a 4x4 pattern in the inference image which is overlayed on top of the original image.   \n",
    "\n",
    "The inference results in this example are displayed as a per-chanel mask on top of the original image.  \n",
    "\n",
    "Of particular note for this study is\n",
    "1. [onnx-trt.py](./target/onnx-trt.py) implements the ONNX to TRT conversion described in the [NVIDIA TENSORRT DOCUMENTATION](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#network_python)\n",
    "1. [target.trtinference](./target/trtinference.py) implements TRT inference described in the [NVIDIA TENSORRT DOCUMENTATION](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#network_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from glob import glob\n",
    "import tqdm\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "import IPython\n",
    "# locate imports from either mllib directory or target directory\n",
    "sys.path.append(os.path.abspath(''))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.jsonutil import ReadDictJson\n",
    "from datasets.cocostore import resize_crop_or_pad\n",
    "from utils.metrics import MergeIman, DatasetResults\n",
    "from target.trtinference import TrtInference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    import cv2\n",
    "    import IPython\n",
    "    _,ret = cv2.imencode('.jpg', img) \n",
    "    i = IPython.display.Image(data=ret)\n",
    "    IPython.display.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 480\n",
    "width = 512\n",
    "batch_size = 1\n",
    "images = glob('test/*.jpg')\n",
    "class_dictionary_name= 'test/coco.json'\n",
    "onnxmodelname='test/segment_nas_512x442_20211119_00.onnx'\n",
    "trtmodelname='test/segment_nas_512x442_20211119_00.trt'\n",
    "trtprecision='fp16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dictionary = ReadDictJson(class_dictionary_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnxsess = ort.InferenceSession(onnxmodelname)\n",
    "input_name = onnxsess.get_inputs()[0].name\n",
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsResults = DatasetResults(class_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagename in images:\n",
    "    image = cv2.imread(imagename)\n",
    "    image, imgMean, imgStd = resize_crop_or_pad(image, height,width)\n",
    "    imageBCHW = np.expand_dims(image.transpose(2, 0, 1),0).astype('float32')\n",
    "    predonnx = onnxsess.run(None, {input_name: imageBCHW})\n",
    "    segmentation = np.argmax(predonnx[0], axis=1).astype('uint8')[0]\n",
    "\n",
    "    iman = MergeIman(image, segmentation, dsResults.lut,imgMean, imgStd)\n",
    "    imshow(iman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 target/onnx-trt-file.py  -onnxname {onnxmodelname} -trtname {trtmodelname} -precision {trtprecision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trtserializedmodel = None\n",
    "with open(trtmodelname, \"rb\") as f:\n",
    "    trtserializedmodel = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = TrtInference(trtserializedmodel, batch_size, height, width, class_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagename in images:\n",
    "    image = cv2.imread(imagename)\n",
    "    image, imgMean, imgStd = resize_crop_or_pad(image, height,width)\n",
    "    imageBCHW = np.expand_dims(image.transpose(2, 0, 1),0).astype('float32')\n",
    "    print('input shape: {}'.format(imageBCHW.shape))\n",
    "    segmentation = inf.predict(np.ascontiguousarray(imageBCHW))[0]\n",
    "    iman = MergeIman(image, segmentation, dsResults.lut,imgMean, imgStd)\n",
    "    imshow(iman)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
